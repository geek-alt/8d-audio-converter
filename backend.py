#!/usr/bin/env python3
"""
8D Audio Converter â€” Deep Analysis Backend v9.0
================================================

ANALYSIS ENGINE  (v6.0 additions over v4.0)
  âœ¦ MFCC timbral fingerprint (13 coefficients) â€” detects timbre type
  âœ¦ Chroma-based key / mode detection (major / minor)
  âœ¦ Tonnetz tonal tension analysis
  âœ¦ Interaural correlation analysis â€” detects fake stereo sources
  âœ¦ Crest factor / peak-to-RMS â€” detects over-compressed material
  âœ¦ Harmonic-to-noise ratio (HNR) via HPSS
  âœ¦ Onset rate & transient density classification
  âœ¦ Spectral rolloff & brightness ratio
  âœ¦ Zero-crossing rate â†’ breathiness / noisiness score
  âœ¦ Perceptual sharpness estimate (psychoacoustic model)
  âœ¦ Per-segment energy variance â†’ identifies intros/drops/outros
  âœ¦ Enhanced _optimize_parameters: uses crest factor, mode, transient
    density, stereo correlation, and HNR to fine-tune every parameter

8D ENGINE  (v9.0 â€” full feature set)
  âœ¦ HRIR Convolution Engine (NEW in v9.0) â€” breaks the FFmpeg ceiling:
      Real Head-Related Impulse Response convolution using scipy.signal.fftconvolve
      Woodworth ITD model + Duda/Martens ILD + pinna notch resonances
      OLA (overlap-add) block processing for time-varying spatial position
      Optional KEMAR dataset support (drop kemar_hrirs.npy alongside backend.py)
  âœ¦ Beat-Locked Rotation (NEW in v9.0):
      Beat positions from librosa drive a piecewise-linear LFO
      Source returns to the same azimuth on every beat â€” no drift
      Falls back to BPM-synced free-running LFO if no beat data
  âœ¦ Dual-Band Elevation v8.1 (ported to ALL engines):
      HF shelf at 8 kHz + sub cut/boost at 80 Hz
      Previously only in the 8-band engine; now in simple/vocal/6-band too
  âœ¦ ITD (Interaural Time Difference) simulation via per-band static
    channel delays â€” higher bands get up to 630 Î¼s inter-ear offset
  âœ¦ Pinna notch filters: narrow EQ cuts at ~8.5 kHz and ~10.5 kHz
    applied to the contra-lateral channel to simulate pinna reflections
  âœ¦ Pre-reverb delay (15â€“35 ms) â€” simulates room distance
  âœ¦ Allpass diffusion network â€” series of 4 prime-spaced aecho taps
    before the main reverb for richer early reflections
  âœ¦ Diffuse-field EQ (IEC 711 compensation for headphone listening)
  âœ¦ Equal-loudness compensation shelf (Fletcher-Munson at 70 phons)
  âœ¦ Frequency-dependent stereo width (lows narrower, highs wider)
  âœ¦ Per-band hard limiter before amix to prevent inter-band clipping
  âœ¦ Phase rotation awareness â€” alternates LFO phase offset per band

INFRASTRUCTURE  (v9.0)
  âœ¦ TTL eviction: stem sessions + batch queues auto-purged after 2 hours
  âœ¦ Cancel endpoint: DELETE /process/{job_id} terminates FFmpeg + asyncio task
  âœ¦ Active proc registry: _active_procs / _active_tasks for clean teardown

FILTERGRAPH ENGINES  (all retained + enhanced)
  âœ¦ HRIR Convolution + Mastering  v9.0 â€” Python HRIR â†’ FFmpeg reverb/EQ
  âœ¦ Studio Grade v9.0             â€” 8-band HRTF + beat-locked ITD + pinna
  âœ¦ 6-band multiband              â€” for enable_multi_band without full HRTF
  âœ¦ Vocal-aware 3-band            â€” legacy vocal center mode
  âœ¦ Simple 2-channel              â€” fallback for minimal FFmpeg installations

EQ  (12-band, unchanged API)
  âœ¦ 30 / 60 / 100 / 200 / 350 / 700 / 1500 / 3000 / 5000 / 8000 / 12000 / 16000 Hz
"""

import os
import sys
import json
import time
import asyncio
import subprocess
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime
import uuid
import numpy as np

from fastapi import FastAPI, UploadFile, File, WebSocket, HTTPException, Form
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

try:
    import librosa
    import soundfile as sf
    from scipy import signal
    ADVANCED_ANALYSIS = True
except ImportError:
    print("âš ï¸  Advanced analysis disabled. Run: pip install librosa soundfile scipy")
    ADVANCED_ANALYSIS = False

try:
    import h5py
    H5PY_AVAILABLE = True
except ImportError:
    H5PY_AVAILABLE = False
    print("âš ï¸  SOFA/HDF5 support disabled. Run: pip install h5py")

try:
    import yt_dlp
    YOUTUBE_SUPPORT = True
except ImportError:
    print("âš ï¸  YouTube disabled. Run: pip install yt-dlp")
    YOUTUBE_SUPPORT = False

# Three-tier demucs detection
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tier 1 â€” demucs.api Python API  (demucs â‰¥ 4.0.0, preferred)
#   Fastest: no subprocess, direct tensor access, runs in-process.
#
# Tier 2 â€” demucs CLI  (`python -m demucs`)
#   Falls back here when demucs IS installed but demucs.api fails to import.
#   This happens with demucs 4.0.1 when a sub-dependency (e.g. dora-search,
#   einops) has a version conflict or is not fully initialised at import time.
#   The CLI entrypoint is a separate code path inside the package and is often
#   unaffected by such conflicts.
#
# Tier 3 â€” Spleeter  (last resort, 4-stem only)
#
# STEM_ENGINE values: "demucs_api" | "demucs_cli" | "spleeter" | "none"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STEM_ENGINE     = "none"
STEM_SEPARATION = False
DemucsSeperator = None   # set only when Tier 1 succeeds

# â”€â”€ Tier 1: demucs Python API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import torch        # noqa: F401
    import demucs       # noqa: F401 (version check)
    from demucs.api import Separator as DemucsSeperator   # 4.x API
    STEM_SEPARATION = True
    STEM_ENGINE     = "demucs_api"
    _dv = getattr(demucs, "__version__", "4.x")
    print(f"âœ…  Demucs stem separation available â€” Python API (v{_dv})")

except ImportError as _e1:
    print(f"âš ï¸  demucs.api import failed: {_e1}")

    # â”€â”€ Tier 2: demucs CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Probe by running `python -m demucs --help` â€” if the package is
    # installed and runnable the exit code is 0 (or 1 with usage), never
    # FileNotFoundError / ModuleNotFoundError.
    try:
        import demucs as _demucs_pkg   # noqa: F401  (confirms package exists)
        _probe = subprocess.run(
            [sys.executable, "-m", "demucs", "--help"],
            capture_output=True, timeout=15,
        )
        # returncode 0 = success; some builds return 1 for --help but still work.
        # We treat anything except a hard import/module error as "usable".
        if _probe.returncode in (0, 1) and b"demucs" in (_probe.stdout + _probe.stderr).lower():
            STEM_SEPARATION = True
            STEM_ENGINE     = "demucs_cli"
            _dv2 = getattr(_demucs_pkg, "__version__", "4.x")
            print(f"âœ…  Demucs stem separation available â€” CLI fallback (v{_dv2})")
            print(f"    (demucs.api import failed; CLI still works â€” "
                  f"run `pip install --upgrade demucs` to restore API mode)")
        else:
            _err_out = (_probe.stdout + _probe.stderr).decode(errors="replace")[:300]
            print(f"âš ï¸  demucs CLI probe failed (rc={_probe.returncode}): {_err_out}")
            raise RuntimeError("CLI probe failed")

    except Exception as _e2:
        print(f"âš ï¸  demucs CLI unavailable: {_e2}")

        # â”€â”€ Tier 3: Spleeter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            from spleeter.separator import Separator as SpleeterSeparator
            STEM_SEPARATION = True
            STEM_ENGINE     = "spleeter"
            print("âœ…  Spleeter stem separation available (last-resort fallback)")
        except ImportError as _e3:
            print(f"âš ï¸  Spleeter import failed: {_e3}")
            print("âŒ  Stem separation DISABLED â€” run: pip install --upgrade demucs")

print(f"ðŸ“Š  Stem engine: {STEM_ENGINE} | Available: {STEM_SEPARATION}")

# Stem cache: session_id -> {stem_name: path}
stem_sessions: Dict[str, Dict[str, str]] = {}
# Timestamps for TTL eviction: session_id / batch_id -> creation time (epoch seconds)
_session_timestamps: Dict[str, float] = {}
# Active FFmpeg subprocesses: job_id -> asyncio.subprocess.Process
# Used by the cancel endpoint to terminate in-flight jobs.
_active_procs: Dict[str, Any] = {}
# Active asyncio Tasks: job_id -> Task â€” used to cancel the coroutine itself
_active_tasks: Dict[str, asyncio.Task] = {}

# ============================================================================
# APP SETUP
# ============================================================================

# lifespan must be defined before FastAPI() so the forward reference resolves.
# The TTL sweeper coroutine (_ttl_sweeper) is defined later in the file but is
# only *called* at runtime, so the late binding is fine.
@asynccontextmanager
async def lifespan(app):
    """
    FastAPI lifespan context manager â€” replaces deprecated @app.on_event("startup").
    Schedules the TTL sweeper and installs a loop exception handler that suppresses
    noisy ConnectionResetError tracebacks from client disconnects.
    """
    loop = asyncio.get_running_loop()
    def _exc_handler(loop, ctx):
        if isinstance(ctx.get("exception"), ConnectionResetError):
            return  # silence client-disconnect noise
        loop.default_exception_handler(ctx)
    loop.set_exception_handler(_exc_handler)

    sweeper = asyncio.create_task(_ttl_sweeper())
    try:
        yield
    finally:
        sweeper.cancel()
        try:
            await sweeper
        except asyncio.CancelledError:
            pass

app = FastAPI(title="8D Audio Converter AI API", version="10.0.0", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True,
                   allow_methods=["*"], allow_headers=["*"])

BASE_DIR   = Path(__file__).parent
UPLOAD_DIR = BASE_DIR / "uploads"
OUTPUT_DIR = BASE_DIR / "outputs"
TEMP_DIR   = BASE_DIR / "temp"
for d in [UPLOAD_DIR, OUTPUT_DIR, TEMP_DIR]:
    d.mkdir(exist_ok=True)

# ============================================================================
# MODELS
# ============================================================================

class ProcessingParams(BaseModel):
    # 8D parameters
    # ANALYSIS-CALIBRATED (38-song measurement campaign, Feb 2026):
    #   Median rotation = 0.0965 Hz (10.4 s orbit). Range 0.03â€“0.25 Hz.
    #   DRR = âˆ’32.6 dB â†’ reverb ~42Ã— louder than direct signal â‰ˆ 90% wet.
    #   Bass ILD swing Â±4 dB; HF ILD swing Â±18 dB â†’ freq-dependent depth critical.
    rotation_speed:     float = 0.10   # 10.0 s/orbit â€” analysis median 0.0965 Hz
    reverb_room:        float = 0.75   # large hall â€” analysis: diffuse echo density 0.054
    reverb_mix:         float = 0.85   # 90% wet target (DRR mean âˆ’32.6 dB in dataset)
    bass_rotation:      float = 0.03   # sub-bass barely moves (low freqs poorly localisable)
    treble_rotation:    float = 0.14   # HF leads â€” analysis treble ILD swing Â±18 dB
    stereo_width:       float = 1.0
    elevation:          float = 0.0
    distance:           float = 0.5
    intensity_multiplier: float = 1.0
    enable_vocal_center:  bool = True   # vocals anchored center by default
    enable_multi_band:    bool = True
    enable_hrtf:          bool = True
    enable_convolution_reverb: bool = True

    # Output
    # output_format: "mp3" | "wav" | "flac" | "ambisonics_foa" | "atmos_71_4"
    output_format: str = "mp3"
    bitrate:       int = 320
    sample_rate:   int = 48000
    bit_depth:     int = 24

    # Stem separation
    enable_stem_separation: bool = False
    stem_session_id:        Optional[str] = None   # reuse already-separated stems
    stem_engine_model:      str   = "htdemucs"     # htdemucs | htdemucs_6s | spleeter

    @classmethod
    def model_validate(cls, obj, *args, **kwargs):
        # BUG FIX: The frontend sends "stem_model" but Pydantic only knows
        # "stem_engine_model".  Pydantic silently drops unknown fields, so the
        # 6-stem model selector never reached the backend (always fell back to
        # "htdemucs").  Map the alias here before validation so both names work.
        if isinstance(obj, dict) and "stem_model" in obj and "stem_engine_model" not in obj:
            obj = {**obj, "stem_engine_model": obj.pop("stem_model")}
        return super().model_validate(obj, *args, **kwargs)

    # Stem psychoacoustics (None = InstrumentRouter auto-assign)
    stem_auto_route:        bool  = True   # let InstrumentRouter assign all per-stem params
    enable_gain_staging:    bool  = True   # normalize each stem to target LUFS before process
    stem_target_lufs:       float = -23.0  # LUFS target for per-stem normalization
    enable_multiband_master: bool = True   # post-mix multiband compressor on master bus

    # Per-stem override rotation (None = use global params / InstrumentRouter)
    stem_vocals_rotation:         Optional[float] = None
    stem_drums_rotation:          Optional[float] = None
    stem_bass_rotation_override:  Optional[float] = None
    stem_other_rotation:          Optional[float] = None
    stem_guitar_rotation:         Optional[float] = None
    stem_piano_rotation:          Optional[float] = None

    # Per-stem override width (None = InstrumentRouter)
    stem_vocals_width:    Optional[float] = None
    stem_drums_width:     Optional[float] = None
    stem_bass_width:      Optional[float] = None
    stem_other_width:     Optional[float] = None
    stem_guitar_width:    Optional[float] = None
    stem_piano_width:     Optional[float] = None

    # Per-stem override elevation (None = InstrumentRouter)
    stem_vocals_elevation:  Optional[float] = None
    stem_drums_elevation:   Optional[float] = None
    stem_bass_elevation:    Optional[float] = None
    stem_other_elevation:   Optional[float] = None
    stem_guitar_elevation:  Optional[float] = None
    stem_piano_elevation:   Optional[float] = None

    # Per-stem override reverb mix (None = InstrumentRouter)
    stem_vocals_reverb:   Optional[float] = None
    stem_drums_reverb:    Optional[float] = None
    stem_bass_reverb:     Optional[float] = None
    stem_other_reverb:    Optional[float] = None
    stem_guitar_reverb:   Optional[float] = None
    stem_piano_reverb:    Optional[float] = None

    # Video visualizer
    generate_video:  bool = False
    video_style:     str  = "waveform"   # "waveform" | "spectrum" | "vectorscope"
    video_resolution: str = "1280x720"
    video_fps:       int  = 25

    # 12-band EQ (dB)
    # FIXED: Old defaults had +7/+8/+5 dB bass boost = ~20 dB LF boost.
    # That directly caused QA "muffled" (HF 35+ dB below LF).
    # Now: neutral bass, boosted HF to compensate HRTF orbital ILD loss.
    eq_sub30_gain:       float =  1.5   # light sub body
    eq_sub60_gain:       float =  2.5   # gentle bass depth
    eq_bass100_gain:     float =  1.5   # mild bass body
    eq_ubass200_gain:    float =  0.5   # neutral upper bass
    eq_lowmid350_gain:   float = -1.5   # cut low-mid mud
    eq_mid700_gain:      float =  0.0   # neutral
    eq_umid1500_gain:    float =  1.5   # vocal clarity
    eq_presence3k_gain:  float =  3.0   # presence
    eq_def5k_gain:       float =  3.5   # definition â€” compensates ILD
    eq_bril8k_gain:      float =  4.5   # brilliance â€” compensates ILD HF loss
    eq_air12k_gain:      float =  5.5   # air â€” critical for non-muffled result
    eq_uair16k_gain:     float =  3.5   # ultra-air sparkle

    # Legacy aliases (absorbed into new bands)
    eq_sub_bass_gain:    float =  0.0
    eq_bass_gain:        float =  0.0
    eq_low_mid_gain:     float =  0.0
    eq_presence_gain:    float =  0.0
    eq_air_gain:         float =  0.0

    # Enhancement toggles
    vocal_safe_bass:       bool = True
    instrument_enhance:    bool = True

    # Studio v5.0+ Fields
    reverb_density:   float = 0.7
    hrtf_intensity:   float = 1.0
    enable_limiter:   bool  = True

    # Preset management
    preset_id: Optional[str] = None  # genre key or 'auto'

    @classmethod
    def apply_preset(cls, base: 'ProcessingParams', preset_data: Dict[str, Any]) -> 'ProcessingParams':
        """Merge preset dict into ProcessingParams, ignoring unknown keys."""
        valid = {
            k: v for k, v in preset_data.items()
            if k in cls.model_fields and k != 'name'
        }
        return base.model_copy(update=valid)


class YouTubeDownloadRequest(BaseModel):
    url: str

class BatchJob(BaseModel):
    job_id:     str
    filename:   str
    status:     str
    progress:   int = 0
    output_url: Optional[str] = None
    error:      Optional[str] = None

batch_queue: Dict[str, List[BatchJob]] = {}


# ============================================================================
# PRESET LIBRARY v2.0 â€” Genre & Characteristic-Based Presets
# ============================================================================

class PresetLibrary:
    """
    Comprehensive preset library with automatic selection based on audio analysis.
    Each preset defines rotation, reverb, EQ, and spatial params optimised for
    a specific music style or spectral characteristic.
    """

    GENRE_PRESETS: Dict[str, Dict[str, Any]] = {
        # â”€â”€ Electronic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'electronic': {
            'name': 'Electronic Pulse',
            'rotation_speed': 0.14, 'bass_rotation': 0.04, 'treble_rotation': 0.22,
            'reverb_room': 0.70, 'reverb_mix': 0.88, 'reverb_density': 0.80,
            'stereo_width': 1.35, 'elevation': 0.05, 'distance': 0.9,
            'eq_sub30_gain': 1.8, 'eq_sub60_gain': 2.2, 'eq_bass100_gain': 1.5,
            'eq_lowmid350_gain': -3.5, 'eq_mid700_gain': -1.5,
            'eq_presence3k_gain': 3.0, 'eq_bril8k_gain': 4.9, 'eq_air12k_gain': 6.3,
            'enable_vocal_center': False, 'instrument_enhance': True,
            'intensity_multiplier': 1.15, 'hrtf_intensity': 1.1,
        },
        'house': {
            'name': 'House Groove',
            'rotation_speed': 0.10, 'bass_rotation': 0.035, 'treble_rotation': 0.175,
            'reverb_room': 0.75, 'reverb_mix': 0.87, 'reverb_density': 0.80,
            'stereo_width': 1.25, 'elevation': 0.0, 'distance': 1.0,
            'eq_sub30_gain': 2.1, 'eq_sub60_gain': 2.4, 'eq_bass100_gain': 1.6,
            'eq_lowmid350_gain': -2.5, 'eq_mid700_gain': -0.5,
            'eq_presence3k_gain': 2.5, 'eq_bril8k_gain': 4.2, 'eq_air12k_gain': 5.6,
            'enable_vocal_center': False, 'instrument_enhance': True,
            'intensity_multiplier': 1.1, 'hrtf_intensity': 1.0,
        },
        'trance': {
            'name': 'Trance Atmosphere',
            'rotation_speed': 0.08, 'bass_rotation': 0.028, 'treble_rotation': 0.154,
            'reverb_room': 0.85, 'reverb_mix': 0.45, 'reverb_density': 0.90,
            'stereo_width': 1.45, 'elevation': 0.12, 'distance': 1.2,
            'eq_sub30_gain': 1.5, 'eq_sub60_gain': 1.9, 'eq_bass100_gain': 1.3,
            'eq_lowmid350_gain': -2.0, 'eq_mid700_gain': 0.0,
            'eq_presence3k_gain': 3.5, 'eq_bril8k_gain': 5.6, 'eq_air12k_gain': 7.7,
            'enable_vocal_center': False, 'instrument_enhance': True,
            'intensity_multiplier': 1.05, 'hrtf_intensity': 1.2,
        },
        # â”€â”€ Rock / Metal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'rock': {
            'name': 'Rock Arena',
            'rotation_speed': 0.095, 'bass_rotation': 0.044, 'treble_rotation': 0.176,
            'reverb_room': 0.55, 'reverb_mix': 0.30, 'reverb_density': 0.70,
            'stereo_width': 1.20, 'elevation': 0.0, 'distance': 1.0,
            'eq_sub30_gain': 1.2, 'eq_sub60_gain': 1.5, 'eq_bass100_gain': 1.6,
            'eq_lowmid350_gain': -3.5, 'eq_mid700_gain': -2.0,
            'eq_presence3k_gain': 4.5, 'eq_def5k_gain': 3.5, 'eq_bril8k_gain': 4.2,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.1, 'hrtf_intensity': 1.0,
        },
        'metal': {
            'name': 'Metal Intensity',
            'rotation_speed': 0.12, 'bass_rotation': 0.052, 'treble_rotation': 0.22,
            'reverb_room': 0.45, 'reverb_mix': 0.25, 'reverb_density': 0.65,
            'stereo_width': 1.30, 'elevation': -0.05, 'distance': 0.95,
            'eq_sub30_gain': 1.6, 'eq_sub60_gain': 1.8, 'eq_bass100_gain': 1.3,
            'eq_lowmid350_gain': -5.0, 'eq_mid700_gain': -3.5,
            'eq_presence3k_gain': 6.0, 'eq_def5k_gain': 4.5, 'eq_bril8k_gain': 4.9,
            'enable_vocal_center': True, 'instrument_enhance': False,
            'intensity_multiplier': 1.2, 'hrtf_intensity': 1.1,
        },
        'indie': {
            'name': 'Indie Intimate',
            'rotation_speed': 0.07, 'bass_rotation': 0.028, 'treble_rotation': 0.121,
            'reverb_room': 0.68, 'reverb_mix': 0.38, 'reverb_density': 0.75,
            'stereo_width': 1.10, 'elevation': 0.05, 'distance': 1.1,
            'eq_sub30_gain': 0.8, 'eq_sub60_gain': 1.1, 'eq_bass100_gain': 1.2,
            'eq_lowmid350_gain': -1.5, 'eq_mid700_gain': 0.5,
            'eq_presence3k_gain': 2.5, 'eq_bril8k_gain': 3.5, 'eq_air12k_gain': 4.2,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.0, 'hrtf_intensity': 0.95,
        },
        # â”€â”€ Hip-Hop / R&B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'hip_hop': {
            'name': 'Hip-Hop Bass',
            'rotation_speed': 0.065, 'bass_rotation': 0.024, 'treble_rotation': 0.11,
            'reverb_room': 0.50, 'reverb_mix': 0.25, 'reverb_density': 0.65,
            'stereo_width': 1.15, 'elevation': -0.08, 'distance': 0.85,
            'eq_sub30_gain': 2.5, 'eq_sub60_gain': 2.9, 'eq_bass100_gain': 1.9,
            'eq_lowmid350_gain': -4.5, 'eq_mid700_gain': -2.5,
            'eq_presence3k_gain': 2.0, 'eq_bril8k_gain': 2.8, 'eq_air12k_gain': 3.5,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.05, 'hrtf_intensity': 0.9,
        },
        'trap': {
            'name': 'Trap Spatial',
            'rotation_speed': 0.075, 'bass_rotation': 0.028, 'treble_rotation': 0.138,
            'reverb_room': 0.60, 'reverb_mix': 0.30, 'reverb_density': 0.70,
            'stereo_width': 1.25, 'elevation': 0.0, 'distance': 0.9,
            'eq_sub30_gain': 2.7, 'eq_sub60_gain': 3.0, 'eq_bass100_gain': 1.8,
            'eq_lowmid350_gain': -4.0, 'eq_mid700_gain': -2.0,
            'eq_presence3k_gain': 2.5, 'eq_bril8k_gain': 4.2, 'eq_air12k_gain': 4.9,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.1, 'hrtf_intensity': 1.0,
        },
        'rnb': {
            'name': 'R&B Smooth',
            'rotation_speed': 0.07, 'bass_rotation': 0.028, 'treble_rotation': 0.121,
            'reverb_room': 0.72, 'reverb_mix': 0.40, 'reverb_density': 0.80,
            'stereo_width': 1.15, 'elevation': 0.05, 'distance': 1.1,
            'eq_sub30_gain': 1.5, 'eq_sub60_gain': 1.8, 'eq_bass100_gain': 1.5,
            'eq_lowmid350_gain': -2.0, 'eq_mid700_gain': 0.0,
            'eq_presence3k_gain': 3.0, 'eq_bril8k_gain': 4.2, 'eq_air12k_gain': 5.6,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.0, 'hrtf_intensity': 0.95,
        },
        # â”€â”€ Classical / Acoustic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'classical': {
            'name': 'Classical Hall',
            'rotation_speed': 0.05, 'bass_rotation': 0.02, 'treble_rotation': 0.088,
            'reverb_room': 0.90, 'reverb_mix': 0.55, 'reverb_density': 0.92,
            'stereo_width': 1.35, 'elevation': 0.15, 'distance': 1.4,
            'eq_sub30_gain': 0.3, 'eq_sub60_gain': 0.4, 'eq_bass100_gain': 0.6,
            'eq_lowmid350_gain': -1.0, 'eq_mid700_gain': 0.5,
            'eq_presence3k_gain': 3.5, 'eq_bril8k_gain': 4.2, 'eq_air12k_gain': 5.6,
            'enable_vocal_center': False, 'instrument_enhance': False,
            'intensity_multiplier': 0.95, 'hrtf_intensity': 1.15,
        },
        'orchestral': {
            'name': 'Orchestral Wide',
            'rotation_speed': 0.055, 'bass_rotation': 0.02, 'treble_rotation': 0.099,
            'reverb_room': 0.88, 'reverb_mix': 0.50, 'reverb_density': 0.90,
            'stereo_width': 1.40, 'elevation': 0.12, 'distance': 1.3,
            'eq_sub30_gain': 0.4, 'eq_sub60_gain': 0.6, 'eq_bass100_gain': 0.8,
            'eq_lowmid350_gain': -1.5, 'eq_mid700_gain': 0.0,
            'eq_presence3k_gain': 3.0, 'eq_bril8k_gain': 4.9, 'eq_air12k_gain': 6.3,
            'enable_vocal_center': False, 'instrument_enhance': False,
            'intensity_multiplier': 1.0, 'hrtf_intensity': 1.2,
        },
        'acoustic': {
            'name': 'Acoustic Natural',
            'rotation_speed': 0.06, 'bass_rotation': 0.024, 'treble_rotation': 0.105,
            'reverb_room': 0.75, 'reverb_mix': 0.42, 'reverb_density': 0.78,
            'stereo_width': 1.15, 'elevation': 0.08, 'distance': 1.2,
            'eq_sub30_gain': 0.6, 'eq_sub60_gain': 0.9, 'eq_bass100_gain': 1.1,
            'eq_lowmid350_gain': -1.5, 'eq_mid700_gain': 0.5,
            'eq_presence3k_gain': 2.5, 'eq_bril8k_gain': 3.5, 'eq_air12k_gain': 4.9,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.0, 'hrtf_intensity': 1.0,
        },
        # â”€â”€ Jazz / Blues â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'jazz': {
            'name': 'Jazz Club',
            'rotation_speed': 0.075, 'bass_rotation': 0.028, 'treble_rotation': 0.132,
            'reverb_room': 0.70, 'reverb_mix': 0.40, 'reverb_density': 0.75,
            'stereo_width': 1.15, 'elevation': 0.0, 'distance': 1.0,
            'eq_sub30_gain': 0.8, 'eq_sub60_gain': 1.1, 'eq_bass100_gain': 0.9,
            'eq_lowmid350_gain': -1.5, 'eq_mid700_gain': 0.0,
            'eq_presence3k_gain': 2.5, 'eq_bril8k_gain': 3.5, 'eq_air12k_gain': 4.2,
            'enable_vocal_center': False, 'instrument_enhance': True,
            'intensity_multiplier': 1.0, 'hrtf_intensity': 1.0,
        },
        'blues': {
            'name': 'Blues Soul',
            'rotation_speed': 0.065, 'bass_rotation': 0.024, 'treble_rotation': 0.116,
            'reverb_room': 0.65, 'reverb_mix': 0.35, 'reverb_density': 0.70,
            'stereo_width': 1.10, 'elevation': 0.0, 'distance': 1.0,
            'eq_sub30_gain': 1.1, 'eq_sub60_gain': 1.3, 'eq_bass100_gain': 1.2,
            'eq_lowmid350_gain': -2.0, 'eq_mid700_gain': -0.5,
            'eq_presence3k_gain': 3.0, 'eq_bril8k_gain': 3.5, 'eq_air12k_gain': 4.2,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.0, 'hrtf_intensity': 0.95,
        },
        # â”€â”€ Pop / Vocal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'pop': {
            'name': 'Pop Polish',
            # Analysis: medium tier 0.07â€“0.12 Hz for pop (median 0.0965 Hz)
            'rotation_speed': 0.10, 'bass_rotation': 0.032, 'treble_rotation': 0.165,
            'reverb_room': 0.75, 'reverb_mix': 0.87, 'reverb_density': 0.75,
            'stereo_width': 1.20, 'elevation': 0.0, 'distance': 1.0,
            'eq_sub30_gain': 1.3, 'eq_sub60_gain': 1.6, 'eq_bass100_gain': 1.3,
            'eq_lowmid350_gain': -2.5, 'eq_mid700_gain': -0.5,
            'eq_presence3k_gain': 3.5, 'eq_bril8k_gain': 4.2, 'eq_air12k_gain': 5.6,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.05, 'hrtf_intensity': 1.0,
        },
        'ballad': {
            'name': 'Ballad Emotion',
            # Analysis: slow tier 0.03â€“0.06 Hz for ballads (e.g. Ed Perfect 0.056 Hz)
            # Reverb: ballads use longer decay (4.0 s) per analysis recommendation
            'rotation_speed': 0.05, 'bass_rotation': 0.018, 'treble_rotation': 0.088,
            'reverb_room': 0.88, 'reverb_mix': 0.92, 'reverb_density': 0.82,
            'stereo_width': 1.10, 'elevation': 0.08, 'distance': 1.2,
            'eq_sub30_gain': 0.9, 'eq_sub60_gain': 1.2, 'eq_bass100_gain': 1.2,
            'eq_lowmid350_gain': -1.5, 'eq_mid700_gain': 0.5,
            'eq_presence3k_gain': 3.0, 'eq_bril8k_gain': 3.5, 'eq_air12k_gain': 4.9,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 0.95, 'hrtf_intensity': 0.95,
        },
        # â”€â”€ Ambient / Chill â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'ambient': {
            'name': 'Ambient Drift',
            # Analysis slow tier: 0.03â€“0.06 Hz, extreme reverb wet (DRR > 34 dB)
            'rotation_speed': 0.035, 'bass_rotation': 0.012, 'treble_rotation': 0.061,
            'reverb_room': 0.95, 'reverb_mix': 0.95, 'reverb_density': 0.95,
            'stereo_width': 1.50, 'elevation': 0.18, 'distance': 1.6,
            'eq_sub30_gain': 0.4, 'eq_sub60_gain': 0.8, 'eq_bass100_gain': 0.6,
            'eq_lowmid350_gain': -1.0, 'eq_mid700_gain': 0.0,
            'eq_presence3k_gain': 2.0, 'eq_bril8k_gain': 4.2, 'eq_air12k_gain': 7.7,
            'enable_vocal_center': False, 'instrument_enhance': False,
            'intensity_multiplier': 0.9, 'hrtf_intensity': 1.25,
        },
        'lofi': {
            'name': 'Lo-Fi Chill',
            'rotation_speed': 0.045, 'bass_rotation': 0.016, 'treble_rotation': 0.077,
            'reverb_room': 0.72, 'reverb_mix': 0.38, 'reverb_density': 0.70,
            'stereo_width': 1.20, 'elevation': 0.0, 'distance': 1.1,
            'eq_sub30_gain': 0.9, 'eq_sub60_gain': 1.2, 'eq_bass100_gain': 1.1,
            'eq_lowmid350_gain': -1.5, 'eq_mid700_gain': -0.5,
            'eq_presence3k_gain': 1.5, 'eq_bril8k_gain': 2.1, 'eq_air12k_gain': 2.8,
            'enable_vocal_center': False, 'instrument_enhance': False,
            'intensity_multiplier': 0.95, 'hrtf_intensity': 0.9,
        },
        # â”€â”€ South Asian â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'bollywood': {
            'name': 'Bollywood Grand',
            # Analysis: Indian/Nepali pop has highest ILD (17â€“20 dB) and medium rotation
            # Reverb: deep wet mix for the characteristic Bollywood reverb tail
            'rotation_speed': 0.09, 'bass_rotation': 0.032, 'treble_rotation': 0.155,
            'reverb_room': 0.82, 'reverb_mix': 0.90, 'reverb_density': 0.80,
            'stereo_width': 1.20, 'elevation': 0.05, 'distance': 1.1,
            'eq_sub30_gain': 1.2, 'eq_sub60_gain': 1.6, 'eq_bass100_gain': 1.3,
            'eq_lowmid350_gain': -2.0, 'eq_mid700_gain': 0.5,
            'eq_presence3k_gain': 4.0, 'eq_bril8k_gain': 5.0, 'eq_air12k_gain': 5.6,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.05, 'hrtf_intensity': 1.1,
        },
        'bhangra': {
            'name': 'Bhangra Energy',
            # Analysis fast tier: 0.10â€“0.18 Hz
            'rotation_speed': 0.14, 'bass_rotation': 0.05, 'treble_rotation': 0.238,
            'reverb_room': 0.68, 'reverb_mix': 0.84, 'reverb_density': 0.75,
            'stereo_width': 1.28, 'elevation': 0.0, 'distance': 0.95,
            'eq_sub30_gain': 1.8, 'eq_sub60_gain': 2.2, 'eq_bass100_gain': 1.8,
            'eq_lowmid350_gain': -3.5, 'eq_mid700_gain': -1.0,
            'eq_presence3k_gain': 4.5, 'eq_bril8k_gain': 5.6, 'eq_air12k_gain': 5.6,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 1.15, 'hrtf_intensity': 1.05,
        },
        'indian_classical': {
            'name': 'Indian Classical',
            'rotation_speed': 0.05, 'bass_rotation': 0.02, 'treble_rotation': 0.088,
            'reverb_room': 0.85, 'reverb_mix': 0.52, 'reverb_density': 0.88,
            'stereo_width': 1.15, 'elevation': 0.12, 'distance': 1.3,
            'eq_sub30_gain': 0.4, 'eq_sub60_gain': 0.8, 'eq_bass100_gain': 0.9,
            'eq_lowmid350_gain': -1.0, 'eq_mid700_gain': 1.0,
            'eq_presence3k_gain': 3.5, 'eq_bril8k_gain': 4.2, 'eq_air12k_gain': 5.6,
            'enable_vocal_center': True, 'instrument_enhance': True,
            'intensity_multiplier': 0.95, 'hrtf_intensity': 1.15,
        },
    }

    @staticmethod
    def select_preset(
        analysis: Dict[str, Any],
        user_genre: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Auto-select optimal preset from analysis. Returns preset dict."""
        gp = PresetLibrary.GENRE_PRESETS

        # 1. Explicit genre override
        if user_genre and user_genre in gp:
            print(f"  â†³ Auto-preset: user genre '{user_genre}'")
            return gp[user_genre].copy()

        # 2. Detected genre
        genre = analysis.get('genre', 'unknown')
        selected = gp.get(genre, gp['pop']).copy()
        print(f"  â†³ Auto-preset: detected '{genre}' â†’ '{selected['name']}'")

        # 3. Tempo-based rotation tweak
        bpm = analysis.get('bpm', 120)
        if bpm < 90:
            selected['rotation_speed'] = min(selected.get('rotation_speed', 0.15) * 0.85, 0.15)
            selected['bass_rotation']  = min(selected.get('bass_rotation', 0.08) * 0.85, 0.08)
            print(f"  â†³ Auto-adjust: slow tempo ({bpm} BPM)")
        elif bpm > 140:
            selected['rotation_speed']   = min(selected.get('rotation_speed', 0.15) * 1.15, 0.30)
            selected['treble_rotation']  = min(selected.get('treble_rotation', 0.20) * 1.15, 0.45)
            print(f"  â†³ Auto-adjust: fast tempo ({bpm} BPM)")

        # 4. Vocal protection
        if analysis.get('has_vocals') and analysis.get('vocal_prominence', 0) > 0.6:
            selected['enable_vocal_center'] = True
            selected['vocal_safe_bass']     = True
            selected['eq_mid700_gain']      = max(selected.get('eq_mid700_gain', -0.5), -0.5)
            print("  â†³ Auto-adjust: strong vocals â†’ vocal protection enabled")

        # 5. Dynamic range
        crest = analysis.get('crest_factor_db', 12)
        if crest < 6:
            selected['intensity_multiplier'] = selected.get('intensity_multiplier', 1.0) * 1.1
            print(f"  â†³ Auto-adjust: over-compressed ({crest}dB)")
        elif crest > 18:
            selected['reverb_mix'] = min(selected.get('reverb_mix', 0.3) * 1.1, 0.55)
            print(f"  â†³ Auto-adjust: very dynamic ({crest}dB)")

        # 6. Spectral balance: bass-heavy
        sub  = analysis.get('sub_bass_ratio', 0) + analysis.get('upper_sub_ratio', 0)
        bass = analysis.get('bass_ratio', 0)     + analysis.get('upper_bass_ratio', 0)
        if sub + bass > 0.45:
            selected['eq_sub30_gain']  = min(selected.get('eq_sub30_gain', 7.0) + 1.0, 10.0)
            selected['eq_sub60_gain']  = min(selected.get('eq_sub60_gain', 8.0) + 1.0, 11.0)
            print("  â†³ Auto-adjust: bass-heavy mix")

        # 7. Fake stereo widening
        if analysis.get('is_fake_stereo'):
            selected['stereo_width'] = min(selected.get('stereo_width', 1.0) * 1.25, 1.6)
            print("  â†³ Auto-adjust: fake stereo widening")

        return selected

    @staticmethod
    def get_preset_list() -> List[Dict[str, str]]:
        """Return list of presets for the frontend dropdown."""
        CATEGORY_LABEL = {
            'electronic': 'Electronic', 'house': 'Electronic', 'trance': 'Electronic',
            'rock': 'Rock', 'metal': 'Rock', 'indie': 'Rock',
            'hip_hop': 'Hip-Hop', 'trap': 'Hip-Hop', 'rnb': 'Hip-Hop',
            'classical': 'Classical', 'orchestral': 'Classical', 'acoustic': 'Classical',
            'jazz': 'Jazz', 'blues': 'Jazz',
            'pop': 'Pop', 'ballad': 'Pop',
            'ambient': 'Ambient', 'lofi': 'Ambient',
            'bollywood': 'South Asian', 'bhangra': 'South Asian', 'indian_classical': 'South Asian',
        }
        out = [{'id': 'auto', 'name': '\U0001f916 Auto-Detect', 'category': 'auto',
                'description': 'Analyze audio and select optimal preset automatically'}]
        for key, preset in PresetLibrary.GENRE_PRESETS.items():
            out.append({
                'id': key,
                'name': preset['name'],
                'category': CATEGORY_LABEL.get(key, 'Other'),
                'description': f"Optimised for {key.replace('_', ' ')} music",
            })
        return out

# ============================================================================
# TTL EVICTION â€” background sweeper for stem sessions and batch jobs
# ============================================================================
#
# stem_sessions and batch_queue previously grew indefinitely â€” every track
# processed accumulates orphaned WAV stems on disk and stale queue entries
# in memory.  This sweeper runs every 10 minutes and evicts entries older
# than TTL_HOURS (default 2 hours), unlinking associated files.

TTL_HOURS  = 2
TTL_SECS   = TTL_HOURS * 3600

async def _ttl_sweeper():
    """Background coroutine: sweeps stem_sessions and batch_queue every 10 min."""
    import time
    while True:
        await asyncio.sleep(600)   # run every 10 minutes
        now = time.time()
        cutoff = now - TTL_SECS

        # â”€â”€ Stem sessions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        expired_sessions = [
            sid for sid, ts in _session_timestamps.items()
            if ts < cutoff and sid in stem_sessions
        ]
        for sid in expired_sessions:
            stems = stem_sessions.pop(sid, {})
            for stem_path in stems.values():
                try:
                    p = Path(stem_path)
                    if p.exists():
                        p.unlink()
                    # Also remove the parent temp dir if empty
                    parent = p.parent
                    if parent.exists() and not any(parent.iterdir()):
                        parent.rmdir()
                except Exception:
                    pass
            _session_timestamps.pop(sid, None)
            print(f"  ðŸ—‘  TTL evicted stem session {sid[:8]}â€¦ ({len(stems)} files)")

        # â”€â”€ Batch jobs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        expired_batches = [
            bid for bid, ts in _session_timestamps.items()
            if ts < cutoff and bid in batch_queue
        ]
        for bid in expired_batches:
            batch_queue.pop(bid, None)
            _session_timestamps.pop(bid, None)
            print(f"  ðŸ—‘  TTL evicted batch queue {bid[:8]}â€¦")


# ============================================================================
# DEEP AUDIO ANALYZER  v6.0
# ============================================================================

class IntelligentAudioAnalyzer:
    """
    12-dimensional spectral analysis, 15-genre classifier, vocal center
    detection, key/mode detection, ITD-awareness, and a comprehensive
    8D parameter recommendation engine.
    """

    GENRE_SIGNATURES = {
        # Western
        'electronic':       {'low_ratio': (0.60, 1.0),  'centroid': (2000, 8000), 'rhythm': (0.5, 1.0)},
        'classical':        {'low_ratio': (0.25, 0.55), 'centroid': (1000, 4000), 'rhythm': (0.1, 0.5)},
        'rock':             {'low_ratio': (0.40, 0.70), 'centroid': (1500, 5000), 'rhythm': (0.4, 0.8)},
        'hip_hop':          {'low_ratio': (0.65, 1.0),  'centroid': (600,  3000), 'rhythm': (0.3, 0.7)},
        'jazz':             {'low_ratio': (0.30, 0.60), 'centroid': (1200, 4500), 'rhythm': (0.2, 0.6)},
        'pop':              {'low_ratio': (0.45, 0.70), 'centroid': (1500, 5000), 'rhythm': (0.3, 0.6)},
        'ambient':          {'low_ratio': (0.35, 0.65), 'centroid': (400,  2000), 'rhythm': (0.0, 0.3)},
        'rnb':              {'low_ratio': (0.55, 0.90), 'centroid': (800,  3500), 'rhythm': (0.3, 0.65)},
        'metal':            {'low_ratio': (0.45, 0.80), 'centroid': (2500, 7000), 'rhythm': (0.6, 1.0)},
        # South Asian
        'bollywood':        {'low_ratio': (0.45, 0.75), 'centroid': (900,  3500), 'rhythm': (0.35, 0.70)},
        'bhangra':          {'low_ratio': (0.55, 0.85), 'centroid': (800,  3000), 'rhythm': (0.50, 0.85)},
        'nepali_folk':      {'low_ratio': (0.25, 0.55), 'centroid': (1000, 4000), 'rhythm': (0.25, 0.60)},
        'ghazal':           {'low_ratio': (0.20, 0.50), 'centroid': (600,  2500), 'rhythm': (0.10, 0.45)},
        'indian_classical':  {'low_ratio': (0.25, 0.55), 'centroid': (700,  3000), 'rhythm': (0.10, 0.50)},
        'devotional':       {'low_ratio': (0.25, 0.55), 'centroid': (600,  2500), 'rhythm': (0.10, 0.50)},
    }

    BANDS = [
        ('sub_bass',    20,    60),
        ('upper_sub',   60,   100),
        ('bass',       100,   200),
        ('upper_bass', 200,   350),
        ('low_mid',    350,   700),
        ('mid',        700,  1500),
        ('upper_mid', 1500,  3000),
        ('presence',  3000,  6000),
        ('brilliance',6000, 10000),
        ('air',      10000, 20000),
    ]

    KEYS = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']

    def analyze_comprehensive(self, file_path: str) -> Dict[str, Any]:
        if not ADVANCED_ANALYSIS:
            return self._basic_analysis(file_path)
        try:
            print(f"ðŸ§  Deep Analysis v6.0 â€” {Path(file_path).name}")
            y, sr = librosa.load(file_path, sr=None, mono=False)
            y_mono = librosa.to_mono(y) if y.ndim > 1 else y
            duration = librosa.get_duration(y=y_mono, sr=sr)

            analysis: Dict[str, Any] = {
                'duration': round(duration, 2),
                'sample_rate': sr,
                'channels': 2 if y.ndim > 1 else 1,
            }

            # â”€â”€ BPM & Beat Tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            tempo, beats = librosa.beat.beat_track(y=y_mono, sr=sr)
            tempo = float(tempo[0] if isinstance(tempo, np.ndarray) else tempo)
            analysis['bpm'] = round(tempo)
            analysis['beat_positions'] = beats.tolist()[:100]

            # â”€â”€ Spectral Frame Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            S     = np.abs(librosa.stft(y_mono))
            freqs = librosa.fft_frequencies(sr=sr)
            total_energy = float(np.sum(S)) + 1e-10

            # Spectral centroid
            sc = librosa.feature.spectral_centroid(y=y_mono, sr=sr)[0]
            analysis['spectral_centroid_mean'] = float(np.mean(sc))
            analysis['spectral_centroid_std']  = float(np.std(sc))

            # Spectral flatness (0=tonal, 1=noise)
            sf_val = librosa.feature.spectral_flatness(y=y_mono)[0]
            analysis['spectral_flatness'] = float(np.mean(sf_val))

            # Spectral rolloff (frequency below which 85% of energy resides)
            rolloff = librosa.feature.spectral_rolloff(y=y_mono, sr=sr, roll_percent=0.85)[0]
            analysis['spectral_rolloff'] = float(np.mean(rolloff))
            analysis['spectral_brightness'] = min(float(np.mean(rolloff)) / (sr / 2), 1.0)

            # â”€â”€ 10-Band Energy Ratios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            band_ratios: Dict[str, float] = {}
            for name, lo, hi in self.BANDS:
                idx = (freqs >= lo) & (freqs < hi)
                e = float(np.sum(S[idx, :]))
                ratio = e / total_energy
                band_ratios[f'{name}_ratio'] = ratio
                analysis[f'{name}_ratio'] = ratio

            # Aggregate legacy fields
            analysis['bass_ratio']    = band_ratios['bass_ratio'] + band_ratios['upper_bass_ratio']
            analysis['sub_bass_ratio'] = band_ratios['sub_bass_ratio']
            analysis['mid_ratio']     = band_ratios['mid_ratio']
            analysis['hi_mid_ratio']  = band_ratios['upper_mid_ratio'] + band_ratios['presence_ratio']
            analysis['high_ratio']    = band_ratios['brilliance_ratio'] + band_ratios['air_ratio']
            analysis['low_mid_ratio'] = band_ratios['low_mid_ratio']
            analysis['air_ratio']     = band_ratios['air_ratio']

            # â”€â”€ Dynamic Range & Crest Factor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            rms = librosa.feature.rms(y=y_mono)[0]
            analysis['dynamic_range'] = float(20 * np.log10(
                np.max(rms) / (np.mean(rms) + 1e-10)))

            peak    = float(np.max(np.abs(y_mono)))
            rms_val = float(np.sqrt(np.mean(y_mono ** 2)))
            crest   = 20 * np.log10(peak / (rms_val + 1e-10))
            analysis['crest_factor_db']    = round(float(crest), 1)
            analysis['is_over_compressed'] = bool(crest < 6.0)
            analysis['loudness_headroom']  = round(20 * np.log10(1.0 / (peak + 1e-10)), 1)

            # â”€â”€ Vocal & Harmonic Separation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            y_harm, y_perc = librosa.effects.hpss(y_mono)
            h_e = float(np.sum(np.abs(y_harm)))
            p_e = float(np.sum(np.abs(y_perc)))
            analysis['has_vocals']       = bool(h_e > p_e * 0.7)
            analysis['vocal_prominence'] = h_e / (h_e + p_e + 1e-10)

            # HNR (Harmonic-to-Noise Ratio)
            hnr = h_e / (p_e + 1e-10)
            analysis['harmonic_to_noise_ratio'] = round(float(hnr), 3)
            analysis['hnr_db'] = round(float(20 * np.log10(hnr + 1e-10)), 1)

            # Vocal fundamental frequency (strongest harmonic 100â€“800 Hz)
            vocal_idx = (freqs >= 100) & (freqs < 800)
            harm_stft = np.abs(librosa.stft(y_harm))
            vocal_mag  = np.mean(harm_stft[vocal_idx, :], axis=1)
            if len(vocal_mag) > 0:
                peak_bin = int(np.argmax(vocal_mag))
                analysis['vocal_fundamental_hz'] = float(freqs[vocal_idx][peak_bin])
            else:
                analysis['vocal_fundamental_hz'] = 250.0

            # â”€â”€ Instrument Prominence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            harm_S = np.abs(librosa.stft(y_harm))
            def harm_band(lo, hi):
                idx = (freqs >= lo) & (freqs < hi)
                return float(np.sum(harm_S[idx, :])) / (total_energy + 1e-10)

            analysis['string_instrument_prominence'] = harm_band(200, 1200)
            analysis['wind_instrument_prominence']   = harm_band(300, 2500)
            analysis['perc_instrument_prominence']   = p_e / (total_energy + 1e-10)
            analysis['brass_instrument_prominence']  = harm_band(150, 900)
            analysis['sitar_sarangi_prominence']     = harm_band(300, 1800)  # South Asian

            # â”€â”€ Rhythm & Transient Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            onset_env = librosa.onset.onset_strength(y=y_mono, sr=sr)
            analysis['rhythm_complexity']      = float(np.std(onset_env))
            analysis['rhythm_complexity_norm'] = min(float(np.std(onset_env)) / 5.0, 1.0)

            onsets = librosa.onset.onset_detect(y=y_mono, sr=sr)
            onset_rate = float(len(onsets) / duration) if duration > 0 else 0.0
            analysis['onset_rate'] = round(onset_rate, 2)
            analysis['transient_density'] = (
                'sparse'   if onset_rate < 1.0 else
                'moderate' if onset_rate < 3.5 else
                'dense'
            )

            # â”€â”€ MFCC Timbral Fingerprint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            try:
                mfccs = librosa.feature.mfcc(y=y_mono, sr=sr, n_mfcc=13)
                analysis['mfcc_mean']       = np.mean(mfccs, axis=1).tolist()
                analysis['mfcc_std']        = np.std(mfccs, axis=1).tolist()
                # MFCC[1] inversely correlates with brightness
                analysis['timbre_brightness'] = float(np.mean(mfccs[1]))
                # MFCC[0] (energy), MFCC[1..3] encode spectral slope
            except Exception as e:
                print(f"  âš  MFCC failed: {e}")
                analysis['mfcc_mean'] = [0.0] * 13
                analysis['timbre_brightness'] = 0.0

            # â”€â”€ Key & Mode Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            try:
                chroma     = librosa.feature.chroma_cqt(y=y_mono, sr=sr)
                chroma_avg = np.mean(chroma, axis=1)
                chroma_n   = chroma_avg / (np.sum(chroma_avg) + 1e-10)
                key_idx    = int(np.argmax(chroma_avg))
                analysis['key']             = self.KEYS[key_idx]
                analysis['key_confidence']  = round(float(chroma_n[key_idx]), 3)

                # Major / minor classification
                maj = np.array([1,0,1,0,1,1,0,1,0,1,0,1], dtype=float)
                min_ = np.array([1,0,1,1,0,1,0,1,1,0,1,0], dtype=float)
                maj_rot  = np.roll(maj,  key_idx) / maj.sum()
                min_rot  = np.roll(min_, key_idx) / min_.sum()
                maj_corr = float(np.dot(chroma_n, maj_rot))
                min_corr = float(np.dot(chroma_n, min_rot))
                analysis['mode']           = 'major' if maj_corr > min_corr else 'minor'
                analysis['mode_strength']  = round(abs(maj_corr - min_corr), 3)
            except Exception as e:
                print(f"  âš  Key detection failed: {e}")
                analysis['key'] = 'C'; analysis['mode'] = 'unknown'; analysis['key_confidence'] = 0.0

            # â”€â”€ Tonnetz Tonal Complexity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            try:
                tonnetz = librosa.feature.tonnetz(y=y_harm, sr=sr)
                analysis['tonal_complexity'] = float(np.mean(np.std(tonnetz, axis=1)))
                analysis['tonal_movement']   = float(np.mean(np.abs(np.diff(tonnetz, axis=1))))
            except Exception as e:
                print(f"  âš  Tonnetz failed: {e}")
                analysis['tonal_complexity'] = 0.3
                analysis['tonal_movement']   = 0.1

            # â”€â”€ Stereo Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if y.ndim > 1 and y.shape[0] >= 2:
                # Interaural correlation
                min_len = min(len(y[0]), len(y[1]))
                corr = np.corrcoef(y[0, :min_len], y[1, :min_len])[0, 1]
                analysis['stereo_correlation'] = round(float(corr), 3)
                analysis['is_fake_stereo']      = bool(corr > 0.95)

                # Mid/Side balance
                mid  = (y[0] + y[1]) / 2
                side = (y[0] - y[1]) / 2
                mid_e  = float(np.mean(mid ** 2))
                side_e = float(np.mean(side ** 2))
                analysis['stereo_width_measured'] = round(
                    float(np.sqrt(side_e / (mid_e + 1e-10))), 3)
            else:
                analysis['stereo_correlation']    = 1.0
                analysis['is_fake_stereo']        = True
                analysis['stereo_width_measured'] = 0.0

            # â”€â”€ Zero Crossing Rate (noisiness / breathiness) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            zcr = librosa.feature.zero_crossing_rate(y_mono)[0]
            analysis['zero_crossing_rate'] = float(np.mean(zcr))
            analysis['breathiness']        = min(float(np.mean(zcr)) * 20.0, 1.0)

            # â”€â”€ Perceptual Sharpness Estimate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Weighted centroid above 1.5kHz vs below â€” heuristic
            sc_norm = analysis['spectral_centroid_mean'] / (sr / 2.0)
            analysis['perceptual_sharpness'] = round(float(sc_norm) * 2.0, 3)

            # â”€â”€ Energy Profile (per 2-second segment) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            analysis['energy_profile'] = self._energy_profile(y_mono, sr)

            # â”€â”€ Genre Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            analysis['genre'] = self._classify_genre(analysis)

            print(f"  â†³ genre={analysis['genre']}  key={analysis['key']} {analysis['mode']}  "
                  f"bpm={analysis['bpm']}  crest={analysis['crest_factor_db']}dB  "
                  f"HNR={analysis.get('hnr_db', '?')}dB  "
                  f"ITD-source={'fake' if analysis['is_fake_stereo'] else 'true'} stereo  "
                  f"transients={analysis['transient_density']}")

            # â”€â”€ Recommended Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            analysis['recommended_settings'] = self._optimize_parameters(analysis)
            return analysis

        except Exception as e:
            print(f"âŒ Analysis error: {e}")
            import traceback; traceback.print_exc()
            return self._basic_analysis(file_path)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Genre classifier
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _classify_genre(self, feat: Dict) -> str:
        low      = feat['bass_ratio'] + feat['sub_bass_ratio']
        centroid = feat['spectral_centroid_mean']
        rhythm   = feat.get('rhythm_complexity_norm', 0.4)

        best, best_score = 'unknown', float('inf')
        for genre, sig in self.GENRE_SIGNATURES.items():
            lo, hi     = sig['low_ratio']
            clo, chi   = sig['centroid']
            rlo, rhi   = sig.get('rhythm', (0, 1))
            d_low = max(0.0, lo - low, low - hi)
            d_cen = max(0.0, clo - centroid, centroid - chi) / 1000.0
            d_rhy = max(0.0, rlo - rhythm, rhythm - rhi)
            score = d_low + d_cen + d_rhy * 0.5
            if score < best_score:
                best_score, best = score, genre
        return best

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Energy profile
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _energy_profile(self, y: np.ndarray, sr: int) -> Dict:
        seg = sr * 2
        n   = len(y) // seg
        curve = [float(np.sqrt(np.mean(y[i*seg:(i+1)*seg]**2))) for i in range(n)]
        if not curve:
            curve = [0.0]
        return {
            'curve':       curve[:50],
            'mean':        float(np.mean(curve)),
            'std':         float(np.std(curve)),
            'has_buildup': len(curve) >= 10 and
                           bool(np.mean(curve[len(curve)//2:]) >
                                np.mean(curve[:len(curve)//2]) * 1.3),
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Parameter optimizer  (v6.0 â€” uses all new analysis fields)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _optimize_parameters(self, a: Dict) -> Dict:
        genre       = a['genre']
        bpm         = float(a.get('bpm', 120) or 120)
        bass_r      = a.get('bass_ratio', 0.2)
        sub_r       = a.get('sub_bass_ratio', 0.1)
        has_vocals  = a.get('has_vocals', False)
        voc_prom    = a.get('vocal_prominence', 0.5)
        dyn         = a.get('dynamic_range', 10)
        rhythm      = a.get('rhythm_complexity_norm', 0.4)
        string_prom = a.get('string_instrument_prominence', 0.0)
        voc_f0      = a.get('vocal_fundamental_hz', 250)
        crest       = a.get('crest_factor_db', 12.0)
        mode        = a.get('mode', 'major')
        is_fake_st  = a.get('is_fake_stereo', False)
        transients  = a.get('transient_density', 'moderate')
        hnr         = a.get('harmonic_to_noise_ratio', 1.0)
        onset_rate  = a.get('onset_rate', 2.0)

        # â”€â”€ Defaults (analysis-calibrated Feb 2026) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # rotation_speed: analysis median 0.0965 Hz across 35 confirmed 8D songs
        # reverb_room/mix: DRR mean âˆ’32.6 dB â†’ ~90% wet; large-hall echo density 0.054
        # bass/treble rotation: ILD swing sub=Â±4 dB, treble=Â±18 dB â†’ slow bass, fast HF
        p = {
            'rotation_speed':      0.10,   # median from dataset
            'reverb_room':         0.75,   # large hall (echo density â‰ˆ 0.054)
            'reverb_mix':          0.85,   # 90% wet (DRR âˆ’32.6 dB)
            'bass_rotation':       0.03,   # sub-bass barely moves
            'treble_rotation':     0.14,   # HF rotates 1.4Ã— main speed
            'stereo_width':        1.0,
            'elevation':           0.0,
            'distance':            1.0,
            'enable_vocal_center': has_vocals,
            'intensity_multiplier': 1.0,
            'vocal_safe_bass':     True,
            'instrument_enhance':  True,
            'eq_sub30_gain':      1.5,
            'eq_sub60_gain':      2.5,
            'eq_bass100_gain':    1.5,
            'eq_ubass200_gain':   0.5,
            'eq_lowmid350_gain': -1.5,
            'eq_mid700_gain':     0.0,
            'eq_umid1500_gain':   1.5,
            'eq_presence3k_gain': 3.0,
            'eq_def5k_gain':      3.5,
            'eq_bril8k_gain':     4.5,
            'eq_air12k_gain':     5.5,
            'eq_uair16k_gain':    3.5,
        }

        # â”€â”€ Genre overrides â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        genre_overrides = {
            'electronic': {
                # Analysis fast tier: 0.14â€“0.25 Hz (Tove Lo 0.253, Alan Walker Darkside 0.179)
                'rotation_speed': 0.18, 'treble_rotation': 0.28, 'bass_rotation': 0.06,
                'reverb_room': 0.75, 'reverb_mix': 0.88, 'stereo_width': 1.30,
                'eq_sub30_gain': 1.8, 'eq_sub60_gain': 2.2, 'eq_bass100_gain': 1.5,
                'eq_lowmid350_gain': -3.0, 'eq_air12k_gain': 5.5, 'eq_uair16k_gain': 3.5,
            },
            'classical': {
                # Analysis slow tier: 0.05â€“0.08 Hz, large hall reverb
                'rotation_speed': 0.07, 'treble_rotation': 0.12, 'bass_rotation': 0.025,
                'reverb_room': 0.90, 'reverb_mix': 0.92, 'elevation': 0.15,
                'eq_sub30_gain': 0.5, 'eq_sub60_gain': 1.0, 'eq_bass100_gain': 1.5,
                'eq_lowmid350_gain': -1.5, 'eq_presence3k_gain': 3.0,
                'eq_bril8k_gain': 3.5, 'eq_air12k_gain': 4.5,
            },
            'rock': {
                # Analysis: rock ~0.10â€“0.15 Hz range
                'rotation_speed': 0.13, 'bass_rotation': 0.05, 'treble_rotation': 0.22,
                'stereo_width': 1.20, 'reverb_room': 0.65, 'reverb_mix': 0.82,
                'eq_sub60_gain': 2.0, 'eq_bass100_gain': 2.5, 'eq_ubass200_gain': 1.5,
                'eq_lowmid350_gain': -3.0, 'eq_presence3k_gain': 4.0, 'eq_def5k_gain': 3.0,
            },
            'hip_hop': {
                # Analysis: hip-hop 0.10â€“0.14 Hz (CarryMinati 0.052, Post Malone 0.166)
                'rotation_speed': 0.12, 'bass_rotation': 0.04, 'treble_rotation': 0.18,
                'reverb_mix': 0.82, 'reverb_room': 0.70, 'distance': 0.80,
                'eq_sub30_gain': 2.5, 'eq_sub60_gain': 3.0, 'eq_bass100_gain': 2.0,
                'eq_lowmid350_gain': -4.0, 'eq_mid700_gain': -2.0,
                'eq_presence3k_gain': 2.5, 'eq_air12k_gain': 3.5,
            },
            'rnb': {
                'rotation_speed': 0.10, 'bass_rotation': 0.035, 'stereo_width': 1.10,
                'reverb_room': 0.78, 'reverb_mix': 0.86,
                'eq_sub60_gain': 2.5, 'eq_bass100_gain': 2.0, 'eq_ubass200_gain': 1.5,
                'eq_presence3k_gain': 2.5, 'eq_bril8k_gain': 3.5,
            },
            'metal': {
                'rotation_speed': 0.16, 'bass_rotation': 0.06, 'treble_rotation': 0.28,
                'stereo_width': 1.30, 'reverb_room': 0.60, 'reverb_mix': 0.80,
                'eq_sub60_gain': 2.5, 'eq_bass100_gain': 2.0,
                'eq_lowmid350_gain': -5.0, 'eq_mid700_gain': -3.0,
                'eq_presence3k_gain': 6.0, 'eq_def5k_gain': 4.0, 'eq_bril8k_gain': 3.0,
            },
            'jazz': {
                # Analysis: jazz 0.07â€“0.10 Hz, moderate wet
                'rotation_speed': 0.09, 'reverb_room': 0.78, 'reverb_mix': 0.85,
                'stereo_width': 1.10,
                'eq_bass100_gain': 2.0, 'eq_ubass200_gain': 1.0,
                'eq_presence3k_gain': 2.0, 'eq_def5k_gain': 1.5,
            },
            'ambient': {
                # Analysis slow tier extreme: 0.03â€“0.05 Hz, max reverb
                'rotation_speed': 0.04, 'bass_rotation': 0.015, 'treble_rotation': 0.07,
                'reverb_room': 0.95, 'reverb_mix': 0.95, 'stereo_width': 1.45, 'distance': 1.60,
                'eq_sub30_gain': 0.5, 'eq_sub60_gain': 1.0, 'eq_air12k_gain': 6.0,
                'eq_uair16k_gain': 4.0,
            },
            'bollywood': {
                # Analysis: Bollywood medium tier, highest ILD in dataset (17â€“20 dB)
                'rotation_speed': 0.09, 'bass_rotation': 0.032, 'treble_rotation': 0.155,
                'reverb_room': 0.82, 'reverb_mix': 0.90, 'stereo_width': 1.15,
                'enable_vocal_center': True,
                'eq_sub30_gain': 1.2, 'eq_sub60_gain': 1.8, 'eq_bass100_gain': 1.5,
                'eq_ubass200_gain': 0.8, 'eq_lowmid350_gain': -1.5,
                'eq_mid700_gain': 0.5,
                'eq_umid1500_gain': 2.0, 'eq_presence3k_gain': 4.0,
                'eq_def5k_gain': 3.5, 'eq_bril8k_gain': 4.5, 'eq_air12k_gain': 5.5,
                'instrument_enhance': True, 'vocal_safe_bass': True,
            },
            'bhangra': {
                # Analysis fast tier: 0.10â€“0.18 Hz
                'rotation_speed': 0.14, 'bass_rotation': 0.05, 'treble_rotation': 0.24,
                'reverb_room': 0.65, 'reverb_mix': 0.84, 'stereo_width': 1.25,
                'eq_sub30_gain': 2.0, 'eq_sub60_gain': 2.5, 'eq_bass100_gain': 2.0,
                'eq_ubass200_gain': 1.0, 'eq_lowmid350_gain': -3.0,
                'eq_presence3k_gain': 4.0, 'eq_def5k_gain': 3.0, 'eq_bril8k_gain': 4.0,
            },
            'nepali_folk': {
                # Analysis: Nepali songs medium tier, moderate ILD
                'rotation_speed': 0.09, 'bass_rotation': 0.032, 'treble_rotation': 0.155,
                'reverb_room': 0.80, 'reverb_mix': 0.88, 'stereo_width': 1.15,
                'elevation': 0.08,
                'eq_sub30_gain': 1.5, 'eq_sub60_gain': 2.0, 'eq_bass100_gain': 2.0,
                'eq_ubass200_gain': 1.0, 'eq_lowmid350_gain': -1.0,
                'eq_umid1500_gain': 2.5, 'eq_presence3k_gain': 3.5,
                'eq_def5k_gain': 2.5, 'eq_bril8k_gain': 3.5, 'eq_air12k_gain': 4.0,
                'instrument_enhance': True,
            },
            'ghazal': {
                # Analysis slow tier: 0.05â€“0.08 Hz, very wet reverb, vocal-centred
                'rotation_speed': 0.06, 'bass_rotation': 0.022, 'treble_rotation': 0.105,
                'reverb_room': 0.88, 'reverb_mix': 0.92, 'stereo_width': 1.05,
                'enable_vocal_center': True,
                'eq_sub30_gain': 0.5, 'eq_sub60_gain': 1.0, 'eq_bass100_gain': 1.5,
                'eq_ubass200_gain': 0.8, 'eq_lowmid350_gain': -1.0,
                'eq_mid700_gain': 1.5,
                'eq_umid1500_gain': 1.5, 'eq_presence3k_gain': 2.5,
                'eq_bril8k_gain': 3.0, 'eq_air12k_gain': 3.5,
                'vocal_safe_bass': True,
            },
            'indian_classical': {
                # Analysis slow tier: 0.05â€“0.08 Hz, large hall
                'rotation_speed': 0.07, 'bass_rotation': 0.025, 'treble_rotation': 0.12,
                'reverb_room': 0.90, 'reverb_mix': 0.92, 'elevation': 0.12,
                'stereo_width': 1.10,
                'eq_sub30_gain': 0.8, 'eq_sub60_gain': 1.5, 'eq_bass100_gain': 2.0,
                'eq_ubass200_gain': 1.5,
                'eq_lowmid350_gain': -1.0, 'eq_umid1500_gain': 2.0,
                'eq_presence3k_gain': 3.0, 'eq_def5k_gain': 2.0,
                'eq_bril8k_gain': 3.5, 'eq_air12k_gain': 4.5,
                'instrument_enhance': True,
            },
            'devotional': {
                'rotation_speed': 0.07, 'bass_rotation': 0.025, 'treble_rotation': 0.12,
                'reverb_room': 0.92, 'reverb_mix': 0.93, 'stereo_width': 1.20,
                'elevation': 0.10, 'enable_vocal_center': True,
                'eq_sub30_gain': 0.8, 'eq_sub60_gain': 1.5, 'eq_bass100_gain': 2.0,
                'eq_lowmid350_gain': -1.0, 'eq_umid1500_gain': 1.5,
                'eq_presence3k_gain': 2.5, 'eq_bril8k_gain': 3.0, 'eq_air12k_gain': 4.0,
                'vocal_safe_bass': True,
            },
        }

        if genre in genre_overrides:
            p.update(genre_overrides[genre])

        # â”€â”€ BPM-synced rotation speed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        bps = bpm / 60.0
        for ratio in [1.0, 0.5, 0.25, 0.125, 0.0625]:
            cand = bps * ratio
            if 0.05 <= cand <= 0.50:
                p['rotation_speed'] = round(cand, 3)
                break

        # â”€â”€ Heavy sub-bass â†’ ease reverb & bass rotation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if (sub_r + bass_r) > 0.55:
            p['bass_rotation'] = round(p['bass_rotation'] * 0.78, 3)
            p['reverb_mix']    = round(p['reverb_mix']    * 0.88, 3)

        # â”€â”€ Vocal protection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if has_vocals and voc_prom > 0.55:
            p['enable_vocal_center'] = True
            p['rotation_speed']      = round(p['rotation_speed'] * 0.85, 3)
            # Protect vocal fundamental Â± 1 octave
            voc_lo = voc_f0 * 0.6
            voc_hi = voc_f0 * 1.8
            if voc_lo < 700 < voc_hi:
                p['eq_mid700_gain']  = max(p['eq_mid700_gain'], -0.5)
            if voc_lo < 1500 < voc_hi:
                p['eq_umid1500_gain'] = min(p['eq_umid1500_gain'], 0.5)

        # â”€â”€ Dynamic range adjustments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if dyn > 16:
            p['reverb_mix']   = round(p['reverb_mix'] * 0.78, 3)
            p['stereo_width'] = round(p['stereo_width'] * 0.90, 3)
        elif dyn < 7:
            p['stereo_width']         = round(p['stereo_width'] * 1.12, 3)
            p['intensity_multiplier'] = 1.25

        # â”€â”€ Crest factor correction (over-compressed material) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if crest < 6.0:
            # Heavily compressed â€” reduce rotation speed (it'll sound better)
            p['rotation_speed'] = round(p['rotation_speed'] * 0.90, 3)
            # Boost sub-bass slightly to add missing impact
            p['eq_sub30_gain'] = round(p['eq_sub30_gain'] + 1.5, 1)
            p['eq_sub60_gain'] = round(p['eq_sub60_gain'] + 1.5, 1)
            print("  â†³ Over-compressed source detected â†’ adjusted EQ & rotation")
        elif crest > 18.0:
            # Very dynamic â†’ increase reverb mix for fuller space perception
            p['reverb_mix'] = round(min(p['reverb_mix'] * 1.15, 0.65), 3)

        # â”€â”€ Mode-aware reverb tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if mode == 'minor':
            # Minor keys benefit from slightly more reverb depth
            p['reverb_room'] = round(min(p['reverb_room'] * 1.08, 0.95), 3)
            p['reverb_mix']  = round(min(p['reverb_mix']  * 1.05, 0.65), 3)
        elif mode == 'major' and genre in ('electronic', 'pop', 'bhangra'):
            # Major + energetic â†’ slightly wider stereo
            p['stereo_width'] = round(min(p['stereo_width'] * 1.05, 1.5), 2)

        # â”€â”€ Transient density â†’ rotation adjustment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if transients == 'dense':
            p['rotation_speed'] = round(p['rotation_speed'] * 0.82, 3)
            p['bass_rotation']  = round(p['bass_rotation']  * 0.85, 3)
        elif transients == 'sparse':
            p['rotation_speed'] = round(min(p['rotation_speed'] * 1.10, 0.45), 3)

        # â”€â”€ Fake stereo â†’ compensate by widening â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if is_fake_st:
            p['stereo_width'] = round(min(p['stereo_width'] * 1.20, 1.60), 2)
            print("  â†³ Fake stereo source â†’ increased stereo width compensation")

        # â”€â”€ High HNR + low noise â†’ instrument enhance less needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if hnr > 10.0:
            # Very clean harmonic content â†’ ease off enhancement to avoid artefacts
            p['instrument_enhance'] = False

        # â”€â”€ High rhythm complexity â†’ slightly slower rotation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if rhythm > 0.75:
            p['rotation_speed'] = round(p['rotation_speed'] * 0.82, 3)

        # â”€â”€ Boost hidden strings if subdued â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if string_prom < 0.05 and p.get('instrument_enhance'):
            p['eq_umid1500_gain']   = round(p['eq_umid1500_gain'] + 1.5, 1)
            p['eq_presence3k_gain'] = round(p['eq_presence3k_gain'] + 1.0, 1)

        # â”€â”€ Source brightness pre-compensation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Estimate spectral balance from the analysis dict band ratios.
        # If the source is dark (low HF energy), boost brilliance/air/def bands
        # preemptively so the spatial engine doesn't push it further into mud.
        # Uses analysis dict ratios rather than raw numpy (those are not in scope here).
        hf_e = a.get('brilliance_ratio', 0.04) + a.get('air_ratio', 0.02)
        lf_e = (a.get('sub_bass_ratio',  0.08) + a.get('upper_sub_ratio', 0.06) +
                a.get('bass_ratio',       0.15) + a.get('upper_bass_ratio', 0.10))
        if lf_e > 1e-6:
            hf_lf_ratio = hf_e / lf_e
            if hf_lf_ratio < 0.10:
                # Source is dark â€” pre-boost HF to counteract filter-stack attenuation
                dark_boost = round(min(4.0, (0.10 - hf_lf_ratio) * 40), 1)
                p['eq_bril8k_gain']  = round(p.get('eq_bril8k_gain',  2.0) + dark_boost,       1)
                p['eq_air12k_gain']  = round(p.get('eq_air12k_gain',  2.0) + dark_boost * 1.2, 1)
                p['eq_uair16k_gain'] = round(p.get('eq_uair16k_gain', 1.0) + dark_boost * 0.8, 1)
                p['eq_def5k_gain']   = round(p.get('eq_def5k_gain',   1.5) + dark_boost * 0.5, 1)
                print(f"  â†³ Dark source detected (HF/LF={hf_lf_ratio:.3f}) â†’ pre-boosting HF by +{dark_boost} dB")
            elif hf_lf_ratio > 0.45:
                # Source is bright/harsh â€” gentle cut to prevent over-air
                p['eq_air12k_gain']  = round(p.get('eq_air12k_gain',  2.0) - 1.5, 1)
                p['eq_uair16k_gain'] = round(p.get('eq_uair16k_gain', 1.0) - 1.0, 1)

        # â”€â”€ Vocal-safe bass caps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if p.get('vocal_safe_bass'):
            p['eq_sub30_gain']   = min(p['eq_sub30_gain'],   10.0)
            p['eq_sub60_gain']   = min(p['eq_sub60_gain'],   10.0)
            p['eq_bass100_gain'] = min(p['eq_bass100_gain'],  8.0)
            p['eq_ubass200_gain'] = min(p['eq_ubass200_gain'], 3.0)

        # â”€â”€ Clamp all EQ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for k in ['eq_sub30_gain','eq_sub60_gain','eq_bass100_gain','eq_ubass200_gain',
                  'eq_lowmid350_gain','eq_mid700_gain','eq_umid1500_gain',
                  'eq_presence3k_gain','eq_def5k_gain','eq_bril8k_gain',
                  'eq_air12k_gain','eq_uair16k_gain']:
            p[k] = max(-14.0, min(14.0, round(p[k], 1)))

        return p

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Fallback (no librosa)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _basic_analysis(self, file_path: str) -> Dict:
        return {
            'duration': round(get_audio_duration(file_path), 2),
            'sample_rate': 48000, 'channels': 2,
            'bpm': None, 'genre': 'unknown',
            'key': 'C', 'mode': 'unknown', 'key_confidence': 0.0,
            'sub_bass_ratio': 0.08, 'upper_sub_ratio': 0.07,
            'bass_ratio': 0.15, 'upper_bass_ratio': 0.10,
            'low_mid_ratio': 0.12, 'mid_ratio': 0.25,
            'upper_mid_ratio': 0.12, 'presence_ratio': 0.06,
            'brilliance_ratio': 0.03, 'air_ratio': 0.02,
            'has_vocals': False, 'vocal_fundamental_hz': 250.0,
            'crest_factor_db': 12.0, 'is_over_compressed': False,
            'is_fake_stereo': True, 'stereo_correlation': 1.0,
            'transient_density': 'moderate', 'onset_rate': 2.0,
            'harmonic_to_noise_ratio': 1.0, 'hnr_db': 0.0,
            'recommended_settings': {
                'rotation_speed': 0.15, 'reverb_room': 0.60, 'reverb_mix': 0.30,
                'bass_rotation': 0.08, 'treble_rotation': 0.20,
                'stereo_width': 1.0, 'elevation': 0.0, 'distance': 1.0,
                'enable_vocal_center': False,
                'eq_sub30_gain': 3.0, 'eq_sub60_gain': 4.0,
                'eq_bass100_gain': 3.0, 'eq_ubass200_gain': 1.5,
                'eq_lowmid350_gain': -2.5, 'eq_mid700_gain': -1.0,
                'eq_umid1500_gain': 1.0, 'eq_presence3k_gain': 2.0,
                'eq_def5k_gain': 1.5, 'eq_bril8k_gain': 2.0,
                'eq_air12k_gain': 2.0, 'eq_uair16k_gain': 1.0,
                'vocal_safe_bass': True, 'instrument_enhance': True,
            }
        }


audio_analyzer = IntelligentAudioAnalyzer()


# ============================================================================
# SPATIAL QA ENGINE  v1.0 â€” Post-render output analysis & correction
# ============================================================================
#
# Analyses the RENDERED output audio (not the source) for:
#   1. LR balance         â€” ILD panning collapsed or stuck
#   2. Stereo correlation â€” effectively mono / phase inverted
#   3. Panning modulation â€” rolling cross-corr variance measures rotation depth
#   4. Pan LFO frequency  â€” rotation too fast or too slow
#   5. Spectral tilt      â€” muffled (HF attenuated) or harsh (HF boosted)
#   6. Noise floor        â€” dropout, silence, or excessive noise
#   7. Dynamic range      â€” over-compressed output
#
# Produces a QAReport with score 0â€“1 and a corrections dict.
# The corrections dict is understood by _build_qa_correction_filtergraph()
# which converts it into a second-pass FFmpeg correction chain.
# ============================================================================

class QAReport:
    """Result object from SpatialQAEngine.analyse()."""
    __slots__ = ('passed', 'score', 'issues', 'corrections', 'metrics')

    def __init__(self, passed: bool, score: float,
                 issues: List[str], corrections: Dict[str, Any],
                 metrics: Dict[str, float]):
        self.passed      = passed
        self.score       = round(score, 4)
        self.issues      = issues
        self.corrections = corrections
        self.metrics     = metrics

    def __repr__(self) -> str:
        return (f"QAReport(passed={self.passed}, score={self.score:.2f}, "
                f"issues={len(self.issues)}, corrections={list(self.corrections.keys())})")


class SpatialQAEngine:
    """
    Post-render QA engine â€” analyses output audio and proposes corrections.

    All thresholds are tuned for 8D binaural audio:
      â€¢ A good 8D track should have clear L/R alternation every few seconds.
      â€¢ LR correlation should oscillate between ~0.95 (mono-sum at centre)
        and ~-0.3 (maximum spatial separation). Rolling std of this signal
        is the 'pan modulation depth' and is the primary health indicator.
      â€¢ Spectral tilt for a mastered binaural track should be roughly
        -15 to -20 dB (HF naturally quieter than LF) â€” not -35 dB (muffled).
    """

    # â”€â”€ Thresholds (analysis-calibrated Feb 2026) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Source: 35 confirmed 8D vs 3 non-8D songs from 38-track dataset.
    # panning_movement_std: 8D avg 0.181, non-8D avg 0.086, threshold > 0.12
    # lr_ratio_std:         8D avg 0.404, non-8D avg 0.184, threshold > 0.20
    # pan_excursion:        8D avg ~1.08, non-8D avg ~0.50, threshold > 0.65
    # Avg abs HF ILD: 12.47 dB across dataset (range 2.5â€“20.3 dB)
    BALANCE_MAX_DIFF  = 0.18   # >18% RMS difference between L and R = stuck pan
    CORR_MAX          = 0.92   # >0.92 zero-lag correlation = effectively mono
    CORR_MIN          = -0.80  # <-0.80 = likely phase inversion
    MOD_DEPTH_MIN     = 0.035  # analysis: 8D avg panning_movement 0.181, non-8D 0.086
    PAN_HZ_MIN        = 0.008  # 1 rot/125s â€” very slow genres (classical/ambient) still valid
    PAN_HZ_MAX        = 1.0    # rotation faster than 1.0 Hz = dizzying
    TILT_MIN_DB       = -30.0  # 8D audio adds ~6-8 dB natural tilt; threshold accounts for this
    TILT_MAX_DB       =  4.0   # HF >4 dB above LF = harsh
    PAN_ILD_DB_MIN    =  2.0   # analysis: avg abs HF ILD = 12.5 dB; even weakest 8D = 2.5 dB
    NOISE_RATIO_MAX   = 0.95   # >95% of frames near silence = dropout / silence
    LUFS_MIN          = -35.0  # below -35 dBFS = too quiet
    LUFS_MAX          = -6.0   # above -6 dBFS = clipping risk
    DR_MIN_LU         =  3.0   # <3 LU dynamic range = over-brickwalled
    WINDOW_S          =  2.0   # rolling correlation window (seconds)
    MIN_DUR_S         =  3.0   # skip QA on clips shorter than this
    # Analysis quality thresholds for 8D validation
    QA_PAN_MOV_MIN    =  0.12  # panning_movement_std must exceed this for confirmed 8D
    QA_LR_RATIO_MIN   =  0.20  # lr_ratio_std must exceed this
    QA_EXCURSION_MIN  =  0.65  # pan_excursion must exceed this

    def analyse(self, output_path: str) -> QAReport:
        """
        Run full spatial QA on a rendered output file.

        Returns a QAReport with passed/score/issues/corrections/metrics.
        All measurements gracefully degrade: any single step failure adds
        to issues but does not abort the rest of the analysis.
        """
        if not ADVANCED_ANALYSIS:
            return QAReport(passed=True, score=1.0, issues=[],
                            corrections={}, metrics={"skipped": 1.0})

        # â”€â”€ Load â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            y, sr = librosa.load(output_path, sr=None, mono=False)
        except Exception as e:
            return QAReport(passed=False, score=0.0,
                            issues=[f"Could not load output file: {e}"],
                            corrections={}, metrics={})

        # â”€â”€ Mono guard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if y.ndim == 1 or y.shape[0] < 2:
            return QAReport(passed=False, score=0.0,
                            issues=["Output is mono â€” binaural processing failed entirely"],
                            corrections={"stereo_width_boost": 1.4, "eq_air12k_gain": 3.0},
                            metrics={"channels": 1.0})

        dur = y.shape[1] / sr
        if dur < self.MIN_DUR_S:
            return QAReport(passed=True, score=1.0, issues=[],
                            corrections={}, metrics={"duration_s": round(dur, 2)})

        L = y[0].astype(np.float32)
        R = y[1].astype(np.float32)
        metrics:     Dict[str, float] = {"duration_s": round(dur, 2)}
        issues:      List[str] = []
        corrections: Dict[str, Any]  = {}

        # â”€â”€ 1. RMS / Loudness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        rms_L     = float(np.sqrt(np.mean(L ** 2)))
        rms_R     = float(np.sqrt(np.mean(R ** 2)))
        rms_total = max(rms_L + rms_R, 1e-9)
        metrics["rms_L"]    = round(rms_L, 5)
        metrics["rms_R"]    = round(rms_R, 5)

        lufs_approx = float(20 * np.log10(max((rms_L + rms_R) / 2, 1e-9)))
        metrics["lufs_approx"] = round(lufs_approx, 2)
        if lufs_approx > self.LUFS_MAX:
            issues.append(f"Output too loud ({lufs_approx:.1f} dBFS) â€” clipping risk")
            corrections["output_gain_db"] = -3.0
        elif lufs_approx < self.LUFS_MIN:
            issues.append(f"Output too quiet ({lufs_approx:.1f} dBFS) â€” may have encoding issue")
            corrections["output_gain_db"] = min(6.0, abs(lufs_approx + 20))

        # â”€â”€ 2. LR Balance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        balance_diff = abs(rms_L - rms_R) / rms_total
        metrics["lr_balance_diff"] = round(balance_diff, 4)
        if balance_diff > self.BALANCE_MAX_DIFF:
            lean = "LEFT" if rms_L > rms_R else "RIGHT"
            issues.append(
                f"LR balance off: {lean} channel is {balance_diff*100:.1f}% louder "
                f"â€” ILD panning may be stuck or collapsed"
            )
            corrections["stereo_width_boost"] = 1.15

        # â”€â”€ 3. Zero-lag LR Correlation (spatial width) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        denom    = max(rms_L * rms_R, 1e-12)
        corr_0   = float(np.mean(L * R)) / denom
        metrics["lr_correlation"] = round(corr_0, 4)
        if corr_0 > self.CORR_MAX:
            issues.append(
                f"Channels almost identical (corr={corr_0:.3f}) â€” output is effectively "
                f"mono. Spatial processing may have been bypassed."
            )
            corrections["stereo_width_boost"] = 1.5
            corrections["_needs_regen"]        = True
        elif corr_0 < self.CORR_MIN:
            issues.append(
                f"Channels strongly anti-correlated (corr={corr_0:.3f}) â€” possible "
                f"phase inversion in one channel."
            )
            corrections["stereo_width_boost"] = 0.85

        # â”€â”€ 4. Rolling Panning Modulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        frame_n   = max(1, int(self.WINDOW_S * sr))
        n_frames  = int(dur / self.WINDOW_S) - 1
        corr_series: List[float] = []
        for k in range(max(0, n_frames)):
            s, e   = k * frame_n, (k + 1) * frame_n
            lf, rf = L[s:e].astype(np.float64), R[s:e].astype(np.float64)
            nl, nr = float(np.sqrt(np.mean(lf**2))), float(np.sqrt(np.mean(rf**2)))
            dn     = nl * nr
            corr_series.append(float(np.mean(lf * rf)) / dn if dn > 1e-9 else 1.0)

        if corr_series and len(corr_series) >= 3:
            ca         = np.array(corr_series, dtype=np.float32)
            mod_depth  = float(np.std(ca))
            corr_range = float(np.max(ca) - np.min(ca))
            metrics["pan_mod_depth"]  = round(mod_depth, 4)
            metrics["pan_corr_range"] = round(corr_range, 4)

            if mod_depth < self.MOD_DEPTH_MIN:
                issues.append(
                    f"Panning barely moves (modulation depth={mod_depth:.3f}) â€” "
                    f"the spatial rotation is too weak or not audible. "
                    f"Range was {corr_range:.3f}."
                )
                corrections["intensity_boost"] = 1.4
                corrections["stereo_width_boost"] = corrections.get("stereo_width_boost", 1.0) * 1.15
        else:
            metrics["pan_mod_depth"]  = 0.0
            metrics["pan_corr_range"] = 0.0

        # â”€â”€ 5. Pan LFO Frequency (rotation speed detection) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Primary: FFT of correlation series (sensitive to large pan swings).
        # Secondary: FFT of ILD series (L/R RMS ratio) â€” much more sensitive for
        # HRIR binaural where correlation only varies by ~0.05-0.15 across orbits
        # but ILD varies by 2-8 dB at mid/high frequencies.
        pan_hz = 0.0
        ild_pan_hz = 0.0

        # -- Primary: correlation-based --
        if len(corr_series) >= 8:
            try:
                ca_arr = np.array(corr_series, dtype=np.float32)
                ca_arr -= np.mean(ca_arr)
                freqs_qa = np.fft.rfftfreq(len(ca_arr), d=self.WINDOW_S)
                power    = np.abs(np.fft.rfft(ca_arr)) ** 2
                # Only report if the dominant peak is clearly above noise floor
                # (prevents the FFT returning the 1st bin on near-zero-variance signals)
                if len(power) > 1:
                    dom_idx   = int(np.argmax(power[1:]) + 1)
                    noise_rms = float(np.sqrt(np.mean(power[1:])))
                    peak_val  = float(power[dom_idx])
                    # Require peak â‰¥ 4Ã— noise floor to be credible
                    if noise_rms > 1e-12 and peak_val >= 4.0 * noise_rms:
                        pan_hz = float(freqs_qa[dom_idx])
            except Exception:
                pan_hz = 0.0

        # -- Secondary: ILD-based (L/R RMS ratio in dB per window) --
        # This is more sensitive for HRIR binaural audio where ILD is the
        # main pan cue rather than pure volume panning.
        ild_series: List[float] = []
        for k in range(max(0, n_frames)):
            s, e   = k * frame_n, (k + 1) * frame_n
            lf, rf = L[s:e].astype(np.float64), R[s:e].astype(np.float64)
            rms_l  = float(np.sqrt(np.mean(lf**2)))
            rms_r  = float(np.sqrt(np.mean(rf**2)))
            if rms_r > 1e-9:
                ild_series.append(20.0 * np.log10(max(rms_l / rms_r, 1e-9)))
            else:
                ild_series.append(0.0)

        if len(ild_series) >= 8:
            try:
                ild_arr = np.array(ild_series, dtype=np.float32)
                ild_std_db = float(np.std(ild_arr))
                metrics["pan_ild_std_db"] = round(ild_std_db, 3)

                ild_arr -= np.mean(ild_arr)
                freqs_ild = np.fft.rfftfreq(len(ild_arr), d=self.WINDOW_S)
                pwr_ild   = np.abs(np.fft.rfft(ild_arr)) ** 2
                if len(pwr_ild) > 1:
                    dom_ild   = int(np.argmax(pwr_ild[1:]) + 1)
                    nf_ild    = float(np.sqrt(np.mean(pwr_ild[1:])))
                    pk_ild    = float(pwr_ild[dom_ild])
                    if nf_ild > 1e-12 and pk_ild >= 4.0 * nf_ild:
                        ild_pan_hz = float(freqs_ild[dom_ild])
            except Exception:
                ild_pan_hz = 0.0

        # -- Reconcile: prefer whichever channel gives a higher (more credible) Hz --
        best_pan_hz = max(pan_hz, ild_pan_hz)
        metrics["pan_lfo_hz"]      = round(best_pan_hz, 4)
        metrics["pan_lfo_hz_corr"] = round(pan_hz, 4)
        metrics["pan_lfo_hz_ild"]  = round(ild_pan_hz, 4)

        if 0 < best_pan_hz < self.PAN_HZ_MIN:
            issues.append(
                f"Rotation too slow ({best_pan_hz:.3f} Hz = 1 rotation per {1/best_pan_hz:.0f}s) â€” "
                f"spatial movement won't be perceived as 8D."
            )
        elif best_pan_hz > self.PAN_HZ_MAX:
            issues.append(
                f"Rotation too fast ({best_pan_hz:.3f} Hz = {best_pan_hz*60:.0f} RPM) â€” "
                f"may cause listener discomfort."
            )

        # â”€â”€ 6. Spectral Tilt (muffled / harsh) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            mono_qa = (L + R) * 0.5
            n_fft   = 2048
            hop     = 512
            S_qa    = np.abs(librosa.stft(mono_qa, n_fft=n_fft, hop_length=hop))
            fq_qa   = librosa.fft_frequencies(sr=sr, n_fft=n_fft)
            lf_mask = (fq_qa >= 20)   & (fq_qa <  400)
            hf_mask = (fq_qa >= 4000) & (fq_qa < 16000)
            lf_e    = float(np.mean(S_qa[lf_mask])) if lf_mask.any() else 1e-9
            hf_e    = float(np.mean(S_qa[hf_mask])) if hf_mask.any() else 1e-9
            tilt_db = float(20.0 * np.log10(max(hf_e, 1e-9) / max(lf_e, 1e-9)))
            metrics["freq_tilt_db"] = round(tilt_db, 2)

            if tilt_db < self.TILT_MIN_DB:
                severity   = min(1.0, (abs(tilt_db) - abs(self.TILT_MIN_DB)) / 10.0)
                hf_boost   = round(2.5 + severity * 6.0, 1)   # +2.5 to +8.5 dB
                issues.append(
                    f"Audio is MUFFLED: HF is {abs(tilt_db):.1f} dB below LF "
                    f"(threshold {abs(self.TILT_MIN_DB):.0f} dB). "
                    f"Applying +{hf_boost:.1f} dB HF correction."
                )
                # Cap individual correction gains at Â±14 dB to prevent overdriving
                corrections["eq_def5k_gain"]   = round(min(hf_boost * 0.6,  12.0), 1)
                corrections["eq_bril8k_gain"]  = round(min(hf_boost + 2.0,  14.0), 1)
                corrections["eq_air12k_gain"]  = round(min(hf_boost + 3.0,  14.0), 1)
                corrections["eq_uair16k_gain"] = round(min(hf_boost + 1.0,  14.0), 1)
            elif tilt_db > self.TILT_MAX_DB:
                issues.append(
                    f"Audio is HARSH: HF is {tilt_db:.1f} dB above LF."
                )
                corrections["eq_bril8k_gain"] = -2.5
                corrections["eq_air12k_gain"] = -3.0
        except Exception as e:
            metrics["freq_tilt_db"] = -99.0
            issues.append(f"Spectral tilt analysis failed: {e}")

        # â”€â”€ 7. Dynamic Range â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            mono_qa_dr = (L + R) * 0.5
            block_n    = max(1, int(0.4 * sr))
            n_blocks   = int(len(mono_qa_dr) / block_n)
            if n_blocks >= 4:
                rms_blocks = [
                    float(20 * np.log10(max(
                        float(np.sqrt(np.mean(mono_qa_dr[k*block_n:(k+1)*block_n] ** 2))),
                        1e-9
                    )))
                    for k in range(n_blocks)
                ]
                dr_lu = float(np.std(rms_blocks))
                metrics["dr_lu"] = round(dr_lu, 2)
                if dr_lu < self.DR_MIN_LU and dur > 30.0:
                    issues.append(
                        f"Very low dynamic range ({dr_lu:.2f} LU) â€” "
                        f"output may be over-limited or clipped."
                    )

                # Noise floor: fraction of blocks below -55 dBFS
                silent_frac = sum(1 for r in rms_blocks if r < -55) / len(rms_blocks)
                metrics["noise_floor_ratio"] = round(silent_frac, 3)
                if silent_frac > self.NOISE_RATIO_MAX:
                    issues.append(
                        f"Output is mostly silence ({silent_frac*100:.0f}% silent blocks) "
                        f"â€” possible encoding failure or zero-gain filtergraph."
                    )
                    corrections["_needs_regen"] = True
        except Exception as e:
            metrics["dr_lu"] = 0.0
            issues.append(f"Dynamic range analysis failed: {e}")

        # â”€â”€ 8. Analysis-derived 8D Quality Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # These three metrics were derived from 38-song measurement campaign.
        # Non-8D tracks (stereo-widened only) consistently fall below all thresholds.
        try:
            # Measure panning_movement_std equivalent (LR ratio standard deviation)
            block_n   = max(1, int(0.5 * sr))
            n_b       = int(len(L) / block_n)
            lr_ratios = []
            excursions= []
            if n_b >= 4:
                for k in range(n_b):
                    sl = L[k*block_n:(k+1)*block_n]
                    sr_ = R[k*block_n:(k+1)*block_n]
                    rl = float(np.sqrt(np.mean(sl**2) + 1e-12))
                    rr = float(np.sqrt(np.mean(sr_**2) + 1e-12))
                    lr_ratios.append(rl / (rl + rr))
                    excursions.append(abs(rl - rr) / (rl + rr))
                pan_mov_std  = float(np.std(lr_ratios))
                lr_ratio_std = float(np.std(lr_ratios))
                pan_excursion= float(np.max(excursions) - np.min(excursions))
                metrics["qa_pan_mov_std"]  = round(pan_mov_std,  4)
                metrics["qa_lr_ratio_std"] = round(lr_ratio_std, 4)
                metrics["qa_pan_excursion"]= round(pan_excursion,4)
                fails = []
                if pan_mov_std  < self.QA_PAN_MOV_MIN:
                    fails.append(f"pan_movement_std={pan_mov_std:.3f} < {self.QA_PAN_MOV_MIN}")
                if lr_ratio_std < self.QA_LR_RATIO_MIN:
                    fails.append(f"lr_ratio_std={lr_ratio_std:.3f} < {self.QA_LR_RATIO_MIN}")
                if pan_excursion < self.QA_EXCURSION_MIN:
                    fails.append(f"pan_excursion={pan_excursion:.3f} < {self.QA_EXCURSION_MIN}")
                if fails:
                    issues.append(
                        f"8D quality check FAILED â€” output resembles widened stereo, not spatial 8D. "
                        f"Failed: {'; '.join(fails)}. Increase pan_depth or rotation_speed."
                    )
                    corrections["intensity_boost"]      = 1.5
                    corrections["stereo_width_boost"]   = corrections.get("stereo_width_boost", 1.0) * 1.2
        except Exception as e:
            metrics["qa_8d_check"] = -1.0

        # â”€â”€ Score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        penalty_map = {
            "muffled":           0.25,   # â†“ was 0.30 â€” HRIR naturally adds HF tilt
            "harsh":             0.15,
            "mono":              0.50,
            "balance off":       0.15,
            "almost identical":  0.40,
            "anti-correlated":   0.20,
            "barely moves":      0.25,   # â†“ was 0.30 â€” HRIR modulation naturally lower
            "too slow":          0.08,   # â†“ was 0.10 â€” ILD channel may compensate
            "too fast":          0.10,
            "too quiet":         0.10,
            "too loud":          0.10,
            "dynamic range":     0.10,
            "mostly silence":    0.60,
            "8d quality check":  0.30,   # analysis-derived: fails all 3 discriminators
        }
        score = 1.0
        for iss in issues:
            iss_lc = iss.lower()
            for key, pen in penalty_map.items():
                if key in iss_lc:
                    score -= pen
                    break

        # ILD-based spatial bonus: if the ILD envelope shows measurable
        # pan movement (â‰¥ 0.5 dB std across windows), partially offset penalties.
        # This rewards proper HRIR binaural where correlation-based metrics
        # are inherently lower than in pan-volume-only processing.
        ild_std_bonus = metrics.get("pan_ild_std_db", 0.0)
        if ild_std_bonus > self.PAN_ILD_DB_MIN:
            ild_reward = min(0.10, (ild_std_bonus - self.PAN_ILD_DB_MIN) / 5.0)
            score = min(1.0, score + ild_reward)

        score = max(0.0, round(score, 4))

        # Print summary
        status = "âœ… PASS" if score >= 0.75 else ("âš ï¸  WARN" if score >= 0.50 else "âŒ FAIL")
        ild_info = f"  ild_pan_std={metrics.get('pan_ild_std_db','?')}dB" if 'pan_ild_std_db' in metrics else ""
        print(f"  ðŸ” QA {status} â€” score={score:.2f}  tilt={metrics.get('freq_tilt_db','?')}dB  "
              f"corr={metrics.get('lr_correlation','?')}  modulation={metrics.get('pan_mod_depth','?')}"
              f"  pan_lfo={metrics.get('pan_lfo_hz','?')}Hz"
              f"{ild_info}")
        for iss in issues:
            print(f"     âš   {iss}")

        return QAReport(
            passed      = score >= 0.65,
            score       = score,
            issues      = issues,
            corrections = corrections,
            metrics     = metrics,
        )

def _build_qa_correction_filtergraph(corrections: Dict[str, Any]) -> str:
    """
    Convert a QAReport.corrections dict into an FFmpeg -af filter string
    for a second-pass correction run.

    Supported correction keys:
      eq_bril8k_gain    â€” high shelf @ 8 kHz  (uses highshelf filter)
      eq_air12k_gain    â€” high shelf @ 12 kHz
      eq_uair16k_gain   â€” high shelf @ 16 kHz
      eq_def5k_gain     â€” bell @ 5 kHz
      stereo_width_boost â€” stereotools mlev adjustment
      intensity_boost   â€” stereotools sbal-free wide expand
      output_gain_db    â€” overall volume gain
    """
    parts: List[str] = []

    # Bell EQ at 5 kHz
    g5k = corrections.get("eq_def5k_gain", 0.0)
    if abs(g5k) > 0.3:
        parts.append(f"equalizer=f=5000:t=q:w=2000:g={g5k:.1f}")

    # High shelves â€” use `highshelf` filter (correct for shelf EQ)
    # Clamp to Â±14 dB to avoid clipping
    shelf_map = [
        ("eq_bril8k_gain",  8000,  3000),
        ("eq_air12k_gain",  12000, 5000),
        ("eq_uair16k_gain", 16000, 6000),
    ]
    for key, freq, bw in shelf_map:
        g = corrections.get(key, 0.0)
        g = max(-14.0, min(14.0, g))
        if abs(g) > 0.3:
            parts.append(f"equalizer=f={freq}:t=h:w={bw}:g={g:.1f}")

    # Stereo width (intensity_boost folds into stereo width)
    width_boost   = corrections.get("stereo_width_boost", 1.0)
    int_boost     = corrections.get("intensity_boost",    1.0)
    combined_mlev = round(width_boost * int_boost, 3)
    combined_mlev = max(0.5, min(2.0, combined_mlev))
    if abs(combined_mlev - 1.0) > 0.04:
        parts.append(f"stereotools=mlev={combined_mlev:.3f}:softclip=1")

    # Overall gain
    gain_db = corrections.get("output_gain_db", 0.0)
    if abs(gain_db) > 0.1:
        parts.append(f"volume={gain_db:.1f}dB")

    # Always re-normalise after correction
    parts.append("loudnorm=I=-16:TP=-1.0:LRA=11:linear=true")

    # Limiter
    parts.append("alimiter=limit=1:attack=20:release=200:level=false")

    return ",".join(parts) if parts else ""


# Singleton
spatial_qa = SpatialQAEngine()


# ============================================================================
#
# Based on the psychoacoustic principles:
#   â€¢ Vocals / bass / kick â†’ centre, slow rotation (localisable at low freq)
#   â€¢ Hi-hats / cymbals â†’ wide, fast rotation (non-localisable, add space)
#   â€¢ Sustained pads â†’ slow wide rotation for immersion
#   â€¢ Transient-heavy â†’ faster, more localised movements for punch
#   â€¢ Elevation: cymbals/flutes above (+), bass/kick below (-)
# ============================================================================

class InstrumentRouter:
    """
    Maps stem/instrument class â†’ psychoacoustically optimal spatial params.
    Implements the full rule table from the design document, then applies
    BPM-sync, energy-section modulation, and key/mode adjustments.
    """

    # â”€â”€ Base parameter table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Keys: stem names as returned by Demucs / Spleeter
    # 'other' is a catch-all for piano, strings, guitar when 4-stem is used.
    #
    # Format: rotation_speed, bass_rotation, treble_rotation,
    #         stereo_width, elevation, reverb_mix, enable_vocal_center
    STEM_TABLE = {
        # â”€â”€ Vocals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'vocals': {
            'rotation_speed': 0.07,   # slow â€” keeps voice centred and intelligible
            'bass_rotation':  0.02,
            'treble_rotation':0.08,
            'stereo_width':   0.75,   # narrow â€” voice should feel inside your head
            'elevation':      0.0,
            'reverb_mix':     0.25,   # modest reverb â€” too much smears lyrics
            'reverb_room':    0.55,
            'enable_vocal_center': True,
            'instrument_enhance': False,  # vocals don't need harmonic enhancement
        },
        # â”€â”€ Drums (full kit) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'drums': {
            'rotation_speed': 0.14,   # moderate â€” kit spreads around the space
            'bass_rotation':  0.05,   # kick stays more centred
            'treble_rotation':0.30,   # hi-hats/cymbals orbit fast
            'stereo_width':   1.20,
            'elevation':      0.05,   # slight top-of-room feel
            'reverb_mix':     0.15,   # tight â€” preserve transient punch
            'reverb_room':    0.40,
            'enable_vocal_center': False,
        },
        # â”€â”€ Bass (bass guitar, synth bass) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'bass': {
            'rotation_speed': 0.04,   # very slow â€” low freq poorly localisable
            'bass_rotation':  0.03,
            'treble_rotation':0.06,
            'stereo_width':   0.80,   # fairly narrow â€” preserve mono compatibility
            'elevation':      -0.08,  # slightly below listener (psychoacoustic)
            'reverb_mix':     0.12,   # minimal reverb â€” keeps bass tight
            'reverb_room':    0.35,
            'enable_vocal_center': False,
        },
        # â”€â”€ Guitar (electric / acoustic â€” 6-stem model) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'guitar': {
            'rotation_speed': 0.12,
            'bass_rotation':  0.06,
            'treble_rotation':0.18,
            'stereo_width':   1.10,
            'elevation':      0.0,
            'reverb_mix':     0.28,
            'reverb_room':    0.60,
            'enable_vocal_center': False,
        },
        # â”€â”€ Piano (6-stem model) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'piano': {
            'rotation_speed': 0.09,
            'bass_rotation':  0.04,
            'treble_rotation':0.15,
            'stereo_width':   1.00,
            'elevation':      0.0,
            'reverb_mix':     0.28,
            'reverb_room':    0.65,
            'enable_vocal_center': False,
        },
        # â”€â”€ Other (pads, strings, synth â€” catch-all) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        'other': {
            'rotation_speed': 0.07,   # slow, wide orbits for pads and strings
            'bass_rotation':  0.03,
            'treble_rotation':0.12,
            'stereo_width':   1.40,   # wide â€” pads fill the space
            'elevation':      0.08,   # slight upward tilt (strings/pads feel "above")
            'reverb_mix':     0.42,   # lush reverb for atmosphere
            'reverb_room':    0.78,
            'enable_vocal_center': False,
        },
    }

    # â”€â”€ Psychoacoustic rules applied ON TOP of the base table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # These scale factors are applied based on overall analysis results.

    def get_stem_params(
        self,
        stem_name: str,
        base_params: ProcessingParams,
        analysis: Optional[Dict[str, Any]] = None,
        gain_db: float = 0.0,   # makeup gain from gain staging
    ) -> ProcessingParams:
        """
        Return a ProcessingParams tuned for the given stem using the
        psychoacoustic base table, modulated by analysis results.

        Priority order:
          1. User per-stem override (if set on base_params)
          2. Analysis-modulated base table value
          3. Base table default
        """
        table = self.STEM_TABLE.get(stem_name, self.STEM_TABLE['other'])

        # â”€â”€ Start from a copy of global params so EQ, bitrate, etc. carry over
        d: Dict[str, Any] = {}

        # Base spatial params from table
        for key in ['rotation_speed', 'bass_rotation', 'treble_rotation',
                    'stereo_width', 'elevation', 'reverb_mix', 'reverb_room',
                    'enable_vocal_center', 'instrument_enhance']:
            if key in table:
                d[key] = table[key]

        # â”€â”€ Apply BPM-synced rotation speed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if analysis:
            bpm = analysis.get('bpm') or 120
            bps = bpm / 60.0
            base_rot = d.get('rotation_speed', 0.10)
            for ratio in [1.0, 0.5, 0.25, 0.125, 0.0625]:
                cand = bps * ratio
                # Clamp within musically useful range per stem type
                lo, hi = self._rot_range(stem_name)
                if lo <= cand <= hi:
                    d['rotation_speed'] = round(cand, 4)
                    break

            # Treble rotation: always faster than rotation_speed
            d['treble_rotation'] = round(
                max(d.get('treble_rotation', 0.15),
                    d['rotation_speed'] * self._treble_ratio(stem_name)), 4)

            # â”€â”€ Energy-based width modulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            energy = analysis.get('dynamic_range', 12)
            if energy > 16:  # very dynamic â†’ narrow a touch
                d['stereo_width'] = round(d.get('stereo_width', 1.0) * 0.92, 3)
            elif energy < 7:  # over-compressed â†’ widen to compensate
                d['stereo_width'] = round(
                    min(d.get('stereo_width', 1.0) * 1.10, 1.60), 3)

            # â”€â”€ Minor key â†’ slightly more reverb depth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if analysis.get('mode') == 'minor':
                d['reverb_room'] = round(
                    min(d.get('reverb_room', 0.6) * 1.06, 0.92), 3)
                d['reverb_mix'] = round(
                    min(d.get('reverb_mix', 0.3) * 1.05, 0.60), 3)

        # â”€â”€ Per-stem user overrides â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        override_map = {
            'vocals': ('stem_vocals_rotation', 'stem_vocals_width',
                       'stem_vocals_elevation', 'stem_vocals_reverb'),
            'drums':  ('stem_drums_rotation',  'stem_drums_width',
                       'stem_drums_elevation',  'stem_drums_reverb'),
            'bass':   ('stem_bass_rotation_override', 'stem_bass_width',
                       'stem_bass_elevation',  'stem_bass_reverb'),
            'guitar': ('stem_guitar_rotation', 'stem_guitar_width',
                       'stem_guitar_elevation','stem_guitar_reverb'),
            'piano':  ('stem_piano_rotation',  'stem_piano_width',
                       'stem_piano_elevation', 'stem_piano_reverb'),
            'other':  ('stem_other_rotation',  'stem_other_width',
                       'stem_other_elevation', 'stem_other_reverb'),
        }
        keys_for_stem = override_map.get(stem_name, override_map['other'])
        field_names = ['rotation_speed', 'stereo_width', 'elevation', 'reverb_mix']
        for attr, field in zip(keys_for_stem, field_names):
            val = getattr(base_params, attr, None)
            if val is not None:
                d[field] = val

        # â”€â”€ Apply makeup gain from gain staging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Embed as intensity_multiplier so the HRTF engine scales the output
        if abs(gain_db) > 0.1:
            linear = 10 ** (gain_db / 20.0)
            d['intensity_multiplier'] = round(
                base_params.intensity_multiplier * linear, 4)

        # â”€â”€ Build the final params object â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        return base_params.copy(update=d)

    def _rot_range(self, stem_name: str):
        """Musically valid rotation speed range per stem type."""
        ranges = {
            'vocals':  (0.04, 0.18),
            'drums':   (0.08, 0.35),
            'bass':    (0.02, 0.08),
            'guitar':  (0.06, 0.25),
            'piano':   (0.05, 0.20),
            'other':   (0.03, 0.15),
        }
        return ranges.get(stem_name, (0.04, 0.30))

    def _treble_ratio(self, stem_name: str) -> float:
        """How much faster treble rotates vs main rotation, per stem type."""
        ratios = {
            'vocals':  1.2,
            'drums':   2.5,  # hi-hats spin much faster than the main kit orbit
            'bass':    1.3,
            'guitar':  1.6,
            'piano':   1.5,
            'other':   1.8,
        }
        return ratios.get(stem_name, 1.5)

    def estimate_stem_gain_db(
        self, stem_path: str, target_lufs: float = -23.0
    ) -> float:
        """
        Estimate makeup gain (dB) to normalise a stem to target_lufs.
        Uses a fast RMS proxy: rms_dBFS = 20Â·log10(rms), then
        gain = target_lufs âˆ’ rms_dBFS.

        Example: stem RMS = âˆ’20 dBFS, target = âˆ’23 LUFS
          â†’ gain = âˆ’23 âˆ’ (âˆ’20) = âˆ’3 dB  (reduce by 3 dB)

        Returns 0.0 if librosa is unavailable or measurement fails.
        Clamped to [âˆ’18, +12] dB to avoid boosting near-silent stems
        into audibility or over-compressing loud ones.
        """
        if not ADVANCED_ANALYSIS:
            return 0.0
        try:
            y, sr = librosa.load(stem_path, sr=None, mono=True, duration=60)
            rms = float(np.sqrt(np.mean(y ** 2) + 1e-10))
            rms_dbfs = 20 * np.log10(rms)          # RMS in dBFS (e.g. -20.0)
            gain = target_lufs - rms_dbfs            # e.g. -23 - (-20) = -3 dB
            return float(np.clip(gain, -18.0, 12.0))
        except Exception:
            return 0.0


instrument_router = InstrumentRouter()


# ============================================================================
# HRIR CONVOLUTION ENGINE  â€” v9.0  (breaks the FFmpeg filtergraph ceiling)
# ============================================================================
#
# All previous HRTF simulation was done inside FFmpeg's volume + equalizer
# filters. That architecture has a hard ceiling:
#   â€¢ ILD via volume automation â€” accurate only for a spherical head model
#   â€¢ ITD via static adelay â€” fixed integer-sample delays, no interp
#   â€¢ Pinna notches via 4 EQ bells â€” real pinna HRIRs have 200â€“300 ms
#     impulse responses with complex spectral structure impossible to model
#     with parametric EQ
#
# This engine:
#   1. Loads audio into NumPy (librosa)
#   2. For each 256-sample block, looks up the binaural HRIR for the current
#      rotation angle (beat-locked if beats available)
#   3. Convolves each block via OLA (overlap-add) with scipy.signal.fftconvolve
#   4. Outputs a float32 stereo WAV that FFmpeg ingests for reverb + EQ + encode
#
# HRIR source: synthesized from first principles using:
#   â€¢ Woodworth ITD model (head radius r=0.0875 m, c=343 m/s)
#   â€¢ Spherical head ILD model (Duda & Martens 1998)
#   â€¢ Pinna notch resonances: 4-notch comb at 8, 10.5, 13, 16 kHz
#   â€¢ Optional: load real KEMAR compact-pinnae dataset (.npy format)
#     Drop a file named "kemar_hrirs.npy" next to backend.py with shape
#     (36, 2, N) â€” 36 azimuths (0â€“350Â° step 10Â°), L/R channels, N samples.

class HRIREngine:
    """
    Block-based binaural renderer using Head-Related Impulse Responses.

    v10.1 â€” Full SOFA support (KEMAR near-field dataset):
        â€¢ Loads any *.sofa HDF5 file directly â€” no conversion step needed.
        â€¢ Builds a 3-D lookup table: (distance_cm, elevation_deg, azimuth_deg)
        â€¢ Trilinear interpolation across all three spatial dimensions.
        â€¢ Distance range : 20â€“110 cm  (dataset 1 cm steps; engine samples ~11)
        â€¢ Elevation range: âˆ’25Â° â†’ +35Â° (full KEMAR measurement range)
        â€¢ Azimuth range  : 0Â° â†’ 360Â°  (full circle, per-measurement interp)
        â€¢ Falls back to legacy .npy then synthetic if SOFA unavailable.

    Usage:
        engine = HRIREngine(sample_rate=48000)
        stereo = engine.render(
            mono,           # np.ndarray (N,) float32
            beat_times_s,   # List[float] | None
            rotation_speed, # Hz
            elevation,      # normalised [-1, 1]
            distance_m,     # metres  (0.20 â€“ 1.10 maps into SOFA range)
        )
        # stereo shape: (2, N)
    """

    BLOCK  = 256    # OLA processing block size (samples)
    HRIR_N = 512    # Target IR length after resampling / truncation
    # Legacy constants â€” still used when falling back to numpy / synthetic
    AZ_STEP = 10
    N_AZ    = 36

    def __init__(self, sample_rate: int = 48000):
        self.sr         = sample_rate
        self.using_sofa = False

        # â”€â”€ SOFA 3-D lookup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # sofa_table[dist_cm][elev_deg] = (az_arr, L_mat, R_mat)
        #   az_arr : float32 (N_az,)         sorted azimuths in degrees
        #   L_mat  : float32 (N_az, HRIR_N)  left  HRIR for each azimuth
        #   R_mat  : float32 (N_az, HRIR_N)  right HRIR for each azimuth
        self.sofa_table : Dict[int, Dict[int, tuple]] = {}
        self.sofa_dists : List[int] = []   # sorted unique loaded distances (cm)
        self.sofa_elevs : List[int] = []   # sorted unique elevations (deg)

        # â”€â”€ Legacy flat lookup (numpy / synthetic) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # hrirs[az_idx] = (L_ir, R_ir)  both shape (HRIR_N,)
        self.hrirs: Dict[int, tuple] = {}

        self._load()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Loading â€” SOFA â†’ numpy â†’ synthetic cascade

    def _load(self):
        # 1. Try ALL *.sofa files in BASE_DIR â€” merge every file found so that
        #    e.g. KEMAR_NFHRIRmea_1cm + _1.5cm combine into one richer table.
        sofa_candidates = sorted(
            list(BASE_DIR.glob("*.sofa")) + list(BASE_DIR.glob("*.SOFA")),
            key=lambda p: (0 if "mea" in p.name.lower() else 1)
        )
        if sofa_candidates:
            loaded_any = False
            for sofa_path in sofa_candidates:
                try:
                    self._load_sofa(sofa_path, merge=loaded_any)
                    loaded_any = True
                    n_dists = len(getattr(self, "sofa_dists", []))
                    print(f"  â†³ Merged {sofa_path.name} (table: {n_dists} distances)")
                except Exception as e:
                    print(f"âš   HRIR: {sofa_path.name} failed ({e}), skipping")
            if loaded_any:
                return

        # 2. Legacy numpy  (36 Ã— 2 Ã— N  or  36 Ã— N Ã— 2)
        kemar_path = BASE_DIR / "kemar_hrirs.npy"
        if kemar_path.exists():
            try:
                data = np.load(str(kemar_path))
                if data.ndim == 3:
                    n = data.shape[0]
                    for i in range(n):
                        if data.shape[1] == 2:
                            self.hrirs[i] = (data[i, 0].astype(np.float32),
                                             data[i, 1].astype(np.float32))
                        else:
                            self.hrirs[i] = (data[i, :, 0].astype(np.float32),
                                             data[i, :, 1].astype(np.float32))
                    self.AZ_STEP = 360 // n
                    self.N_AZ    = n
                    print(f"âœ… HRIR: Loaded numpy dataset ({n} azimuths)")
                    return
            except Exception as e:
                print(f"âš   HRIR: numpy load failed ({e})")

        # 3. Synthetic model
        self._build_synthetic()
        print(f"âœ… HRIR: Synthetic model built ({self.N_AZ} azimuths @ {self.sr} Hz)")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # SOFA loader

    def _load_sofa(self, path: Path, merge: bool = False):
        """
        Load a SOFA (HDF5) HRTF dataset and build (or extend) the 3-D lookup table.

        merge=False: replaces any existing table (first file).
        merge=True:  adds this file's distances to the existing table so that
                     multiple SOFA files (e.g. near-field at different distances)
                     are combined into one richer lookup.

        Coordinate convention assumed:
          SourcePosition[:, 0] = azimuth  (degrees, 0=front, 90=right)
          SourcePosition[:, 1] = elevation (degrees)
          SourcePosition[:, 2] = distance  (metres â€” SOFA standard)
        """
        if not H5PY_AVAILABLE:
            raise RuntimeError("h5py not installed â€” pip install h5py")
        try:
            import h5py
        except ImportError:
            raise RuntimeError("h5py required for SOFA â€” pip install h5py")
        try:
            from scipy.signal import resample_poly
        except ImportError:
            raise RuntimeError("scipy required for SOFA â€” pip install scipy")
        from math import gcd

        print(f"  âŸ³ HRIR: Loading {path.name} â€¦")

        with h5py.File(str(path), 'r') as f:
            ir_data   = f['Data.IR'][:]            # (M, R, N_orig)
            positions = f['SourcePosition'][:]      # (M, 3)
            sr_sofa   = int(np.squeeze(f['Data.SamplingRate'][:]))

        M, R, N_orig = ir_data.shape
        print(f"  â†³ {M:,} measurements  SR={sr_sofa} Hz  IR={N_orig} smp  "
              f"receivers={R}")

        # â”€â”€ Coordinate parse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        az_raw   = positions[:, 0] % 360.0
        elev_raw = positions[:, 1]
        dist_raw = positions[:, 2]

        # Convert to cm (SOFA stores metres; guard against files already in cm)
        dist_cm_f = dist_raw * 100.0 if dist_raw.max() < 5.0 else dist_raw.copy()

        # â”€â”€ Resampling setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        needs_resample = (sr_sofa != self.sr)
        if needs_resample:
            g    = gcd(self.sr, sr_sofa)
            up   = self.sr   // g
            down = sr_sofa   // g
            n_rs = int(np.ceil(N_orig * up / down))
            print(f"  â†³ Resampling {sr_sofa}â†’{self.sr} Hz  (Ã—{up}/Ã·{down})")
        else:
            up = down = 1
            n_rs = N_orig

        target_n = min(self.HRIR_N, n_rs)

        # â”€â”€ Choose subset of distances to load â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        all_dist_cm = np.unique(np.round(dist_cm_f).astype(int))
        n_dists     = len(all_dist_cm)
        if n_dists > 1:
            # Sample ~11 distances evenly (always include endpoints)
            n_load = min(11, n_dists)
            idx    = np.round(np.linspace(0, n_dists - 1, n_load)).astype(int)
            load_set = set(all_dist_cm[np.unique(idx)].tolist())
        else:
            load_set = set(all_dist_cm.tolist())

        # â”€â”€ Quantise elevation + azimuth to dataset grid â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Round elevation to nearest 5Â°  (KEMAR dataset is at 5Â° steps)
        # Round azimuth   to nearest 5Â°
        elev_q = (np.round(elev_raw / 5.0) * 5.0).astype(int)
        az_q   = (np.round(az_raw   / 5.0) * 5.0 % 360).astype(int)
        dist_q =  np.round(dist_cm_f).astype(int)

        unique_elevs = sorted(set(elev_q.tolist()))
        print(f"  â†³ Elevations: {unique_elevs[0]}Â° â†’ {unique_elevs[-1]}Â°  "
              f"({len(unique_elevs)} levels)")
        print(f"  â†³ Loading {len(load_set)}/{n_dists} distances  "
              f"({min(load_set)}â€“{max(load_set)} cm) â€¦")

        table: Dict[int, Dict[int, tuple]] = {}
        n_irs_loaded = 0

        for d_cm in sorted(load_set):
            table[d_cm] = {}
            for e_deg in unique_elevs:
                mask    = (dist_q == d_cm) & (elev_q == e_deg)
                indices = np.where(mask)[0]
                if len(indices) == 0:
                    continue

                az_vals = az_q[indices]
                order   = np.argsort(az_vals)
                az_sorted    = az_vals[order].astype(np.float32)
                meas_sorted  = indices[order]

                L_list: List[np.ndarray] = []
                R_list: List[np.ndarray] = []

                for m_idx in meas_sorted:
                    # Use only the first two receivers (L=0, R=1)
                    ir_L = ir_data[m_idx, 0].astype(np.float64)
                    ir_R = ir_data[m_idx, 1].astype(np.float64)

                    if needs_resample:
                        ir_L = resample_poly(ir_L, up, down)
                        ir_R = resample_poly(ir_R, up, down)

                    # Truncate / zero-pad to target_n
                    L_t = np.zeros(target_n, dtype=np.float32)
                    R_t = np.zeros(target_n, dtype=np.float32)
                    cn  = min(target_n, len(ir_L))
                    L_t[:cn] = ir_L[:cn].astype(np.float32)
                    R_t[:cn] = ir_R[:cn].astype(np.float32)

                    L_list.append(L_t)
                    R_list.append(R_t)
                    n_irs_loaded += 1

                table[d_cm][e_deg] = (
                    az_sorted,
                    np.stack(L_list),   # (N_az, target_n)
                    np.stack(R_list),
                )

        if merge and hasattr(self, 'sofa_table') and self.sofa_table:
            # Merge new distances into existing table; don't overwrite existing ones
            for d_cm_new, e_dict in table.items():
                if d_cm_new not in self.sofa_table:
                    self.sofa_table[d_cm_new] = e_dict
                else:
                    # Distance already present â€” merge elevation slots
                    for e_deg, slot in e_dict.items():
                        if e_deg not in self.sofa_table[d_cm_new]:
                            self.sofa_table[d_cm_new][e_deg] = slot
            self.sofa_dists = sorted(self.sofa_table.keys())
            # Keep elevation list as union of both
            merged_elevs = sorted(set(self.sofa_elevs) | set(unique_elevs))
            self.sofa_elevs = merged_elevs
        else:
            self.sofa_table  = table
            self.sofa_dists  = sorted(table.keys())
            self.sofa_elevs  = unique_elevs
        self.HRIR_N      = target_n
        self.using_sofa  = True

        mem_mb = n_irs_loaded * 2 * target_n * 4 / 1024**2
        print(f"âœ… HRIR: SOFA loaded â€” {n_irs_loaded:,} IRs  "
              f"({len(self.sofa_dists)} dists Ã— {len(unique_elevs)} elevs)  "
              f"IR={target_n} taps @ {self.sr} Hz  â‰ˆ{mem_mb:.0f} MB")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Synthetic HRIR generation

    def _build_synthetic(self):
        for az_idx in range(self.N_AZ):
            az_deg = az_idx * self.AZ_STEP
            L, R = self._synthesize_hrir(az_deg)
            self.hrirs[az_idx] = (L, R)

    def _synthesize_hrir(self, az_deg: float) -> tuple:
        """
        Synthesise a binaural HRIR pair â€” v10.2 full perceptual model.

        Corrects three fundamental flaws in the previous model:

        FLAW 1 â€” Woodworth ITD formula misapplication:
          Old code used raw az_rad in `(r/c)*(az_rad + sin(az_rad))`, which
          gives non-zero ITD at 180Â° (back). Correct Woodworth uses the
          lateral angle Î¸ from the median sagittal plane (0 at front/back,
          Ï€/2 at 90Â° side). Fixed: use arcsin(|sin(az_rad)|).

        FLAW 2 â€” No front/back spectral differentiation:
          At 0Â° (front) and 180Â° (back), lateral=0 so ILD=0 and ITD=0 â€”
          both positions produced IDENTICAL HRIRs. This makes circular
          rotation sound like a simple L/R sweep because there are no perceptual
          cues distinguishing front from back. Fixed: add pinna-inspired spectral
          coloring that changes as the source moves frontâ†’sideâ†’back.

        FLAW 3 â€” ILD too aggressive in midrange (âˆ’15 dB above 1 kHz):
          Musical content is dominated by 200 Hzâ€“3 kHz. With 15 dB attenuation
          there, the contralateral channel nearly disappears during side panning,
          creating a harsh "mute-then-appear" tremolo rather than smooth rotation.
          Fixed: frequency-matched ILD profile (0 dB below 200 Hz, 6 dB at 1 kHz,
          12 dB at 4 kHz, 15 dB at 8 kHz+).

        The result: convincing circular 3D movement rather than L/R volume pulsing.
        """
        sr  = self.sr
        N   = self.HRIR_N
        r   = 0.0875    # head radius (m)
        c   = 343.0     # speed of sound (m/s)

        az_norm = float(az_deg % 360.0)
        az_rad  = np.radians(az_norm)

        # Sine/cosine of azimuth:
        #   sin(az) > 0 â†’ source on right;  sin(az) < 0 â†’ source on left
        #   cos(az) > 0 â†’ source in front;  cos(az) < 0 â†’ source behind
        sin_az  = float(np.sin(az_rad))
        cos_az  = float(np.cos(az_rad))

        # True lateral angle: 0 at front/back, 1.0 at 90Â° side
        lateral = abs(sin_az)          # 0..1
        # Front-back factor: +1 at front (0Â°), 0 at sides (Â±90Â°), -1 at back (180Â°)
        front_back = cos_az            # [-1, +1]

        # â”€â”€ 1. ITD â€” corrected Woodworth formula â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Î¸_lateral: angle from median sagittal plane (0 at front/back, Ï€/2 at sides)
        theta_lat = np.arcsin(min(lateral, 1.0))   # 0..Ï€/2
        itd_magnitude = (r / c) * (theta_lat + np.sin(theta_lat))   # 0..~0.65ms
        # Sign: right side â†’ right ear leads (positive), left side â†’ left ear leads
        itd_signed  = itd_magnitude * (1.0 if sin_az >= 0 else -1.0)
        itd_samples = int(round(itd_signed * sr))
        itd_samples = max(-N // 4, min(N // 4, itd_samples))

        # â”€â”€ 2. Base impulse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        L_ir = np.zeros(N, dtype=np.float64)
        R_ir = np.zeros(N, dtype=np.float64)
        onset = 4
        if itd_samples >= 0:
            R_ir[onset]               = 1.0   # right leads
            L_ir[onset + itd_samples] = 1.0
        else:
            L_ir[onset]                = 1.0  # left leads
            R_ir[onset - itd_samples]  = 1.0

        # â”€â”€ 3. Frequency-domain filter design â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        freqs   = np.fft.rfftfreq(N, d=1.0 / sr)
        n_freqs = len(freqs)

        # 3a. ILD â€” frequency-matched acoustic head shadow on contralateral ear.
        #     TUNED FOR MUSIC: KEMAR values at 90Â° are physically correct but
        #     in a music context, 27 dB HF attenuation creates a "shutter" effect:
        #     the far ear sounds DEAD at the sides â†’ feels like L/R channel switching.
        #     YouTube 8D uses moderate ILD so sound never fully disappears from either ear.
        #     Sweet spot: enough ILD to feel spatial, not so much that far ear goes silent.
        ILD_BREAKPOINTS = [
            (  100,  0.0),  # sub: zero (sub-bass is omnidirectional)
            (  300,  2.0),  # low-mid: very mild
            (  700,  5.0),  # mid: moderate â€” audible localization
            ( 1500,  8.0),  # upper-mid: clear L/R difference without dropout
            ( 3000, 11.0),  # presence: significant but not muffling
            ( 5000, 13.0),  # brilliance: strong, realistic
            ( 8000, 15.0),  # air: max â€” enough for spatial, not so much far ear dies
        ]
        H_ild = np.ones(n_freqs, dtype=np.float64)
        for fi, f in enumerate(freqs):
            if f <= ILD_BREAKPOINTS[0][0]:
                ild_db = 0.0
            elif f >= ILD_BREAKPOINTS[-1][0]:
                ild_db = ILD_BREAKPOINTS[-1][1]
            else:
                for j in range(len(ILD_BREAKPOINTS) - 1):
                    f0, db0 = ILD_BREAKPOINTS[j]
                    f1, db1 = ILD_BREAKPOINTS[j + 1]
                    if f0 <= f <= f1:
                        t = (f - f0) / (f1 - f0)
                        ild_db = db0 + t * (db1 - db0)
                        break
            H_ild[fi] = 10.0 ** (-ild_db * lateral / 20.0)

        # 3b. Front-back coloring â€” the KEY cue for circular 8D movement.
        #     Applied to BOTH channels so the overall timbre changes as the source
        #     moves frontâ†’back, regardless of which channel is louder.
        #
        #     FRONT (cos_az = +1):  crisp, present, forward-sounding
        #       â€¢ +5 dB concha resonance at 3 kHz â†’ clear, projecting
        #       â€¢ +3 dB at 5 kHz                   â†’ bright, forward
        #       â€¢ +2 dB pinna peak at 8-9 kHz       â†’ characteristic front sparkle
        #
        #     BACK (cos_az = -1):  warm, veiled, distance-behind feel
        #       â€¢ Shoulder/torso shadow: -2 dB dip at 400-600 Hz
        #       â€¢ Gentle HF rolloff above 3 kHz (âˆ’8 dB at 16 kHz)
        #       â€¢ Deep notch at 10â€“11 kHz (-10 dB): classic HRTF rear signature
        #       â€¢ +2 dB around 1â€“1.5 kHz: ear canal diffraction from rear
        H_fb = np.ones(n_freqs, dtype=np.float64)
        back_amt  = max(0.0, -front_back)    # 0 at front/sides, 1 at back
        front_amt = max(0.0,  front_back)    # 1 at front, 0 at sides/back

        # â”€â”€ Pre-compute rear makeup gain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # H_fb attenuates BOTH channels (unlike ILD which only cuts the far ear).
        # Without compensation the orbit gets audibly quieter at the rear,
        # producing the "volume goes to 0" artefact.
        # We apply a broadband makeup gain proportional to back_amt so the
        # overall RMS stays constant throughout the full orbit.
        # ~3 dB total RMS loss at 180Â° is compensated 1:1.
        rear_rms_loss_db = 3.0 * back_amt   # 0 dB at front/sides, +3 dB at rear
        rear_makeup = 10.0 ** (rear_rms_loss_db / 20.0)

        for fi, f in enumerate(freqs):
            # Front: concha resonance +5 dB at 3 kHz, presence +3 dB at 5 kHz,
            #        pinna peak +2 dB around 8.5 kHz
            front_3k  = front_amt * 5.0 * np.exp(-((f - 3000) / 1500.0) ** 2)
            front_5k  = front_amt * 3.0 * np.exp(-((f - 5000) / 2200.0) ** 2)
            front_9k  = front_amt * 2.0 * np.exp(-((f - 8500) / 1800.0) ** 2)

            # Back: torso/shoulder shadow dip at 500 Hz (blocks direct path from rear)
            back_torso = -1.5 * back_amt * np.exp(-((f - 500) / 300.0) ** 2)

            # Back HF rolloff: gentle shelf above 3 kHz, capped at -5 dB max.
            # BUG FIX: the old formula `(f - 3000) / 13000` was unbounded â€”
            # at 22 kHz it reached -11.7 dB, far exceeding the intended ceiling.
            # Now clamped to [0, 1] so the rolloff tops out at exactly -5 dB.
            hf_ramp = min(1.0, max(0.0, (f - 3000.0) / 13000.0))
            back_hf_cut = -5.0 * back_amt * hf_ramp

            # Back pinna notch at 10.5 kHz (dominant rear cue in HRTF literature).
            # Reduced from -10 dB to -6 dB: the old depth, stacked with
            # back_hf_cut, produced up to -14 dB on BOTH channels at 10 kHz
            # turning the rear passage into near-silence on musical content.
            back_notch  = -6.0 * back_amt * np.exp(-((f - 10500) / 1200.0) ** 2)

            # Back mid bump: ear canal diffraction from rear at 1.3 kHz
            back_mid_bmp = +2.0 * back_amt * np.exp(-((f - 1300) / 900.0) ** 2)

            total_db = (front_3k + front_5k + front_9k
                        + back_torso + back_hf_cut + back_notch + back_mid_bmp)
            H_fb[fi] = 10.0 ** (total_db / 20.0) * rear_makeup

        # â”€â”€ 4. Apply filters in frequency domain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        L_F = np.fft.rfft(L_ir)
        R_F = np.fft.rfft(R_ir)

        if itd_samples >= 0:
            # Right side: R = ipsilateral (no ILD), L = contralateral (ILD applied)
            R_F *= H_fb                  # front-back coloring on ipsi
            L_F *= H_fb * H_ild          # front-back coloring + ILD on contra
        else:
            # Left side: L = ipsilateral, R = contralateral
            L_F *= H_fb
            R_F *= H_fb * H_ild

        # Front pinna notch on contralateral ear (standard binaural cue)
        # At ~8-9 kHz the pinna creates a characteristic notch for lateral-front sources.
        # This helps distinguish front from side â€” a subtle but important depth cue.
        if lateral > 0.05 and front_amt > 0.05:
            notch_db = -8.0 * lateral * front_amt   # âˆ’8 dB max at 90Â° front (was -5)
            A_notch  = 10.0 ** (notch_db / 20.0)
            H_notch  = np.ones(n_freqs, dtype=np.float64)
            for fi, f in enumerate(freqs):
                # Gaussian notch centered at 8500 Hz with ~1kHz width
                bpf = np.exp(-((f - 8500.0) / 900.0) ** 2)
                H_notch[fi] = 1.0 + (A_notch - 1.0) * bpf
            if itd_samples >= 0:
                L_F *= H_notch   # contra = left
            else:
                R_F *= H_notch   # contra = right

        L_ir = np.fft.irfft(L_F, n=N).astype(np.float64)
        R_ir = np.fft.irfft(R_F, n=N).astype(np.float64)

        # â”€â”€ 5. Raised-cosine window â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        win = np.ones(N, dtype=np.float64)
        taper = 32
        win[:taper]  = 0.5 - 0.5 * np.cos(np.pi * np.arange(taper)       / taper)
        win[-taper:] = 0.5 - 0.5 * np.cos(np.pi * np.arange(taper, 0, -1) / taper)
        L_ir *= win
        R_ir *= win

        return L_ir.astype(np.float32), R_ir.astype(np.float32)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # HRIR lookup â€” 3-D trilinear interpolation

    @staticmethod
    def _bracket(arr: List[int], val: float) -> tuple:
        """
        Return (lo, hi, t) for linear interpolation in a sorted list.
        Clamps to [arr[0], arr[-1]].
        """
        if not arr:
            return 0, 0, 0.0
        if val <= arr[0]:
            return arr[0], arr[0], 0.0
        if val >= arr[-1]:
            return arr[-1], arr[-1], 0.0
        for i in range(len(arr) - 1):
            if arr[i] <= val <= arr[i + 1]:
                span = float(arr[i + 1] - arr[i])
                t    = (val - arr[i]) / span if span > 0.0 else 0.0
                return arr[i], arr[i + 1], float(np.clip(t, 0.0, 1.0))
        return arr[-1], arr[-1], 0.0

    def _interp_azimuth(
        self,
        az_arr: np.ndarray,   # float32 (N_az,)  sorted azimuths
        L_mat:  np.ndarray,   # float32 (N_az, HRIR_N)
        R_mat:  np.ndarray,
        az_deg: float,
    ) -> tuple:
        """
        Circular linear interpolation between adjacent azimuth measurements.
        Handles the 355Â° â†’ 0Â° wrap correctly.
        """
        az_norm = float(az_deg % 360.0)
        n       = len(az_arr)

        # Find insertion point; wrap index for circular lookup
        idx_hi = int(np.searchsorted(az_arr, az_norm)) % n
        idx_lo = (idx_hi - 1) % n

        az_lo = float(az_arr[idx_lo])
        az_hi = float(az_arr[idx_hi])

        # Unwrap high side when wrapping past 360Â°
        if idx_hi == 0 or az_hi < az_lo:
            az_hi += 360.0
            az_w   = az_norm if az_norm >= az_lo else az_norm + 360.0
        else:
            az_w   = az_norm

        span = max(az_hi - az_lo, 1e-6)
        t    = float(np.clip((az_w - az_lo) / span, 0.0, 1.0))

        L = ((1.0 - t) * L_mat[idx_lo] + t * L_mat[idx_hi]).astype(np.float32)
        R = ((1.0 - t) * R_mat[idx_lo] + t * R_mat[idx_hi]).astype(np.float32)
        return L, R

    def _sofa_slice(
        self, d_cm: int, e_deg: int, az_deg: float
    ) -> Optional[tuple]:
        """
        Return azimuth-interpolated HRIR pair at an exact (dist, elev) grid
        point.  Returns None if that combination is missing from the table.
        """
        d_tbl = self.sofa_table.get(d_cm)
        if d_tbl is None:
            return None
        slot = d_tbl.get(e_deg)
        if slot is None:
            return None
        az_arr, L_mat, R_mat = slot
        return self._interp_azimuth(az_arr, L_mat, R_mat, az_deg)

    def _hrir_for_position(
        self,
        az_deg:   float,
        elev_deg: float = 0.0,
        dist_cm:  float = 100.0,
    ) -> tuple:
        """
        Trilinear interpolation in (azimuth Ã— elevation Ã— distance) space
        using the SOFA lookup table.

        Falls back to legacy _hrir_for_angle() when SOFA is not loaded.
        """
        if not self.using_sofa:
            return self._hrir_for_angle(az_deg)

        d_lo, d_hi, td = self._bracket(self.sofa_dists, dist_cm)
        e_lo, e_hi, te = self._bracket(self.sofa_elevs, elev_deg)

        # 4 bilinear corners in (dist, elev) â€” azimuth interpolation is done
        # inside _sofa_slice for each corner
        corners = [(d_lo, e_lo), (d_lo, e_hi), (d_hi, e_lo), (d_hi, e_hi)]
        weights = [
            (1.0 - td) * (1.0 - te),
            (1.0 - td) *        te,
                   td  * (1.0 - te),
                   td  *        te,
        ]

        L_acc = np.zeros(self.HRIR_N, dtype=np.float64)
        R_acc = np.zeros(self.HRIR_N, dtype=np.float64)
        w_sum = 0.0

        for (d, e), w in zip(corners, weights):
            if w < 1e-7:
                continue
            result = self._sofa_slice(d, e, az_deg)
            if result is None:
                continue          # missing measurement â€” skip this corner
            L_c, R_c = result
            L_acc += w * L_c.astype(np.float64)
            R_acc += w * R_c.astype(np.float64)
            w_sum += w

        if w_sum < 1e-7:
            return self._hrir_for_angle(az_deg)   # all corners missing

        # Re-normalise if any corner was absent
        if w_sum < 0.999:
            L_acc /= w_sum
            R_acc /= w_sum

        return L_acc.astype(np.float32), R_acc.astype(np.float32)

    def _hrir_for_angle(self, az_deg: float) -> tuple:
        """
        Legacy azimuth-only lookup for the numpy / synthetic fallback path.
        Linear interpolation between adjacent azimuths.
        """
        az_norm = az_deg % 360
        idx_f   = az_norm / self.AZ_STEP
        idx_lo  = int(idx_f) % self.N_AZ
        idx_hi  = (idx_lo + 1) % self.N_AZ
        t       = idx_f - int(idx_f)

        L_lo, R_lo = self.hrirs[idx_lo]
        L_hi, R_hi = self.hrirs[idx_hi]

        L = ((1.0 - t) * L_lo + t * L_hi).astype(np.float32)
        R = ((1.0 - t) * R_lo + t * R_hi).astype(np.float32)
        return L, R

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Angle trajectory

    def _angle_trajectory(
        self,
        n_samples: int,
        rotation_speed: float,
        beat_times_s: Optional[List[float]],
    ) -> np.ndarray:
        """
        Return azimuth in degrees for every sample.
        Beat-locked: tracks actual beat timing from librosa.
        Free-running: simple ramp at rotation_speed Hz.
        """
        t = np.arange(n_samples, dtype=np.float64) / self.sr

        if beat_times_s and len(beat_times_s) >= 2:
            # Piecewise linear beat phase (same algorithm as _beat_phase_expr)
            beat_phase = np.zeros(n_samples, dtype=np.float64)
            times = beat_times_s
            for k in range(len(times) - 1):
                t_k   = times[k]
                t_nxt = times[k + 1]
                dur   = max(t_nxt - t_k, 0.01)
                mask  = (t >= t_k) & (t < t_nxt)
                beat_phase[mask] = k + (t[mask] - t_k) / dur
            # Extrapolate after last beat
            dt_last = max(times[-1] - times[-2], 0.01)
            mask_tail = t >= times[-1]
            beat_phase[mask_tail] = (
                (len(times) - 1) + (t[mask_tail] - times[-1]) / dt_last
            )
            # Scale: rotation_speed / beat_freq rotations per beat
            median_ibi = np.median(np.diff(times[:min(len(times), 40)]))
            beat_freq  = 1.0 / max(median_ibi, 0.05)
            speed_ratio = rotation_speed / beat_freq
            az_deg = (beat_phase * speed_ratio * 360.0) % 360.0
        else:
            az_deg = (rotation_speed * t * 360.0) % 360.0

        return az_deg

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Main render

    def render(
        self,
        mono:           np.ndarray,
        beat_times_s:   Optional[List[float]],
        rotation_speed: float,
        elevation:      float = 0.0,
        distance_m:     float = 1.0,
    ) -> np.ndarray:
        """
        Convolve mono audio with a time-varying binaural HRIR.

        Arguments:
          mono           : (N,) float32/64 â€” mono audio
          beat_times_s   : beat onsets in seconds (None â†’ free-running LFO)
          rotation_speed : Hz (source orbits/second)
          elevation      : normalised [-1, 1]
                             +1 â†’ top of SOFA elevation range (e.g. +35Â°)
                             -1 â†’ bottom of SOFA elevation range (e.g. -25Â°)
          distance_m     : virtual source distance in metres.
                             0.20â€“1.10 maps directly into SOFA data range.
                             Values outside range are clamped.

        Returns:
          stereo : float32 (2, N)  â€” [0] = Left,  [1] = Right
        """
        if not ADVANCED_ANALYSIS:
            raise RuntimeError("HRIREngine requires scipy (pip install scipy)")

        from scipy.signal import fftconvolve

        n_samples = len(mono)
        az_traj   = self._angle_trajectory(n_samples, rotation_speed, beat_times_s)

        # â”€â”€ Map (elevation, distance) â†’ SOFA coordinates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if self.using_sofa:
            d_min = float(self.sofa_dists[0])
            d_max = float(self.sofa_dists[-1])
            dist_cm = float(np.clip(distance_m * 100.0, d_min, d_max))

            e_min = float(self.sofa_elevs[0])   # e.g. -25
            e_max = float(self.sofa_elevs[-1])   # e.g. +35
            # Positive elevation â†’ towards top of range; negative â†’ towards bottom
            elev_deg = float(
                np.clip(elevation * (e_max if elevation >= 0.0 else -e_min),
                        e_min, e_max)
            )
        else:
            # Synthetic/numpy path: simulate elevation by scaling lateral spread
            dist_cm  = distance_m * 100.0
            elev_deg = elevation * 90.0   # unused in legacy path

        # â”€â”€ OLA (overlap-add) block processing with HRIR cross-fading â”€â”€â”€â”€â”€â”€â”€â”€
        # Cross-fading between adjacent HRIRs eliminates "azimuth snap" clicks
        # at block boundaries, producing the smooth circular motion that defines
        # professional 8D audio.
        #
        # Algorithm:
        #   â€¢ Block size = 512 samples (â‰ˆ10.7 ms at 48 kHz) â€” large enough for
        #     perceptual continuity, small enough for fast azimuth tracking.
        #   â€¢ Each block is convolved with a weighted blend of the *previous*
        #     HRIR (fade-out) and *current* HRIR (fade-in), using a raised-cosine
        #     (Hann) crossfade profile across the block.
        #   â€¢ This is equivalent to OLA with per-block HRIR interpolation.
        BLOCK_XF = 512   # crossfade block size (samples)

        out_L = np.zeros(n_samples + self.HRIR_N, dtype=np.float64)
        out_R = np.zeros(n_samples + self.HRIR_N, dtype=np.float64)

        n_blocks    = (n_samples + BLOCK_XF - 1) // BLOCK_XF
        prev_L_ir   = None
        prev_R_ir   = None

        for b in range(n_blocks):
            start = b * BLOCK_XF
            end   = min(start + BLOCK_XF, n_samples)
            block = mono[start:end].astype(np.float64)
            blen  = len(block)

            # Azimuth at mid-block â€” use mean of traj for stability
            mid_s = min((start + end) // 2, n_samples - 1)
            az    = float(az_traj[mid_s] % 360.0)

            if self.using_sofa:
                L_ir_cur, R_ir_cur = self._hrir_for_position(az, elev_deg, dist_cm)
            else:
                elev_scale = float(np.cos(elevation * np.pi / 2))
                az_eff     = (az - 180.0) * elev_scale + 180.0
                L_ir_cur, R_ir_cur = self._hrir_for_angle(az_eff)

            # Cross-fade with previous HRIR (raised-cosine)
            if prev_L_ir is not None and blen == BLOCK_XF:
                # Blend adjacent HRIRs with a raised-cosine (Hann) crossfade.
                # IR blend is valid because convolution is linear:
                #   conv(x, Î±Â·h1 + (1âˆ’Î±)Â·h2) = Î±Â·conv(x,h1) + (1âˆ’Î±)Â·conv(x,h2)
                # Using the mean of the Hann window (â‰ˆ0.5) gives a smooth
                # 50/50 blend at every block boundary â€” no clicks.
                fade_in  = 0.5 - 0.5 * np.cos(np.pi * np.arange(blen) / blen)
                blend    = float(np.mean(fade_in))   # â‰ˆ 0.5 for symmetric crossfade
                L_ir_use = blend * L_ir_cur.astype(np.float64) + (1.0 - blend) * prev_L_ir.astype(np.float64)
                R_ir_use = blend * R_ir_cur.astype(np.float64) + (1.0 - blend) * prev_R_ir.astype(np.float64)
            else:
                L_ir_use = L_ir_cur.astype(np.float64)
                R_ir_use = R_ir_cur.astype(np.float64)

            seg_len = blen + self.HRIR_N - 1
            out_L[start: start + seg_len] += fftconvolve(block, L_ir_use)[:seg_len]
            out_R[start: start + seg_len] += fftconvolve(block, R_ir_use)[:seg_len]

            prev_L_ir = L_ir_cur.astype(np.float64)
            prev_R_ir = R_ir_cur.astype(np.float64)

        # â”€â”€ Trim + normalise â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stereo = np.stack([
            out_L[:n_samples].astype(np.float32),
            out_R[:n_samples].astype(np.float32),
        ])

        # â”€â”€ HF restoration: compensate average ILD-induced HF loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # With ILD max 15 dB at 8 kHz, average orbital HF loss â‰ˆ 7.5 dB.
        # Conservative 2-stage restoration shelf (kept mild to avoid stacking
        # with the downstream EQ chain which also boosts HF):
        #   +1 dB starting at 4 kHz  â€” presence restoration
        #   +2 dB starting at 7 kHz  â€” air/brilliance restoration (was +5 dB â†’ reduced)
        # NOTE: The EQ chain also applies HF boosts â€” total stack must stay < 6 dB.
        try:
            freqs_r = np.fft.rfftfreq(stereo.shape[1], d=1.0 / self.sr)
            H_restore = np.ones(len(freqs_r), dtype=np.float64)
            for fi, f in enumerate(freqs_r):
                if f <= 4000.0:
                    H_restore[fi] = 1.0
                elif f <= 7000.0:
                    t_s = (f - 4000.0) / 3000.0
                    boost_db = 1.0 * t_s          # 0 â†’ +1 dB (was 0 â†’ +2 dB)
                    H_restore[fi] = 10.0 ** (boost_db / 20.0)
                else:
                    t_s = min((f - 7000.0) / 8000.0, 1.0)
                    boost_db = 1.0 + 1.0 * t_s    # +1 â†’ +2 dB (was +2 â†’ +5 dB)
                    H_restore[fi] = 10.0 ** (boost_db / 20.0)
            for ch in range(2):
                C_f = np.fft.rfft(stereo[ch].astype(np.float64))
                stereo[ch] = np.fft.irfft(C_f * H_restore,
                                           n=stereo.shape[1]).astype(np.float32)
        except Exception:
            pass   # restoration is best-effort; continue without it

        peak = float(np.max(np.abs(stereo))) + 1e-10
        if peak > 0.99:
            stereo /= peak * 1.01

        return stereo


# Singleton â€” instantiated lazily on first use
_hrir_engine: Optional['HRIREngine'] = None

def get_hrir_engine(sr: int = 48000) -> 'HRIREngine':
    global _hrir_engine
    if _hrir_engine is None or _hrir_engine.sr != sr:
        _hrir_engine = HRIREngine(sample_rate=sr)
    return _hrir_engine


# ============================================================================
# WEBSOCKET MANAGER
# ============================================================================

class ConnectionManager:
    def __init__(self):
        self.active: Dict[str, WebSocket] = {}

    async def connect(self, job_id: str, ws: WebSocket):
        await ws.accept()
        self.active[job_id] = ws

    def disconnect(self, job_id: str):
        self.active.pop(job_id, None)

    async def _send(self, job_id: str, data: dict):
        ws = self.active.get(job_id)
        if ws:
            try:
                await ws.send_json(data)
            except Exception:
                pass

    async def send_progress(self, job_id: str, pct: int, stage: str):
        await self._send(job_id, {"type": "progress", "progress": pct, "stage": stage})

    async def send_complete(self, job_id: str, url: str, video_url: Optional[str] = None):
        data = {"type": "complete", "output_url": url}
        if video_url:
            data["video_url"] = video_url
        await self._send(job_id, data)

    async def send_error(self, job_id: str, msg: str):
        await self._send(job_id, {"type": "error", "message": msg})


manager = ConnectionManager()


# ============================================================================
# UTILITIES
# ============================================================================

def auto_detect_ffmpeg():
    import platform
    if platform.system() != "Windows":
        return
    paths = [r"C:\ffmpeg\bin", r"C:\Program Files\ffmpeg\bin",
             os.path.expanduser(r"~\ffmpeg\bin")]
    try:
        subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
        return
    except (subprocess.CalledProcessError, FileNotFoundError):
        pass
    for p in paths:
        if os.path.exists(os.path.join(p, "ffmpeg.exe")):
            if p not in os.environ.get("PATH", ""):
                os.environ["PATH"] += os.pathsep + p
            return

def check_ffmpeg() -> bool:
    try:
        subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
        return True
    except Exception:
        return False

def check_reverberate_filter() -> bool:
    try:
        result = subprocess.run(["ffmpeg", "-filters"], capture_output=True, text=True, check=True)
        return "reverberate" in result.stdout
    except Exception:
        return False

def get_audio_duration(file_path: str) -> float:
    try:
        r = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration",
             "-of", "default=noprint_wrappers=1:nokey=1", str(file_path)],
            capture_output=True, text=True, check=True)
        return float(r.stdout.strip())
    except Exception:
        return 0.0

async def ffmpeg_progress(line: str, total: float) -> float:
    try:
        if 'time=' in line:
            ts = line.split('time=')[1].split()[0]
            h, m, s = ts.split(':')
            secs = int(h)*3600 + int(m)*60 + float(s)
            if total > 0:
                return min(secs / total, 1.0)
    except Exception:
        pass
    return 0.0


# ============================================================================
# EQ CHAIN BUILDER  (12-band â€” unchanged API)
# ============================================================================

def _merge_legacy_eq(p: ProcessingParams) -> ProcessingParams:
    if abs(p.eq_sub_bass_gain) > 0.1:
        p = p.copy(update={
            'eq_sub30_gain': p.eq_sub30_gain + p.eq_sub_bass_gain * 0.5,
            'eq_sub60_gain': p.eq_sub60_gain + p.eq_sub_bass_gain * 0.5,
        })
    if abs(p.eq_bass_gain) > 0.1:
        p = p.copy(update={'eq_bass100_gain': p.eq_bass100_gain + p.eq_bass_gain})
    if abs(p.eq_low_mid_gain) > 0.1:
        p = p.copy(update={'eq_lowmid350_gain': p.eq_lowmid350_gain + p.eq_low_mid_gain})
    if abs(p.eq_presence_gain) > 0.1:
        p = p.copy(update={'eq_presence3k_gain': p.eq_presence3k_gain + p.eq_presence_gain})
    if abs(p.eq_air_gain) > 0.1:
        p = p.copy(update={'eq_air12k_gain': p.eq_air12k_gain + p.eq_air_gain})
    return p


def _eq_chain(p: ProcessingParams) -> str:
    p = _merge_legacy_eq(p)

    def shelf(f, g, w):
        return f"equalizer=f={f}:t=h:w={w}:g={g:.1f}"
    def bell(f, g, w):
        return f"equalizer=f={f}:t=q:w={w}:g={g:.1f}"

    eqs = []
    if abs(p.eq_sub30_gain)     > 0.3: eqs.append(shelf(30,    p.eq_sub30_gain,    30))
    if abs(p.eq_sub60_gain)     > 0.3: eqs.append(bell (60,    p.eq_sub60_gain,    40))
    if abs(p.eq_bass100_gain)   > 0.3: eqs.append(bell (100,   p.eq_bass100_gain,  80))
    if abs(p.eq_ubass200_gain)  > 0.3:
        gain = max(-6.0, min(4.0, p.eq_ubass200_gain))
        eqs.append(bell(200, gain, 100))
    if abs(p.eq_lowmid350_gain) > 0.3: eqs.append(bell (350,   p.eq_lowmid350_gain, 200))
    if abs(p.eq_mid700_gain)    > 0.3: eqs.append(bell (700,   p.eq_mid700_gain,   300))
    if abs(p.eq_umid1500_gain)  > 0.3: eqs.append(bell (1500,  p.eq_umid1500_gain, 600))
    if abs(p.eq_presence3k_gain)> 0.3: eqs.append(bell (3000,  p.eq_presence3k_gain, 1500))
    if abs(p.eq_def5k_gain)     > 0.3: eqs.append(bell (5000,  p.eq_def5k_gain,   2000))
    if abs(p.eq_bril8k_gain)    > 0.3: eqs.append(shelf(8000,  p.eq_bril8k_gain,  3000))
    if abs(p.eq_air12k_gain)    > 0.3: eqs.append(shelf(12000, p.eq_air12k_gain,  5000))
    if abs(p.eq_uair16k_gain)   > 0.3: eqs.append(shelf(16000, p.eq_uair16k_gain, 6000))

    return ",".join(eqs) if eqs else ""


# ============================================================================
# INSTRUMENT ENHANCEMENT CHAIN
# ============================================================================

def _instrument_enhance_chain(p: ProcessingParams) -> str:
    """
    Instrument enhancement chain â€” v8.1 (frequency-selective exciter).

    v7 problem: wideband tanh() saturation added harmonics at ALL frequencies,
    causing sub-bass intermodulation that muddied the HRTF band separation.

    v8.1 fix â€” 3-stage split path:
      Stage 1: Transient compressor (gentler makeup: 2 dB not 3 dB).
      Stage 2: HF exciter â€” boost > 3 kHz by 6 dB, apply soft saturation,
               then cut back by 6 dB. Net result: harmonic content added
               only at 6â€“12 kHz (presence/air), bass untouched.
      Stage 3: +1.5 dB presence shelf at 5 kHz to restore cup-masked detail.
    """
    if not p.instrument_enhance:
        return ""
    return (
        "acompressor=threshold=-30dB:ratio=1.5:attack=5:release=80:makeup=2dB,"
        "equalizer=f=3000:t=h:w=2000:g=6.0,"
        "aeval=val(0)*0.88+0.12*tanh(val(0)*2.5)|val(1)*0.88+0.12*tanh(val(1)*2.5),"
        "equalizer=f=3000:t=h:w=2000:g=-6.0,"
        "equalizer=f=5000:t=h:w=3000:g=1.5"
    )


# ============================================================================
# NEW v6.0: PSYCHOACOUSTIC HELPERS
# ============================================================================

def _diffuse_field_eq() -> str:
    """
    IEC 711 diffuse-field headphone correction â€” v9.0 (3-band, reduced stack).

    Consolidated from 6-band to 3-band to prevent phase cancellation when
    stacked with _equal_loudness_shelf, user EQ, and pinna notch filters.
    Keeps only the three perceptually dominant corrections:

      -2.0 dB @ 700 Hz   â€” ear-canal resonance cut (closed headphones boost here)
      -4.0 dB @ 5.0 kHz  â€” headphone cup resonance cut (biggest in-head culprit)
      +4.0 dB @ 10.0 kHz â€” restore ultra-air roll-off
    """
    return (
        "equalizer=f=700:t=q:w=500:g=-2.0,"    # ear-canal resonance cut
        "equalizer=f=5000:t=q:w=2000:g=-4.0,"  # cup resonance cut
        "equalizer=f=10000:t=h:w=4000:g=4.0"   # air restore
    )


def _equal_loudness_shelf() -> str:
    """
    ISO 226:2003 equal-loudness compensation â€” v9.0 (3-band, reduced stack).

    Consolidated from 6-band to 3-band to prevent phase accumulation when
    stacked with _diffuse_field_eq and user EQ. Keeps the three dominant
    perceptual corrections at ~70 phons:

      +4.5 dB @ 40 Hz    â€” sub-bass boost (corrected from overblown +7.5 @ 50 Hz)
      -1.5 dB @ 3.5 kHz  â€” mid scoop (natural dip in 70-phon curve)
      +3.0 dB @ 12.0 kHz â€” high-air restore
    """
    return (
        "equalizer=f=40:t=h:w=40:g=4.5,"        # sub-bass boost (conservative)
        "equalizer=f=3500:t=q:w=3000:g=-1.5,"   # mid scoop
        "equalizer=f=12000:t=h:w=5000:g=3.0"    # high-air restore
    )

def _pinna_notch_filters(intensity: float = 1.0) -> str:
    """
    Pinna shadow notch filters â€” v8.1 (4-notch model).

    Real pinna reflections create narrow spectral notches the brain uses to
    perceive elevation and front/back. Four notches are modelled here:

      Notch 1: ~8.5 kHz  â€” first pinna concha resonance      (âˆ’6 dB max)
      Notch 2: ~10.5 kHz â€” second pinna shadow / flap        (âˆ’4 dB max)
      Notch 3: ~13.0 kHz â€” upper pinna flap reflection       (âˆ’3 dB max)
      Notch 4: ~16.0 kHz â€” outer helix diffraction shoulder  (âˆ’2.5 dB max)
               [NEW v8.1] â€” adds the subtle ultra-high cue that gives
               elevation perception above the ear plane.

    All gains scale linearly with intensity [0â€“1.5].
    Bandwidths chosen for psychoacoustic selectivity (not surgical cuts).
    """
    if intensity <= 0:
        return ""
    g1 = round(-6.0   * min(intensity, 1.5), 1)
    g2 = round(-4.0   * min(intensity, 1.5), 1)
    g3 = round(-3.0   * min(intensity, 1.5), 1)
    g4 = round(-2.5   * min(intensity, 1.5), 1)
    return (
        f"equalizer=f=8500:t=q:w=1200:g={g1},"
        f"equalizer=f=10500:t=q:w=1800:g={g2},"
        f"equalizer=f=13000:t=q:w=2500:g={g3},"
        f"equalizer=f=16000:t=q:w=3500:g={g4}"
    )


def _allpass_diffuser(room: float = 0.6) -> str:
    """
    Allpass diffusion network â€” four prime-interval aecho taps scaled by room size.

    Tap delays are proportional to room size so that a small room (0.1) produces
    dense early reflections (tight flutter) while a large room (1.0) gives wide,
    spacious pre-echo that merges smoothly into the main reverb tail.

    Small room (r=0.1): delays â‰ˆ 8|10|14|18 ms  â†’ intimate, close-miked feel
    Medium  (r=0.6):    delays â‰ˆ 17|23|31|41 ms  â†’ original values
    Large   (r=1.0):    delays â‰ˆ 27|37|51|67 ms  â†’ concert-hall depth
    """
    r = max(0.05, min(1.0, room))
    scale = r / 0.6          # normalise around original mid-room values
    d1 = max(5,  int(17 * scale))
    d2 = max(7,  int(23 * scale))
    d3 = max(9,  int(31 * scale))
    d4 = max(11, int(41 * scale))
    # Decays: shorter taps die faster (keeps transients clean)
    dc1 = round(min(0.25, 0.14 * (1 + r * 0.3)), 3)
    dc2 = round(min(0.20, 0.11 * (1 + r * 0.3)), 3)
    dc3 = round(min(0.16, 0.08 * (1 + r * 0.3)), 3)
    dc4 = round(min(0.13, 0.06 * (1 + r * 0.3)), 3)
    return (
        f"aecho=in_gain=1.0:out_gain=1.0"
        f":delays={d1}|{d2}|{d3}|{d4}"
        f":decays={dc1}|{dc2}|{dc3}|{dc4}"
    )


def _haas_widener(room: float = 0.6) -> str:
    """
    Haas-effect early reflection widener.

    Applies a short (8â€“18 ms) delay only to the right channel, exploiting the
    Haas (precedence) effect so the brain perceives the delayed copy as spatial
    width rather than a discrete echo. The left channel is undelayed so the
    stereo image is anchored correctly.

    Delay scales with room: smaller rooms produce tighter Haas gaps.
    Range: 8 ms (r=0.1)  â†’  18 ms (r=1.0), mirrored with mirrored gain.
    """
    r = max(0.0, min(1.0, room))
    haas_ms = round(8.0 + r * 10.0, 1)     # 8â€“18 ms
    haas_gain = round(0.72 + r * 0.10, 3)  # 0.72â€“0.82 (slightly quieter)
    return (
        f"aecho=in_gain=1.0:out_gain={haas_gain}"
        f":delays=0|{haas_ms}"
        f":decays=0.0|0.50"
    )


def _reverb(p: ProcessingParams) -> str:
    """4-tap multi-reflection reverb via aecho (core reverb)."""
    ig = max(0.1, 1.0 - p.reverb_mix * 0.35)
    og = p.reverb_mix * 0.65
    r  = p.reverb_room
    d1, d2 = int(r * 40), int(r * 63)
    d3, d4 = int(r * 97), int(r * 131)
    dc1 = r * 0.48; dc2 = r * 0.32; dc3 = r * 0.20; dc4 = r * 0.12
    return (
        f"aecho=in_gain={ig:.2f}:out_gain={og:.2f}"
        f":delays={d1}|{d2}|{d3}|{d4}"
        f":decays={dc1:.2f}|{dc2:.2f}|{dc3:.2f}|{dc4:.2f}"
    )


# ============================================================================
# BEAT-SYNC ROTATION  â€” v9.1 (replaces broken v9.0 piecewise expression)
# ============================================================================
#
# v9.0 generated a piecewise if(gte(t,...), cos(...), if(...)) expression
# with one level per beat â€” up to 300 levels deep. FFmpeg's expression
# evaluator silently failed on expressions that large, defaulting every
# volume filter to gain=1. Result: zero panning on any beat-detected track.
#
# v9.1 replaces this with a simple, reliable approach:
#   1. Compute the median inter-beat interval from librosa's beat array.
#   2. Derive the actual BPM from that interval.
#   3. Pick a rotation speed that is a clean musical ratio of the BPM
#      (e.g. 1 rotation per 4 beats).
#   4. Pass that speed to build_8band_hrtf_engine_v6 as a normal float.
#
# The spatial effect stays musically locked without any complex expressions.
# beat_positions from the analysis result ARE used â€” just for arithmetic,
# not for generating FFmpeg filter strings.

def beat_sync_rotation_speed(
    beat_times_s: List[float],
    base_rotation_speed: float,
) -> float:
    """
    Derive a musically-locked rotation speed from beat timestamps.

    Computes median BPM from beat_times_s, then finds the nearest musical
    ratio (1/8, 1/4, 1/2, 1, 2 rotations per beat) that keeps the rotation
    speed within [0.05, 0.5] Hz.  Falls back to base_rotation_speed if
    beats are too few or the result is out of range.
    """
    if not beat_times_s or len(beat_times_s) < 4:
        return base_rotation_speed

    ibis = [beat_times_s[k+1] - beat_times_s[k]
            for k in range(min(len(beat_times_s)-1, 60))]
    ibis.sort()
    median_ibi = ibis[len(ibis) // 2]
    if median_ibi < 0.05:
        return base_rotation_speed   # unreasonably fast beats

    beat_freq = 1.0 / median_ibi   # Hz (= BPM / 60)

    # Musical ratios: rotations per beat
    ratios = [1/8, 1/6, 1/4, 1/3, 1/2, 1.0, 2.0]
    candidates = [beat_freq * r for r in ratios]

    # Find the candidate closest to base_rotation_speed within [0.04, 0.6] Hz
    valid = [(abs(c - base_rotation_speed), c) for c in candidates if 0.04 <= c <= 0.6]
    if not valid:
        return base_rotation_speed

    _, best = min(valid)
    return round(best, 4)


# ============================================================================
# STUDIO GRADE v6.0  â€” 8-band HRTF + ITD + Pinna EQ + Diffuse-Field EQ
# ============================================================================

def build_stereo_mastering_chain(p: ProcessingParams) -> tuple:
    """
    Post-HRIR mastering chain â€” stereo-in, stereo-out.

    Used after the Python HRIR convolution engine has already produced a
    binaural stereo WAV.  This chain must NOT collapse L/R to mono.

    Applies only:
      â€¢ Reverb (single aecho â€” no triple chain)
      â€¢ Diffuse-field headphone EQ (3 bands)
      â€¢ 12-band master EQ
      â€¢ EBU R128 loudnorm
      â€¢ True peak limiter

    No pan=mono, no stereotools split, no per-channel processing.
    """
    eq  = _eq_chain(p)
    rev = _reverb(p)

    # Diffuse-field EQ â€” tuned for post-HRIR mastering path.
    # Based on IEC 60268-7 DF target and KEMAR measurements.
    # The HRIR engine has applied ILD + pinna notches â€” this EQ restores
    # the frequency balance that headphones hear when listening to spatial content.
    #
    # Key compensations (per psychoacoustic research):
    #   +4.5 dB @ 2700 Hz â€” concha resonance peak (primary DF compensation)
    #   +3.5 dB @ 8000 Hz â€” restore HF lost to ILD orbital averaging
    #   +5.5 dB @ 12000 Hz â€” pinna/tragus resonance restoration (air/sparkle)
    #   -1.5 dB @ 500 Hz  â€” compensate slight torso reflection coloration
    dfeq_simple = (
        "equalizer=f=500:t=q:w=500:g=-1.5,"
        "equalizer=f=2700:t=q:w=2500:g=4.5,"
        "equalizer=f=8000:t=h:w=5000:g=3.5,"
        "equalizer=f=12000:t=h:w=5000:g=5.5"
    )

    parts = [
        # â”€â”€ reverb mix (wet/dry blend using aecho) â”€â”€
        f"[0:a]{rev}[rev_out]",
        # dry/wet recombine is handled inside _reverb via in_gain/out_gain
    ]

    last = "rev_out"

    if eq:
        parts.append(f"[{last}]{eq}[eq_out]")
        last = "eq_out"

    parts.append(f"[{last}]{dfeq_simple}[dfeq_out]")
    last = "dfeq_out"

    parts.append(f"[{last}]loudnorm=I=-16:TP=-1.0:LRA=11:linear=true[loud]")
    last = "loud"

    if p.enable_limiter:
        parts.append(f"[{last}]alimiter=limit=1:attack=20:release=200:level=false[out]")
        return ";".join(parts), "[out]"

    return ";".join(parts), f"[{last}]"


def build_8band_hrtf_engine_v6(
    p: ProcessingParams,
    beat_times_s: Optional[List[float]] = None,
) -> tuple:
    """
    16-Band Spatial Audio Engine v13.0 â€” Calibrated to Real 8D Analysis Data.

    Architecture derived from per-frame analysis of commercial 8D tracks:

      spectral_movement  = 0.0    â†’ No EQ changes during rotation (removed FRONT/REAR EQ)
      phase_variation    = 0.1498 â†’ Static phase diff from reverb, NOT rotating ITD
      amplitude_modulation score = 1.0 â†’ PRIMARY cue: Doppler volume envelope
      avg_phase_diff     = 1.30 rad  â†’ From decorrelated reverb (74.6Â° stereo spread)
      lr_ratio_std       = 0.3338    â†’ Subtle master pan LFO + reverb variation

    Two-stage design:
      STAGE 1 â€” Per-band diffuse spread:
        16 bands Ã— step-of-4 phases â†’ all head positions occupied simultaneously.
        Per-band ILD creates each frequency at a genuinely different azimuth.
        This produces the thick immersive spatial image.

      STAGE 2 â€” Master orbital LFO (applied after mixing):
        A single cosine drives overall L/R pan of the full mix.
        A single sin drives overall amplitude (Doppler: front=loud, rear=quiet).
        This is the COHERENT ORBIT the ear tracks as "sound moving around head".
        Master pan FLOOR=0.70 â†’ max ILD â‰ˆ 3 dB (matches lr_ratio_std target).
        Master dist FLOOR=0.70 â†’ rear is 3 dB quieter (matches am_modulation target).
    """
    i    = p.intensity_multiplier
    dvol = round(1.0 / max(p.distance, 0.3), 4)

    # â”€â”€ Rotation speed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    base_rot = round(p.rotation_speed * i, 5)
    if beat_times_s and len(beat_times_s) >= 4:
        ibi_list = sorted([beat_times_s[k+1] - beat_times_s[k]
                           for k in range(min(len(beat_times_s)-1, 40))])
        med_ibi  = ibi_list[len(ibi_list) // 2]
        if 0.2 < med_ibi < 2.0:
            bpm_rot  = round(1.0 / med_ibi / 4.0, 5)
            base_rot = round((base_rot + bpm_rot) / 2.0, 5)
            print(f"  \u21b3 BPM-synced rotation: {bpm_rot} Hz \u2192 blended {base_rot} Hz")

    # â”€â”€ Genre-based rotation clamping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    GENRE_ROTATION_RANGES = {
        'electronic': (0.14, 0.25), 'edm':      (0.14, 0.25),
        'pop':        (0.08, 0.12), 'bollywood': (0.07, 0.10),
        'bhangra':    (0.07, 0.10), 'ballad':    (0.03, 0.06),
        'ambient':    (0.03, 0.05), 'classical': (0.04, 0.07),
    }
    genre_key = (p.genre_preset or 'unknown').lower() if hasattr(p, 'genre_preset') else 'unknown'
    if genre_key in GENRE_ROTATION_RANGES:
        mn, mx   = GENRE_ROTATION_RANGES[genre_key]
        clamped  = max(mn, min(base_rot, mx))
        if clamped != base_rot:
            print(f"  \u21b3 Genre-clamped: {base_rot:.4f} \u2192 {clamped:.4f} Hz ({genre_key})")
        base_rot = clamped

    # â”€â”€ 16 bands with per-band rotation speeds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    br = p.bass_rotation   * i
    tr = p.treble_rotation * i
    BAND_DEF = [
        # (label,   lo,    hi,    rotation_speed)
        ('sub1',    20,    50,    round(br * 0.30, 5)),
        ('sub2',    50,    80,    round(br * 0.40, 5)),
        ('bass1',   80,   150,    round(br * 0.55, 5)),
        ('bass2',  150,   250,    round(br * 0.70, 5)),
        ('lowm1',  250,   400,    round(base_rot * 0.75, 5)),
        ('lowm2',  400,   600,    round(base_rot * 0.85, 5)),
        ('voc1',   600,  1000,    round(base_rot * 0.30, 5)),
        ('voc2',  1000,  2000,    round(base_rot * 0.35, 5)),
        ('hm1',   2000,  3000,    round(tr * 0.85, 5)),
        ('hm2',   3000,  4000,    round(tr * 0.95, 5)),
        ('pre1',  4000,  6000,    round(tr * 1.05, 5)),
        ('pre2',  6000,  8000,    round(tr * 1.15, 5)),
        ('air1',  8000, 11000,    round(tr * 1.25, 5)),
        ('air2', 11000, 14000,    round(tr * 1.35, 5)),
        ('sp1',  14000, 18000,    round(tr * 1.45, 5)),
        ('sp2',  18000, 22000,    round(tr * 1.55, 5)),
    ]

    # â”€â”€ Phase offsets: step of 4 radians â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4/2Ï€ irrational â†’ bands scatter across full sphere. Max L/R swing Â±2.5%.
    # Circle positions: 0Â°,229Â°,98Â°,328Â°,197Â°,66Â°,295Â°,164Â°,33Â°,263Â°,132Â°,1Â°,230Â°,99Â°,329Â°,198Â°
    PHASES = [k * 4 for k in range(16)]

    # â”€â”€ Per-band ILD (creates diffuse spatial spread between frequencies) â”€â”€â”€â”€â”€â”€
    # Each frequency genuinely occupies a different head position simultaneously.
    # These are the calibrated values from 38-song measurement campaign.
    ILD_MAP = {
        'sub1': 0.794, 'sub2': 0.794,   # Â±2 dB  â€” sub barely pans
        'bass1':0.708, 'bass2':0.708,   # Â±3 dB
        'lowm1':0.562, 'lowm2':0.562,   # Â±5 dB
        'voc1': 0.282, 'voc2': 0.282,   # Â±11 dB
        'hm1':  0.282, 'hm2':  0.282,   # Â±11 dB
        'pre1': 0.158, 'pre2': 0.158,   # Â±16 dB
        'air1': 0.126, 'air2': 0.126,   # Â±18 dB
        'sp1':  0.100, 'sp2':  0.100,   # Â±20 dB
    }

    # â”€â”€ Per-band filter builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def band(lbl, lo, hi, rot, ph):
        px    = []
        th    = f"2*PI*{rot}*t+{ph}"
        cos_t = f"cos({th})"
        FLOOR = ILD_MAP.get(lbl, 0.20)
        depth = round(1.0 - FLOOR, 3)
        is_vocal = lbl in ('voc1', 'voc2')

        # 1. Bandpass
        if lo <= 20:
            px.append(f"[{lbl}_in]lowpass=f={hi}[{lbl}_f]")
        elif hi >= 22000:
            px.append(f"[{lbl}_in]highpass=f={lo}[{lbl}_f]")
        else:
            mid = (lo + hi) // 2
            px.append(f"[{lbl}_in]bandpass=f={mid}:width_type=h:w={hi-lo}[{lbl}_f]")

        # 2. L/R ILD panning â€” pure cosine volume, NO EQ changes (spectral_movement=0)
        px.append(f"[{lbl}_f]asplit=2[{lbl}_Ls][{lbl}_Rs]")

        if is_vocal:
            half  = round(FLOOR + depth * 0.5, 4)
            vc_d  = round(depth * 0.25, 4)
            pan_L = f"({half}-{vc_d}*{cos_t})"
            pan_R = f"({half}+{vc_d}*{cos_t})"
        else:
            pan_L = f"({FLOOR}+{depth}*(0.5-0.5*{cos_t}))"
            pan_R = f"({FLOOR}+{depth}*(0.5+0.5*{cos_t}))"

        px.append(f"[{lbl}_Ls]volume='{pan_L}':eval=frame[{lbl}_Lv]")
        px.append(f"[{lbl}_Rs]volume='{pan_R}':eval=frame[{lbl}_Rv]")

        # 3. Join to stereo + distance scaling
        px.append(f"[{lbl}_Lv][{lbl}_Rv]join=inputs=2:channel_layout=stereo[{lbl}_st]")
        px.append(f"[{lbl}_st]volume={dvol}[{lbl}_out]")
        return px

    # â”€â”€ Assemble 16 bands â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    n_bands   = len(BAND_DEF)
    band_lbls = [b[0] for b in BAND_DEF]
    split_outs = "".join(f"[{b}_in]" for b in band_lbls)

    parts = [
        "[0:a]highpass=f=20:poles=2[dc_blocked]",
        f"[dc_blocked]pan=mono|c0=0.5*c0+0.5*c1[mono_src]",
        f"[mono_src]asplit={n_bands}{split_outs}",
    ]

    for idx, (lbl, lo, hi, rot) in enumerate(BAND_DEF):
        parts += band(lbl, lo, hi, rot, PHASES[idx])

    # â”€â”€ Headroom + mix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    headroom = round(1.0 / (n_bands ** 0.5), 4)   # 1/sqrt(16) = 0.25
    for lbl in band_lbls:
        parts.append(f"[{lbl}_out]volume={headroom}[{lbl}_hr]")
    outs = "".join(f"[{b}_hr]" for b in band_lbls)
    parts.append(f"{outs}amix=inputs={n_bands}:duration=first:normalize=0[bands_mix]")

    # â”€â”€ STAGE 2: Master orbital LFO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Applied to the FULL MIX after all bands are combined.
    # This creates the coherent orbit the ear perceives as "sound around the head".
    #
    # Master pan: cos(Î¸_master) drives L/R of the whole mix
    #   FLOOR=0.70 â†’ max swing Â±3 dB â†’ lr_ratio_std â‰ˆ 0.26 (+ reverb adds ~0.07 more)
    # Master dist: sin(Î¸_master) drives overall amplitude (Doppler simulation)
    #   FLOOR=0.70 â†’ rear is 3 dB quieter â†’ amplitude_modulation â‰ˆ 0.35 âœ…
    #
    # master_theta uses the same base_rot so bands and master orbit stay in sync.
    MASTER_PAN_FLOOR  = 0.70
    MASTER_PAN_DEPTH  = round(1.0 - MASTER_PAN_FLOOR, 3)   # 0.30
    MASTER_DIST_FLOOR = 0.9
    MASTER_DIST_DEPTH = round(1.0 - MASTER_DIST_FLOOR, 3)  # 0.30

    master_th   = f"2*PI*{base_rot}*t"
    master_cos  = f"cos({master_th})"

    master_pan_L = f"({MASTER_PAN_FLOOR}+{MASTER_PAN_DEPTH}*(0.5-0.5*{master_cos}))"
    master_pan_R = f"({MASTER_PAN_FLOOR}+{MASTER_PAN_DEPTH}*(0.5+0.5*{master_cos}))"
    # BUG FIX: old formula used sin(Î¸), placing the amplitude trough at 270Â°
    # (an arbitrary quarter-orbit offset from the pan sweep).
    # Correct behaviour: louder at front (cos=+1), quieter at rear (cos=-1).
    # Using (0.5 + 0.5*cos) maps: frontâ†’1.0, sideâ†’FLOOR+DEPTH*0.5, rearâ†’FLOOR.
    master_dist  = f"({MASTER_DIST_FLOOR}+{MASTER_DIST_DEPTH}*(0.5+0.5*{master_cos}))"

    # Split mix to L/R for master pan
    parts.append("[bands_mix]asplit=2[mix_L][mix_R]")
    parts.append(f"[mix_L]volume='{master_pan_L}':eval=frame[mpan_L]")
    parts.append(f"[mix_R]volume='{master_pan_R}':eval=frame[mpan_R]")
    parts.append("[mpan_L][mpan_R]join=inputs=2:channel_layout=stereo[panned]")

    # Apply master distance amplitude modulation (the primary 8D Doppler cue)
    parts.append(f"[panned]volume='{master_dist}':eval=frame[orbited]")

    # â”€â”€ Reverb â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 90% wet reverb creates the acoustic space and the static 74.6Â° phase difference
    # that the analysis found (avg_phase_diff = 1.30 rad). The decorrelated L/R reverb
    # contributes to stereo_width variation (avg 0.386, max 0.912).
    rev_wet = min(round(p.reverb_mix * 1.05, 3), 1.0)
    rev_dry = round(1.0 - rev_wet, 3)
    r  = max(0.05, min(1.0, p.reverb_room))
    rs = round(0.5 + r * 0.45, 3)
    dt = round(1.0 + r * 1.5,  2)
    dmp= round(0.3 + r * 0.4,  2)
    pre_ms_L = round(20 + r * 10, 1)
    pre_ms_R = round(pre_ms_L + 5 + r * 3, 1)

    parts.append("[orbited]asplit=2[dry][rev_in]")

    # Decorrelated L/R early reflections (asymmetric tap delays on L vs R)
    # This creates the static ~74.6Â° phase difference measured in real 8D.
    parts.append(
        "[rev_in]aecho=in_gain=0.9:out_gain=0.9"
        ":delays=17:23:37:43:61:67:89:97"
        ":decays=0.4:0.35:0.3:0.28:0.25:0.22:0.2:0.18[diff]"
    )
    parts.append(f"[diff]adelay={pre_ms_L}|{pre_ms_R}[pre]")

    if check_reverberate_filter():
        parts.append(
            f"[pre]reverberate=room_size={rs}:time={dt}:damping={dmp:.2f}"
            f":wet={rev_wet:.3f}:dry=0[wet]"
        )
    else:
        decay = round(rev_wet * 0.85, 3)
        parts.append(
            f"[pre]aecho=in_gain=0.85:out_gain={rev_wet:.3f}"
            f":delays=30|47|61|79:decays={decay}:{decay}:{decay}:{decay}[wet]"
        )

    parts.append(f"[dry]volume={rev_dry}[dry_v]")
    parts.append("[dry_v][wet]amix=inputs=2:duration=first:normalize=0[post_rev]")

    # â”€â”€ Mild M/S widening â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append(
        "[post_rev]asplit=2[ms_l][ms_r];"
        "[ms_l]pan=1c|c0=0.5*c0+0.5*c1[mid_raw];"
        "[ms_r]pan=1c|c0=0.5*c0-0.5*c1[side_raw];"
        "[mid_raw]equalizer=f=300:t=q:w=250:g=-1.0[mid_eq];"
        "[side_raw]highpass=f=160:poles=2,"
        "equalizer=f=8000:t=h:w=4000:g=1.5,"
        "equalizer=f=12000:t=h:w=5000:g=2.0[side_eq];"
        "[mid_eq][side_eq]join=inputs=2:channel_layout=stereo[ms_join];"
        "[ms_join]pan=stereo|c0=c0+c1|c1=c0-c1[post_ms]"
    )

    # â”€â”€ Stereo width + user EQ + loudnorm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append(f"[post_ms]stereotools=mlev={p.stereo_width:.3f}:softclip=1[wide]")
    eq = _eq_chain(p)
    if eq:
        parts.append(f"[wide]{eq}[eqd]")
        last = "eqd"
    else:
        last = "wide"
    parts.append(f"[{last}]loudnorm=I=-16:TP=-1.0:LRA=11:linear=true[loud]")
    last = "loud"

    # â”€â”€ Elevation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if abs(p.elevation) > 0.01:
        eg_hi  = round(p.elevation * 6.0, 1)
        eg_sub = round(-p.elevation * 2.5, 1)
        parts.append(
            f"[{last}]equalizer=f=8000:t=h:w=3000:g={eg_hi},"
            f"equalizer=f=80:t=h:w=80:g={eg_sub}[elev]"
        )
        last = "elev"

    # â”€â”€ Limiter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if p.enable_limiter:
        parts.append(f"[{last}]alimiter=limit=1:attack=20:release=200:level=false[8d_out]")
        return ";".join(parts), "[8d_out]"

    return ";".join(parts), f"[{last}]"




def build_6band_filtergraph(p: ProcessingParams) -> tuple:
    i   = p.intensity_multiplier
    ph  = 0.12
    dvol= 1.0 / max(p.distance, 0.3)
    eq  = _eq_chain(p)
    rev = _reverb(p)
    enh = _instrument_enhance_chain(p)

    r_sub  = p.bass_rotation  * i * 0.55
    r_bass = p.bass_rotation  * i
    r_lm   = p.rotation_speed * i * 0.65
    r_mid  = p.rotation_speed * i
    r_pres = p.treble_rotation* i * 0.80
    r_air  = p.treble_rotation* i

    def _band_parts(lbl, lo, hi, rot, dv, ph_off=0.0, is_vocal=False):
        if lo <= 20:
            filt = f"lowpass=f={hi}[{lbl}_filt]"
            inp  = f"[{lbl}_in]"
        elif hi >= 20000:
            filt = f"highpass=f={lo}[{lbl}_filt]"
            inp  = f"[{lbl}_in]"
        else:
            filt = f"highpass=f={lo}[{lbl}_hp];[{lbl}_hp]lowpass=f={hi}[{lbl}_filt]"
            inp  = f"[{lbl}_in]"

        theta = f"2*PI*{rot:.5f}*t+{ph_off:.3f}"
        FLOOR = 0.40  # dataset: quiet ear always hears 40-60% (was 0.12 â†’ -18dB dead zones)
        depth = round(1.0 - 2 * FLOOR, 4)

        if is_vocal:
            # Vocal center: tighter pan, sound stays more central
            vc_d = round(depth * 0.32, 4)
            lfo_l = f"({FLOOR + depth*0.5:.4f}+{vc_d:.4f}*(-cos({theta})))"
            lfo_r = f"({FLOOR + depth*0.5:.4f}+{vc_d:.4f}*(cos({theta})))"
        else:
            # cos-based ILD â€” never hits zero, proper front/back equal levels
            lfo_l = f"({FLOOR:.4f}+{depth:.4f}*(0.5-0.5*cos({theta})))"
            lfo_r = f"({FLOOR:.4f}+{depth:.4f}*(0.5+0.5*cos({theta})))"

        ll, lr = f"{lbl}_l", f"{lbl}_r"
        return [
            f"{inp}{filt}",
            f"[{lbl}_filt]asplit=2[{ll}_m][{lr}_m]",
            f"[{ll}_m]volume='{lfo_l}':eval=frame[{ll}]",
            f"[{lr}_m]volume='{lfo_r}':eval=frame[{lr}]",
            f"[{ll}][{lr}]join=inputs=2:channel_layout=stereo[{lbl}_st]",
            f"[{lbl}_st]volume={dv:.4f}[{lbl}_8d]",
        ]

    parts = [
        "[0:a]highpass=f=20:poles=2[dc_blocked]",
        "[dc_blocked]pan=mono|c0=0.5*c0+0.5*c1[mono_src]",
        "[mono_src]asplit=6[sub_in][bass_in][lm_in][mid_in][pres_in][air_in]",
    ]
    parts += _band_parts("sub",  20,   100,  r_sub,  dvol * 1.05, ph_off=0)   # 0Â°
    parts += _band_parts("bass", 100,  350,  r_bass, dvol,         ph_off=4)   # 229Â°
    parts += _band_parts("lm",   350,  700,  r_lm,   dvol,         ph_off=8)   # 98Â°
    parts += _band_parts("mid",  700,  3000, r_mid,  dvol,         ph_off=12,  is_vocal=p.enable_vocal_center)  # 328Â°
    parts += _band_parts("pres", 3000, 7000, r_pres, dvol,         ph_off=16)  # 197Â°
    parts += _band_parts("air",  7000, 22000,r_air,  dvol,         ph_off=20)  # 66Â°

    parts.append("[sub_8d][bass_8d][lm_8d][mid_8d][pres_8d][air_8d]amix=inputs=6:duration=first:normalize=0[mixed]")
    parts.append(f"[mixed]{rev}[rev]")
    parts.append(f"[rev]stereotools=mlev={p.stereo_width:.3f}:sbal=0[wide]")

    if enh:
        parts.append(f"[wide]{enh}[enhanced]"); last = "enhanced"
    else:
        last = "wide"

    if eq:
        parts.append(f"[{last}]{eq}[eqd]"); last = "eqd"

    # v6: add diffuse-field EQ
    dfeq = _diffuse_field_eq()
    parts.append(f"[{last}]{dfeq}[dfeq]")
    parts.append("[dfeq]loudnorm=I=-16:TP=-1.0:LRA=11[6b_loud]")
    # v8.1 dual-band elevation (ported from 8-band engine)
    elev_parts, last_node = _elevation_dual_band(p, "6b_loud")
    parts.extend(elev_parts)
    return ";".join(parts), f"[{last_node}]"


# ============================================================================
# SHARED HELPERS  â€” used by multiple engine builders
# ============================================================================

def _elevation_dual_band(p: ProcessingParams, node_in: str) -> tuple:
    """
    v8.1 dual-band elevation model â€” shared across ALL engine builders.

    Old model (all legacy engines): single high-shelf at 8 kHz.
      Problem: only boosts/cuts brightness; does nothing for the
      psychoacoustic sub-bass cue that determines perceived height.

    New model (ported from 8-band v8.1):
      Elevation > 0 (above listener):
        +HF shelf  at 8 kHz:  +6 dB Ã— elevation  (bright, airy)
        Sub cut    at 80 Hz:  -2.5 dB Ã— elevation (less "floored")
      Elevation < 0 (below listener):
        -HF shelf  at 8 kHz:  mirrors above (darker, earthed)
        Sub boost  at 80 Hz:  +2.5 dB Ã— elevation (grounded rumble)

    Arguments:
      p        â€” ProcessingParams (reads p.elevation)
      node_in  â€” the FFmpeg label string for the input node, e.g. "loud"

    Returns (filter_parts: List[str], out_label: str)
      Caller appends filter_parts to its own parts list and uses out_label
      as the input to the next stage (limiter / final output).
    """
    if abs(p.elevation) <= 0.01:
        return [], node_in  # nothing to do

    eg_hi  = round(p.elevation * 6.0,  1)   # Â±6 dB at 8 kHz
    eg_sub = round(-p.elevation * 2.5, 1)   # opposite sign: above â†’ sub cut
    out_lbl = "elev_dual"
    filt = (
        f"[{node_in}]"
        f"equalizer=f=8000:t=h:w=3000:g={eg_hi},"
        f"equalizer=f=80:t=h:w=80:g={eg_sub}"
        f"[{out_lbl}]"
    )
    return [filt], out_lbl


# ============================================================================
# LEGACY FILTERGRAPH BUILDERS
# ============================================================================

def build_simple_filtergraph(p: ProcessingParams) -> tuple:
    rot   = p.rotation_speed * p.intensity_multiplier
    FLOOR = 0.40  # dataset: quiet ear always hears 40-60% (was 0.12 â†’ -18dB dead zones)
    depth = round(1.0 - 2 * FLOOR, 4)
    eq    = _eq_chain(p)
    rev   = _reverb(p)
    # cos-based ILD â€” no silent dead zones
    pan_L = f"({FLOOR:.4f}+{depth:.4f}*(0.5-0.5*cos(2*PI*{rot}*t)))"
    pan_R = f"({FLOOR:.4f}+{depth:.4f}*(0.5+0.5*cos(2*PI*{rot}*t)))"
    parts = [
        "[0:a]highpass=f=20:poles=2[dc_blocked_v]",
        "[dc_blocked_v]pan=mono|c0=0.5*c0+0.5*c1[mono_in]",
        f"[mono_in]asplit=2[sl][sr]",
        f"[sl]volume='{pan_L}':eval=frame[vl]",
        f"[sr]volume='{pan_R}':eval=frame[vr]",
        f"[vl][vr]join=inputs=2:channel_layout=stereo[joined]",
        f"[joined]{rev}[rev]",
        f"[rev]stereotools=mlev={p.stereo_width}[wide]",
    ]
    if eq:
        parts += [f"[wide]{eq}[eqd]", "[eqd]loudnorm=I=-16:TP=-1.0:LRA=11[simple_loud]"]
        loud_node = "simple_loud"
    else:
        parts.append("[wide]loudnorm=I=-16:TP=-1.0:LRA=11[simple_loud]")
        loud_node = "simple_loud"
    # v8.1 dual-band elevation (ported from 8-band engine)
    elev_parts, last_node = _elevation_dual_band(p, loud_node)
    parts.extend(elev_parts)
    return ";".join(parts), f"[{last_node}]"


def build_vocal_aware_filtergraph(p: ProcessingParams) -> tuple:
    i     = p.intensity_multiplier
    br    = p.bass_rotation * i
    vr    = p.rotation_speed * i * 0.5
    tr    = p.treble_rotation * i
    rev   = _reverb(p)
    eq    = _eq_chain(p)
    dvol  = 1.0 / max(p.distance, 0.3)
    FLOOR = 0.40  # dataset: quiet ear always hears 40-60% (was 0.12 â†’ -18dB dead zones)
    depth = round(1.0 - 2 * FLOOR, 4)

    # cos-based ILD for each band with staggered phase offsets
    def _pan(rot, ph=0.0):
        t = f"2*PI*{rot:.4f}*t+{ph:.3f}"
        return (
            f"({FLOOR:.4f}+{depth:.4f}*(0.5-0.5*cos({t})))",
            f"({FLOOR:.4f}+{depth:.4f}*(0.5+0.5*cos({t})))"
        )

    # Phase offsets: step of 4 radians (0Â°, 229Â°, 98Â°) â€” same scheme as 8-band engine
    b_L, b_R = _pan(br,       0)   # bass   at 0Â°
    v_L, v_R = _pan(vr * 0.5, 4)   # vocals at 229Â°
    h_L, h_R = _pan(tr,       8)   # treble at 98Â°

    # Vocal center: extra narrow (0.32 depth) keeps voice more centred
    vc_d = round(depth * 0.32, 4)
    vc_L = f"({FLOOR + depth*0.5:.4f}+{vc_d:.4f}*(-cos(2*PI*{vr*0.5:.4f}*t+4)))"
    vc_R = f"({FLOOR + depth*0.5:.4f}+{vc_d:.4f}*(cos(2*PI*{vr*0.5:.4f}*t+4)))"

    parts = [
        "[0:a]highpass=f=20:poles=2[dc_in]",
        "[dc_in]pan=mono|c0=0.5*c0+0.5*c1[mono_src]",
        "[mono_src]asplit=3[bass_in][vocal_in][high_in]",
        "[bass_in]lowpass=f=200[bass_filt]",
        "[bass_filt]asplit=2[bl][br]",
        f"[bl]volume='{b_L}':eval=frame[bvl]",
        f"[br]volume='{b_R}':eval=frame[bvr]",
        "[bvl][bvr]join=inputs=2:channel_layout=stereo[bass_st]",
        f"[bass_st]volume={dvol:.4f}[bass8d]",
        "[vocal_in]bandpass=f=1100:width_type=h:w=2800[voc_filt]",
        "[voc_filt]asplit=2[vl][vr_ch]",
    ]
    if p.enable_vocal_center:
        parts += [
            f"[vl]volume='{vc_L}':eval=frame[vvl]",
            f"[vr_ch]volume='{vc_R}':eval=frame[vvr]",
        ]
    else:
        parts += [
            f"[vl]volume='{v_L}':eval=frame[vvl]",
            f"[vr_ch]volume='{v_R}':eval=frame[vvr]",
        ]
    parts += [
        "[vvl][vvr]join=inputs=2:channel_layout=stereo[voc_st]",
        f"[voc_st]volume={dvol*1.1:.4f}[vocal8d]",
        "[high_in]highpass=f=3000[high_filt]",
        "[high_filt]asplit=2[hl][hr]",
        f"[hl]volume='{h_L}':eval=frame[hvl]",
        f"[hr]volume='{h_R}':eval=frame[hvr]",
        "[hvl][hvr]join=inputs=2:channel_layout=stereo[high_st]",
        f"[high_st]volume={dvol:.4f}[high8d]",
        "[bass8d][vocal8d][high8d]amix=inputs=3:duration=first:normalize=0[mixed]",
        f"[mixed]{rev}[rev]",
        f"[rev]stereotools=mlev={p.stereo_width}[wide]",
    ]
    if eq:
        parts += [f"[wide]{eq}[eqd]", "[eqd]loudnorm=I=-16:TP=-1.0:LRA=11[voc_loud]"]
        loud_node = "voc_loud"
    else:
        parts.append("[wide]loudnorm=I=-16:TP=-1.0:LRA=11[voc_loud]")
        loud_node = "voc_loud"
    # v8.1 dual-band elevation (ported from 8-band engine)
    elev_parts, last_node = _elevation_dual_band(p, loud_node)
    parts.extend(elev_parts)
    return ";".join(parts), f"[{last_node}]"


# ============================================================================
# AMBISONICS FIRST-ORDER (FOA / B-format) ENCODER   â€” v7.0 NEW
# ============================================================================

def build_ambisonics_foa_filtergraph(p: ProcessingParams) -> tuple:
    """
    First-Order Ambisonics encoder producing 4-channel B-format: W, X, Y, Z.

    For a source moving on a horizontal circle (elevation Ï† from params):
      azimuth Î¸(t) = 2Ï€ Ã— rotation_speed Ã— t    (LFO-driven rotation)
      elevation Ï†  = p.elevation Ã— (Ï€/2)         (static tilt)

    Ambisonic gain equations:
      W = 1 / âˆš2                                  (omnidirectional)
      X = cos(Î¸) Ã— cos(Ï†)                         (front-back)
      Y = sin(Î¸) Ã— cos(Ï†)                         (left-right)
      Z = sin(Ï†)                                   (up-down, constant)

    The result is a 4-channel WAV file (channel layout: 4.0 quad used as
    proxy for W/X/Y/Z since FFmpeg has no native B-format layout).
    """
    rot = p.rotation_speed * p.intensity_multiplier
    phi = p.elevation * 1.5708   # map [-1,1] â†’ [-Ï€/2, Ï€/2]

    # BUG FIX: the original guard `if 'np' in dir()` is semantically meaningless â€”
    # numpy is unconditionally imported at module level (line 67), so dir() will
    # always contain 'np'.  Use np directly; if numpy is missing the ImportError
    # at startup is the correct failure signal, not a silent fallback here.
    cos_phi = round(float(np.cos(phi)), 6)
    sin_phi = round(float(np.sin(phi)), 6)

    # W channel: constant 1/âˆš2 â‰ˆ 0.7071
    w_gain = 0.7071

    # Dynamic X = cos(2Ï€ rot t) Ã— cos_phi
    x_lfo = f"volume='{cos_phi:.6f}*cos(2*PI*{rot:.5f}*t)':eval=frame"
    # Dynamic Y = sin(2Ï€ rot t) Ã— cos_phi
    y_lfo = f"volume='{cos_phi:.6f}*sin(2*PI*{rot:.5f}*t)':eval=frame"
    # Static Z  = sin(phi)
    z_gain = round(sin_phi, 6)

    eq = _eq_chain(p)
    rev = _reverb(p)

    parts = [
        # Mix to mono source
        "[0:a]highpass=f=20:poles=2[dc_in]",
        "[dc_in]pan=mono|c0=0.5*c0+0.5*c1[mono_src]",

        # Apply master EQ + reverb to the mono source
        f"[mono_src]{rev}[mono_rev]",
    ]
    if eq:
        parts.append(f"[mono_rev]{eq}[mono_eq]")
        parts.append("[mono_eq]asplit=4[w_raw][x_raw][y_raw][z_raw]")
    else:
        parts.append("[mono_rev]asplit=4[w_raw][x_raw][y_raw][z_raw]")

    # W: scaled by 0.7071
    parts.append(f"[w_raw]volume={w_gain}[w_ch]")

    # X: cos(2Ï€ rot t) Ã— cos_phi
    parts.append(f"[x_raw]{x_lfo}[x_ch]")

    # Y: sin(2Ï€ rot t) Ã— cos_phi
    parts.append(f"[y_raw]{y_lfo}[y_ch]")

    # Z: static elevation component
    if abs(z_gain) > 0.01:
        parts.append(f"[z_raw]volume={z_gain}[z_ch]")
        z_out = "z_ch"
    else:
        parts.append("[z_raw]volume=0.0[z_ch]")
        z_out = "z_ch"

    # Join into 4-channel (using 4.0 quad layout as B-format proxy)
    parts.append(
        f"[w_ch][x_ch][y_ch][{z_out}]"
        "join=inputs=4:channel_layout=4.0[ambi_out]"
    )
    parts.append("[ambi_out]loudnorm=I=-16:TP=-1.0:LRA=11[ambi_norm]")

    return ";".join(parts), "[ambi_norm]"


# ============================================================================
# DOLBY ATMOS 7.1.4 BED ENCODER   â€” v7.0 NEW
# ============================================================================

def build_atmos_71_4_filtergraph(p: ProcessingParams) -> tuple:
    """
    Renders a 7.1.4-channel Atmos bed by running the full 8-band HRTF
    engine twice (front and back perspective), a centre channel for the
    dry signal, and LFE + 4 height channels derived from the mix.

    Channel order (standard 7.1.4):
      FL FR FC LFE BL BR SL SR TFL TFR TBL TBR
    """
    i   = p.intensity_multiplier
    rot = p.rotation_speed * i
    br  = p.bass_rotation * i
    tr  = p.treble_rotation * i
    dvol= 1.0 / max(p.distance, 0.3)
    eq  = _eq_chain(p)
    rev = _reverb(p)

    parts = [
        "[0:a]highpass=f=20:poles=2[dc_in]",
        "[dc_in]pan=mono|c0=0.5*c0+0.5*c1[mono_src]",
        f"[mono_src]{rev}[mono_rev]",
    ]
    if eq:
        parts.append(f"[mono_rev]{eq}[mono_eq]")
        parts.append("[mono_eq]asplit=7[fl_src][fr_src][fc_src][lfe_src][sur_src][h_src][bl_src]")
    else:
        parts.append("[mono_rev]asplit=7[fl_src][fr_src][fc_src][lfe_src][sur_src][h_src][bl_src]")

    # BUG FIX: Atmos LFO previously used 0.5+0.5*sin/cos which drops to exactly
    # 0.0 at the trough â€” creating dead-zone silence for any channel at the back
    # of the rotation. Ported the ILD floor formula from the v7.1 8-band engine:
    #   pan = FLOOR + depth * (0.5 Â± 0.5*cos(Î¸))   where FLOOR=0.12, depth=0.76
    # This keeps the quietest channel at 12% of peak â€” never silent.
    ATMOS_FLOOR = 0.40  # dataset: quiet ear always hears 40-60% (was 0.12 â†’ -18dB dead zones)
    ATMOS_DEPTH = round(1.0 - 2 * ATMOS_FLOOR, 4)  # 0.76

    theta_main = f"2*PI*{rot:.5f}*t"
    theta_tr   = f"2*PI*{tr:.5f}*t"

    # FL (left): louder when source is to the left  â†’ (0.5 âˆ’ 0.5Â·cos Î¸)
    fl_lfo = f"({ATMOS_FLOOR:.2f}+{ATMOS_DEPTH:.2f}*(0.5-0.5*cos({theta_main})))"
    # FR (right): louder when source is to the right â†’ (0.5 + 0.5Â·cos Î¸)
    fr_lfo = f"({ATMOS_FLOOR:.2f}+{ATMOS_DEPTH:.2f}*(0.5+0.5*cos({theta_main})))"
    # BL/BR (surround): phase-shifted by Ï€ for rear image
    bl_lfo = f"(0.10+0.32*(0.5-0.5*cos({theta_main}+3.14159)))"
    br_lfo = f"(0.10+0.32*(0.5+0.5*cos({theta_main}+3.14159)))"
    # SL/SR (sides): phase Ï€/2 for lateral image
    sl_lfo = f"(0.10+0.25*(0.5-0.5*cos({theta_main}+1.5708)))"
    sr_lfo = f"(0.10+0.25*(0.5+0.5*cos({theta_main}+1.5708)))"
    # Height channels: same floor guarantee, treble rotation speed
    tfl_lfo = f"(0.08+0.34*(0.5-0.5*cos({theta_tr})))"
    tfr_lfo = f"(0.08+0.34*(0.5+0.5*cos({theta_tr})))"
    tbl_lfo = f"(0.07+0.26*(0.5-0.5*cos({theta_tr}+3.14159)))"
    tbr_lfo = f"(0.07+0.26*(0.5+0.5*cos({theta_tr}+3.14159)))"

    # FL: ILD-floor LFO (left)
    parts.append(f"[fl_src]volume='{fl_lfo}':eval=frame[fl_ch]")
    # FR: ILD-floor LFO (right)
    parts.append(f"[fr_src]volume='{fr_lfo}':eval=frame[fr_ch]")
    # FC: dry centre
    parts.append(f"[fc_src]volume=0.6[fc_ch]")
    # LFE: lowpass bass
    parts.append(f"[lfe_src]lowpass=f=120[lfe_ch]")
    # BL / BR (surrounds): phase-inverted rotation, ILD floor
    parts.append(f"[sur_src]asplit=2[bl_src2][br_src2]")
    parts.append(f"[bl_src2]volume='{bl_lfo}':eval=frame[bl_ch]")
    parts.append(f"[br_src2]volume='{br_lfo}':eval=frame[br_ch]")
    # SL / SR (sides): blend, ILD floor
    parts.append(f"[bl_src]asplit=2[sl_src][sr_src]")
    parts.append(f"[sl_src]volume='{sl_lfo}':eval=frame[sl_ch]")
    parts.append(f"[sr_src]volume='{sr_lfo}':eval=frame[sr_ch]")
    # Height channels (top): highpass of mix, ILD-floor LFO at treble rotation speed
    parts.append(f"[h_src]highpass=f=3000[h_hp]")
    parts.append("[h_hp]asplit=4[tfl_s][tfr_s][tbl_s][tbr_s]")
    parts.append(f"[tfl_s]volume='{tfl_lfo}':eval=frame[tfl_ch]")
    parts.append(f"[tfr_s]volume='{tfr_lfo}':eval=frame[tfr_ch]")
    parts.append(f"[tbl_s]volume='{tbl_lfo}':eval=frame[tbl_ch]")
    parts.append(f"[tbr_s]volume='{tbr_lfo}':eval=frame[tbr_ch]")

    # Join 12 channels
    parts.append(
        "[fl_ch][fr_ch][fc_ch][lfe_ch][bl_ch][br_ch][sl_ch][sr_ch]"
        "[tfl_ch][tfr_ch][tbl_ch][tbr_ch]"
        "join=inputs=12:channel_layout=7.1.4[atmos_out]"
    )
    parts.append("[atmos_out]loudnorm=I=-16:TP=-1.0:LRA=11[atmos_norm]")
    return ";".join(parts), "[atmos_norm]"


# ============================================================================
# FILTERGRAPH LABEL PREFIXER  â€” multi-instance stem support  (v8.0)
# ============================================================================

import re as _re

def _prefix_filtergraph(fg: str, out_label: str,
                         prefix: str, input_ref: str = "") -> tuple:
    """
    Renames every [label] in fg to [prefix+label], so the same engine
    can be instantiated multiple times within one mega-filtergraph without
    label name collisions.  If input_ref is supplied (e.g. "[0:a]"), it is
    prepended to the very first filter token so FFmpeg knows which -i input
    stream to use.

    Returns (new_fg, new_out_label).
    """
    new_fg  = _re.sub(r'\[([^\]]+)\]',
                      lambda m: f'[{prefix}{m.group(1)}]', fg)
    new_out = f'[{prefix}{out_label[1:-1]}]'
    if input_ref:
        new_fg = input_ref + new_fg
    return new_fg, new_out


# ============================================================================
# MULTIBAND MASTERING BUS  â€” post-mix master chain  (v8.0)
# ============================================================================

def _multiband_master_bus(mix_label: str, out_label: str,
                           p: ProcessingParams) -> str:
    """
    3-band multiband compressor + stereo width + EBU R128 + true peak limiter.

    Applied after all per-stem 8D signals are mixed, before final encode.
    Returns a semicolon-joined filtergraph fragment (no leading/trailing []).
    The fragment reads from [mix_label] and outputs to [out_label].
    """
    parts = []

    # Pre-master corrective EQ
    parts.append(
        f"[{mix_label}]"
        "equalizer=f=35:t=h:w=30:g=-1.5,"
        "equalizer=f=300:t=q:w=250:g=-1.0,"
        "equalizer=f=2500:t=q:w=2000:g=0.8,"
        f"equalizer=f=9000:t=h:w=4000:g=1.0[{out_label}_pmeq]"
    )

    # Split into 3 bands
    parts.append(
        f"[{out_label}_pmeq]asplit=3"
        f"[{out_label}_lo][{out_label}_mi][{out_label}_hi]"
    )

    # Low band: sub + bass (â‰¤ 200 Hz) â€” heavy glue, tight punch
    parts.append(
        f"[{out_label}_lo]lowpass=f=200[{out_label}_lolp]"
    )
    parts.append(
        f"[{out_label}_lolp]"
        "acompressor=threshold=-24dB:ratio=3.5:attack=8:release=120"
        f":makeup=1.5dB:knee=3dB[{out_label}_loc]"
    )

    # Mid band: 200 Hz â€“ 5 kHz â€” moderate control
    parts.append(
        f"[{out_label}_mi]"
        f"highpass=f=200[{out_label}_mihp]"
    )
    parts.append(
        f"[{out_label}_mihp]lowpass=f=5000[{out_label}_milp]"
    )
    parts.append(
        f"[{out_label}_milp]"
        "acompressor=threshold=-20dB:ratio=2.5:attack=12:release=200"
        f":makeup=1.0dB:knee=4dB[{out_label}_mic]"
    )

    # High band: > 5 kHz â€” gentle air control
    parts.append(
        f"[{out_label}_hi]highpass=f=5000[{out_label}_hihp]"
    )
    parts.append(
        f"[{out_label}_hihp]"
        "acompressor=threshold=-18dB:ratio=2.0:attack=5:release=80"
        f":makeup=0.5dB:knee=2dB[{out_label}_hic]"
    )

    # Re-combine bands
    parts.append(
        f"[{out_label}_loc][{out_label}_mic][{out_label}_hic]"
        f"amix=inputs=3:duration=first:normalize=0[{out_label}_glued]"
    )

    # Stereo width on master bus
    master_width = round(min(p.stereo_width * 1.05, 1.40), 3)
    parts.append(
        f"[{out_label}_glued]"
        f"stereotools=mlev={master_width}:sbal=0:softclip=1"
        f"[{out_label}_wide]"
    )

    # EBU R128 loudness normalisation
    parts.append(
        f"[{out_label}_wide]"
        "loudnorm=I=-16:TP=-1.0:LRA=11:linear=true"
        f"[{out_label}_loud]"
    )

    # True peak limiter
    parts.append(
        f"[{out_label}_loud]"
        f"alimiter=limit=1:attack=20:release=200:level=false[{out_label}]"
    )

    return ";".join(parts)


# ============================================================================
# SINGLE-PASS MULTI-STEM FILTERGRAPH  â€” all stems in one FFmpeg call (v8.0)
# ============================================================================

def build_single_pass_stem_filtergraph(
    stem_configs: List[Dict[str, Any]],
    master_params: ProcessingParams,
) -> tuple:
    """
    Build one large FFmpeg filtergraph that processes every stem
    simultaneously:
      1. Each stem runs through a full 8-band HRTF engine (per-stem params)
      2. All processed stems are amixed (normalize=0 to honour gain staging)
      3. The combined signal passes through the multiband mastering bus

    stem_configs: list of dicts, one per stem â€”
      { 'stem_name': str, 'input_idx': int, 'params': ProcessingParams }
    master_params: ProcessingParams for the master bus settings.

    Returns (filtergraph_string, output_label).
    """
    if not stem_configs:
        raise ValueError("build_single_pass_stem_filtergraph: stem_configs is empty")
    all_parts: List[str] = []
    processed_labels: List[str] = []

    for cfg in stem_configs:
        stem_name = cfg['stem_name']
        idx       = cfg['input_idx']
        sp        = cfg['params']
        prefix    = f"{stem_name[:2]}{idx}_"  # e.g. "vo0_", "dr1_"

        fg, out_lbl = build_8band_hrtf_engine_v6(sp)
        pfg, pout   = _prefix_filtergraph(fg, out_lbl, prefix, f"[{idx}:a]")

        all_parts.append(pfg)
        processed_labels.append(pout)
        print(f"  â†³ [{stem_name}] input={idx} out={pout}  "
              f"rot={sp.rotation_speed:.3f} w={sp.stereo_width:.2f} "
              f"rev={sp.reverb_mix:.2f}")

    # Mix all stems
    n = len(processed_labels)
    mix_in = "".join(processed_labels)
    all_parts.append(
        f"{mix_in}amix=inputs={n}:duration=first:normalize=0[stem_mix_raw]"
    )

    # Master bus (reads [stem_mix_raw] â†’ writes [final_out])
    master_bus = _multiband_master_bus("stem_mix_raw", "final_out", master_params)
    all_parts.append(master_bus)

    return ";".join(all_parts), "[final_out]"


# ============================================================================
# STEM SEPARATION   â€” v8.0  (4-stem + 6-stem Demucs, Spleeter fallback)
# ============================================================================

# MODEL â†’ stem names mapping
_DEMUCS_STEMS = {
    "htdemucs":    ["vocals", "drums", "bass", "other"],
    "htdemucs_6s": ["vocals", "drums", "bass", "guitar", "piano", "other"],
    "mdx_extra":   ["vocals", "drums", "bass", "other"],
}

async def separate_stems(
    input_path: str,
    job_id: str,
    model: str = "htdemucs",
) -> Optional[tuple]:
    """
    Separates audio into stems.  Three engine tiers in priority order:

      demucs_api  â€” demucs.api.Separator  (Python API, in-process, fastest)
      demucs_cli  â€” `python -m demucs`    (subprocess; used when the API
                    sub-module fails to import despite the package being
                    installed â€” common with demucs 4.0.1 + dependency conflicts)
      spleeter    â€” last resort, 4-stem only

    model choices:
      "htdemucs"    â€” 4 stems: vocals / drums / bass / other
      "htdemucs_6s" â€” 6 stems: vocals / drums / bass / guitar / piano / other
      "spleeter"    â€” override to Spleeter regardless of installed engine

    Returns (stems_dict, session_id) or None on failure.
    """
    if not STEM_SEPARATION:
        return None

    session_id = str(uuid.uuid4())
    stem_dir   = TEMP_DIR / f"stems_{session_id}"
    stem_dir.mkdir(exist_ok=True)

    # When Spleeter is the only available engine, force 4-stem model
    if STEM_ENGINE == "spleeter":
        use_model = "spleeter"
    else:
        use_model = model
    stem_names = _DEMUCS_STEMS.get(use_model, _DEMUCS_STEMS["htdemucs"])

    await manager.send_progress(
        job_id, 8,
        f"ðŸŽ¸ Separating stems [{use_model}] via {STEM_ENGINE} â€” "
        f"{len(stem_names)} stemsâ€¦"
    )

    stems: Dict[str, str] = {}

    try:
        # â”€â”€ Tier 1: demucs Python API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if STEM_ENGINE == "demucs_api":
            import demucs.api as demucs_api

            def _run_api():
                sep = demucs_api.Separator(model=use_model)
                _, separated = sep.separate_audio_file(input_path)
                stem_dir.mkdir(parents=True, exist_ok=True)
                saved: Dict[str, str] = {}
                for sname, source in separated.items():
                    out_path = stem_dir / f"{sname}.wav"
                    demucs_api.save_audio(source, str(out_path),
                                          samplerate=sep.samplerate)
                    saved[sname] = str(out_path)
                return saved

            loop  = asyncio.get_running_loop()
            stems = await loop.run_in_executor(None, _run_api)

            if not stems and use_model == "htdemucs_6s":
                print("  â†³ htdemucs_6s produced no stems â€” retrying with htdemucsâ€¦")
                return await separate_stems(input_path, job_id, "htdemucs")

        # â”€â”€ Tier 2: demucs CLI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        elif STEM_ENGINE == "demucs_cli":
            cmd = [
                sys.executable, "-m", "demucs",
                "-n", use_model,
                "--out", str(stem_dir),
                input_path,
            ]
            print(f"  â†³ demucs CLI: {' '.join(cmd)}")
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            _stdout, _stderr = await proc.communicate()

            if proc.returncode != 0:
                err_msg = _stderr.decode(errors="replace")[:600]
                print(f"âš   demucs CLI failed (rc={proc.returncode}): {err_msg}")
                if use_model == "htdemucs_6s":
                    print("  â†³ Retrying with htdemucs (4-stem fallback)â€¦")
                    return await separate_stems(input_path, job_id, "htdemucs")
                return None

            # CLI writes to: <stem_dir>/<model>/<input_stem>/<stem>.wav
            input_stem = Path(input_path).stem
            cli_out    = stem_dir / use_model / input_stem
            if not cli_out.exists():
                # Some demucs builds omit the model sub-dir
                cli_out_flat = stem_dir / input_stem
                if cli_out_flat.exists():
                    cli_out = cli_out_flat
                else:
                    print(f"âš   demucs CLI output dir not found: {cli_out}")
                    return None

            for sname in stem_names:
                p = cli_out / f"{sname}.wav"
                if p.exists():
                    stems[sname] = str(p)
                else:
                    print(f"  âš   Stem file missing: {sname}.wav")

        # â”€â”€ Tier 3: Spleeter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        elif STEM_ENGINE == "spleeter":
            from spleeter.separator import Separator as SpleeterSep
            sep  = SpleeterSep("spleeter:4stems")
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(
                None,
                lambda: sep.separate_to_file(input_path, str(stem_dir))
            )
            sp_out = stem_dir / Path(input_path).stem
            for sname in ["vocals", "drums", "bass", "other"]:
                p = sp_out / f"{sname}.wav"
                if p.exists():
                    stems[sname] = str(p)

        # â”€â”€ Result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not stems:
            print("âš   No stems found after separation")
            return None

        stem_sessions[session_id] = stems
        _session_timestamps[session_id] = time.time()  # TTL clock
        print(f"âœ… Stems [{use_model}/{STEM_ENGINE}]: "
              f"{list(stems.keys())} â†’ session {session_id}")
        await manager.send_progress(
            job_id, 18,
            f"âœ… Stems ready ({len(stems)}): {', '.join(stems.keys())}"
        )
        return stems, session_id

    except Exception as e:
        print(f"âŒ Stem separation failed: {e}")
        import traceback; traceback.print_exc()
        return None


# ============================================================================
# VIDEO VISUALIZER GENERATOR   â€” v7.0 NEW
# ============================================================================

async def generate_video(
    audio_path: str,
    video_output_path: str,
    params: ProcessingParams,
    job_id: str
) -> bool:
    """
    Generates a video visualisation synced to the processed audio using
    FFmpeg's built-in audio visualisation filters.

    Styles:
      waveform    â€” showwaves (colour waveform on dark background)
      spectrum    â€” showspectrum (spectrogram with log scale)
      vectorscope â€” avectorscope (Lissajous stereo phase)
    """
    await manager.send_progress(job_id, 94, f"ðŸŽ¬ Rendering video ({params.video_style})â€¦")

    w, h = params.video_resolution.split("x")
    fps  = params.video_fps

    if params.video_style == "spectrum":
        vis_filter = (
            f"[0:a]showspectrum=s={w}x{h}:mode=combined:color=intensity:"
            f"scale=log:saturation=3:fps={fps},format=yuv420p[v]"
        )
    elif params.video_style == "vectorscope":
        vis_filter = (
            f"[0:a]avectorscope=s={w}x{h}:zoom=3:rc=255:gc=180:bc=50:"
            f"rf=0:gf=0:bf=0,format=yuv420p[v]"
        )
    else:  # waveform (default)
        vis_filter = (
            f"[0:a]showwaves=s={w}x{h}:mode=cline:rate={fps}:"
            f"colors=#c87c3a|#e09050,format=yuv420p[v]"
        )

    cmd = [
        "ffmpeg", "-y",
        "-i", audio_path,
        "-filter_complex", vis_filter,
        "-map", "[v]",
        "-map", "0:a",
        "-c:v", "libx264", "-preset", "fast", "-crf", "22",
        "-c:a", "aac", "-b:a", "256k",
        "-shortest",
        video_output_path,
    ]

    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    _, stderr = await proc.communicate()

    if proc.returncode != 0:
        print(f"âš   Video generation failed: {stderr.decode()[-500:]}")
        return False

    print(f"âœ… Video â†’ {video_output_path}")
    return True

async def process_8d_audio(
    input_file: str,
    output_file: str,
    params: ProcessingParams,
    job_id: str,
    audio_analysis: Optional[Dict[str, Any]] = None
):
    try:
        print(f"\nðŸŽ¬ 8D Processing v10.0 â€” job {job_id}")
        await manager.send_progress(job_id, 5, "Running deep audio analysisâ€¦")

        # Re-analyze if not already done
        if audio_analysis is None and ADVANCED_ANALYSIS:
            try:
                audio_analysis = audio_analyzer.analyze_comprehensive(input_file)
                await manager.send_progress(job_id, 12,
                    f"Analysis complete â†’ {audio_analysis.get('genre','?')} / "
                    f"{audio_analysis.get('key','?')} {audio_analysis.get('mode','?')} / "
                    f"BPM {audio_analysis.get('bpm','?')}"
                )
            except Exception:
                pass

        # â”€â”€ Stem Separation path â€” v8.0 single-pass psychoacoustic engine â”€â”€â”€
        if params.enable_stem_separation and STEM_SEPARATION:
            model  = params.stem_engine_model or "htdemucs"
            result = await separate_stems(input_file, job_id, model)
            if result:
                stems_map, session_id = result
                stem_list = list(stems_map.items())   # [(name, path), ...]
                n_stems   = len(stem_list)

                await manager.send_progress(
                    job_id, 20,
                    f"ðŸŽ›  Building per-stem spatial params ({n_stems} stems)â€¦"
                )

                # â”€â”€ Per-stem gain staging + psychoacoustic param routing â”€â”€â”€â”€â”€â”€
                stem_configs: List[Dict[str, Any]] = []
                for idx, (stem_name, stem_path) in enumerate(stem_list):

                    # Gain staging: measure stem RMS â†’ compute makeup gain
                    gain_db = 0.0
                    if params.enable_gain_staging:
                        gain_db = instrument_router.estimate_stem_gain_db(
                            stem_path, params.stem_target_lufs
                        )
                        print(f"  â†³ [{stem_name}] gain staging: {gain_db:+.1f} dB")

                    # InstrumentRouter: get psychoacoustically tuned params
                    if params.stem_auto_route:
                        stem_p = instrument_router.get_stem_params(
                            stem_name, params,
                            analysis=audio_analysis,
                            gain_db=gain_db,
                        )
                    else:
                        # Legacy manual override mode
                        stem_p = params.copy()
                        overrides: Dict[str, Any] = {}
                        if stem_name == "vocals":
                            overrides = {
                                "enable_vocal_center": True,
                                "rotation_speed": params.stem_vocals_rotation
                                    or round(params.rotation_speed * 0.7, 3),
                                "reverb_mix": round(params.reverb_mix * 0.85, 3),
                            }
                        elif stem_name == "drums":
                            overrides = {
                                "rotation_speed": params.stem_drums_rotation
                                    or round(params.rotation_speed * 1.2, 3),
                                "reverb_mix": round(params.reverb_mix * 0.5, 3),
                            }
                        elif stem_name == "bass":
                            overrides = {
                                "rotation_speed": params.stem_bass_rotation_override
                                    or params.bass_rotation,
                                "reverb_mix": round(params.reverb_mix * 0.3, 3),
                            }
                        elif stem_name == "guitar":
                            overrides = {
                                "rotation_speed": params.stem_guitar_rotation
                                    or round(params.rotation_speed * 1.1, 3),
                            }
                        elif stem_name == "piano":
                            overrides = {
                                "rotation_speed": params.stem_piano_rotation
                                    or round(params.rotation_speed * 0.9, 3),
                            }
                        else:
                            overrides = {
                                "rotation_speed": params.stem_other_rotation
                                    or params.treble_rotation,
                            }
                        stem_p = stem_p.copy(update=overrides)

                    # Force WAV output and no video for intermediate stems
                    stem_p = stem_p.copy(update={
                        "output_format": "wav",
                        "generate_video": False,
                    })

                    stem_configs.append({
                        "stem_name": stem_name,
                        "input_idx": idx,
                        "params":    stem_p,
                    })

                await manager.send_progress(
                    job_id, 28,
                    f"ðŸ”® Building single-pass mega-filtergraph ({n_stems} engines)â€¦"
                )

                # â”€â”€ Build single-pass filtergraph (all stems â†’ one FFmpeg call)
                # BUG FIX: the previous if/else had two branches that called the
                # identical function â€” build_single_pass_stem_filtergraph always
                # applies the multiband master bus internally (see _multiband_master_bus).
                # The `enable_multiband_master` flag is read inside that function,
                # so there is no need for separate call sites here.
                fg, out_lbl = build_single_pass_stem_filtergraph(
                    stem_configs, params
                )
                engine_tag = (
                    "Single-Pass 8xHRTF + Multiband Master Bus"
                    if params.enable_multiband_master
                    else "Single-Pass 8xHRTF (no multiband)"
                )

                print(f"  â†³ Engine : {engine_tag}")
                print(f"  â†³ Graph  : {len(fg)} chars across {n_stems} HRTF instances")

                # Codec for final output
                actual_sr = params.sample_rate or 48000
                if params.output_format == "mp3":
                    if actual_sr > 48000:
                        actual_sr = 48000
                    final_codec = ["-c:a", "libmp3lame", "-b:a", f"{params.bitrate}k"]
                elif params.output_format == "flac":
                    final_codec = ["-c:a", "flac", "-compression_level", "8"]
                else:
                    final_codec = ["-c:a", "pcm_s24le"]

                # Build FFmpeg inputs list
                ff_inputs: List[str] = []
                for _, (_, stem_path) in enumerate(stem_list):
                    ff_inputs += ["-i", stem_path]

                mix_cmd = [
                    "ffmpeg", "-y",
                    *ff_inputs,
                    "-filter_complex", fg,
                    "-map", out_lbl,
                    "-ar", str(actual_sr),
                    *final_codec,
                    output_file,
                ]

                await manager.send_progress(
                    job_id, 35,
                    f"âš™ï¸  Rendering {n_stems}-stem 8D audioâ€¦"
                )
                total_dur = get_audio_duration(stem_list[0][1])

                proc = await asyncio.create_subprocess_exec(
                    *mix_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                _active_procs[job_id] = proc   # register for cancel endpoint
                while True:
                    line = await proc.stderr.readline()
                    if not line:
                        break
                    ls = line.decode('utf-8', errors='ignore')
                    stderr_lines.append(ls)
                    if 'time=' in ls:
                        raw = await ffmpeg_progress(ls, total_dur)
                        pct = min(35 + int(raw * 57), 92)
                        await manager.send_progress(
                            job_id, pct, "Encoding stem-based 8D audioâ€¦"
                        )
                await proc.wait()

                if proc.returncode != 0:
                    tail = "".join(stderr_lines[-60:])
                    print(f"âŒ Single-pass stem mix failed:\n{tail}")
                    raise Exception(
                        f"Stem filtergraph failed (code {proc.returncode})."
                    )

                video_url = None
                if params.generate_video and os.path.exists(output_file):
                    vid_out = str(
                        OUTPUT_DIR / (Path(output_file).stem + "_viz.mp4")
                    )
                    ok = await generate_video(
                        output_file, vid_out, params, job_id
                    )
                    if ok:
                        video_url = (
                            f"http://localhost:8000/download/"
                            f"{Path(vid_out).name}"
                        )

                await manager.send_progress(
                    job_id, 100, "âœ… Stem-based 8D processing complete!"
                )
                out_url = (
                    f"http://localhost:8000/download/"
                    f"{Path(output_file).name}"
                )
                await manager.send_complete(job_id, out_url, video_url=video_url)
                return True

        # â”€â”€ Standard path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        await manager.send_progress(job_id, 15, "Building spatial filter graphâ€¦")

        if params.output_format == "ambisonics_foa":
            filtergraph, out_label = build_ambisonics_foa_filtergraph(params)
            engine_name = "Ambisonics FOA (W/X/Y/Z B-format)"
        elif params.output_format == "atmos_71_4":
            filtergraph, out_label = build_atmos_71_4_filtergraph(params)
            engine_name = "Dolby Atmos Bed (7.1.4)"
        elif params.enable_multi_band and params.enable_hrtf:
            # Extract beat positions from analysis for beat-locked rotation
            beat_times_s: Optional[List[float]] = None
            if audio_analysis and ADVANCED_ANALYSIS:
                raw_beats = audio_analysis.get('beat_positions', [])
                sr_val    = audio_analysis.get('sample_rate', 48000)
                if raw_beats and sr_val:
                    try:
                        beat_times_s = [float(b) / sr_val for b in raw_beats]
                    except Exception:
                        beat_times_s = None

            # â”€â”€ HRIR pre-pass â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            hrir_input = input_file
            hrir_temp  = None
            fg_params  = params   # default: unmodified params for FFmpeg stage
            if ADVANCED_ANALYSIS and params.enable_hrtf:
                try:
                    await manager.send_progress(
                        job_id, 18, "ðŸ§  HRIR binaural convolution passâ€¦"
                    )
                    y_raw, sr_raw = librosa.load(input_file, sr=None, mono=True)
                    target_sr = params.sample_rate or 48000
                    if sr_raw != target_sr:
                        y_raw = librosa.resample(y_raw, orig_sr=sr_raw, target_sr=target_sr)
                        sr_raw = target_sr

                    hrir_eng  = get_hrir_engine(sr_raw)
                    # Enforce minimum effective rotation: below 0.12 Hz the
                    # azimuth changes too slowly to create perceptible L/R movement.
                    # Psychoacoustic research: sweetspot is 0.10-0.50 Hz; minimum
                    # for clear circular perception is ~0.10 Hz.
                    # Min 0.06 Hz (not 0.12) â€” slow rotation is CORRECT for 8D. 0.12 was too fast.
                    eff_rot = max(params.rotation_speed * params.intensity_multiplier, 0.06)
                    stereo_np = hrir_eng.render(
                        y_raw,
                        beat_times_s=beat_times_s,
                        rotation_speed=eff_rot,
                        elevation=params.elevation,
                        distance_m=params.distance,   # maps 0.2â€“1.1 m into SOFA range
                    )
                    # Write pre-rendered binaural WAV
                    hrir_temp = str(TEMP_DIR / f"{job_id}_hrir.wav")
                    sf.write(hrir_temp, stereo_np.T, sr_raw, subtype='PCM_24')
                    hrir_input = hrir_temp
                    print(f"  â†³ HRIR pre-pass complete â†’ {Path(hrir_temp).name}")
                    await manager.send_progress(
                        job_id, 22, "âœ… HRIR pass done â€” applying reverb + EQâ€¦"
                    )
                except Exception as hrir_err:
                    print(f"  âš   HRIR pass failed ({hrir_err}), falling back to FFmpeg HRTF")
                    import traceback; traceback.print_exc()
                    hrir_temp = None
                    hrir_input = input_file

            # Build FFmpeg filtergraph
            # When hrir_input is the HRIR-pre-rendered WAV, we suppress the
            # FFmpeg HRTF stage (pan rotation) and only apply reverb + EQ.
            # The elevation is handled in the HRIR engine, so we zero it here.
            fg_params = params
            if hrir_temp:
                # HRIR engine already applied rotation + elevation + ITD + pinna
                # FFmpeg stage: reverb, EQ, loudnorm, limiter only.
                # We use the 6-band engine with rotation zeroed out to get
                # just the mastering chain without spatial movement.
                fg_params = params.copy(update={
                    "rotation_speed": 0.0,
                    "bass_rotation":  0.0,
                    "treble_rotation": 0.0,
                    "elevation": 0.0,
                    "enable_hrtf": False,   # skip FFmpeg HRTF layer
                    "enable_multi_band": False,
                })
                filtergraph, out_label = build_stereo_mastering_chain(fg_params)
                engine_name = (
                    "Studio Grade v9.0 â€” HRIR Convolution + Stereo Mastering"
                )
            else:
                filtergraph, out_label = build_8band_hrtf_engine_v6(
                    params, beat_times_s=beat_times_s
                )
                engine_name = "Studio Grade v9.0 (8-band HRTF + Beat-Locked Rotation)"

            # Use HRIR pre-rendered temp file as input if available
            input_file_for_ffmpeg = hrir_input
        elif params.enable_multi_band:
            filtergraph, out_label = build_6band_filtergraph(params)
            engine_name = "6-band multiband engine"
        elif params.enable_vocal_center:
            filtergraph, out_label = build_vocal_aware_filtergraph(params)
            engine_name = "vocal-aware 3-band engine"
        else:
            filtergraph, out_label = build_simple_filtergraph(params)
            engine_name = "simple 2-channel engine"

        print(f"  â†³ Engine : {engine_name}")
        print(f"  â†³ Graph  : {len(filtergraph)} chars")
        await manager.send_progress(job_id, 25, f"Using {engine_name}â€¦")

        actual_sr = params.sample_rate or 48000
        if params.output_format == "mp3":
            if actual_sr > 48000:
                actual_sr = 48000
            codec = ["-c:a", "libmp3lame", "-b:a", f"{params.bitrate}k"]
        elif params.output_format == "flac":
            codec = ["-c:a", "flac", "-compression_level", "8"]
        else:
            # Add TPDF dither before Float32â†’24-bit PCM truncation to prevent
            # quantization distortion. Appended to the filtergraph chain so it
            # runs inside FFmpeg's native processing without a double-encode.
            dither_label = out_label.strip("[]")
            dither_node  = f"dithered_{dither_label}"
            # Replace adither with aresample using dither_method=triangular
            filtergraph  = filtergraph + f";{out_label}aresample=dither_method=triangular[{dither_node}]"
            out_label    = f"[{dither_node}]"
            codec = ["-c:a", "pcm_s24le"]

        # Use HRIR pre-rendered WAV if available, else original input
        _ffmpeg_input = input_file_for_ffmpeg if 'input_file_for_ffmpeg' in locals() else input_file

        # Sample-rate flag: when the HRIR pre-pass has already written the WAV
        # at target_sr, the FFmpeg input is already at the correct rate.
        # Adding -ar would cause a second resampling pass â†’ artifacts.
        # Only add -ar when needed (non-HRIR path or format-mandated downsampling).
        _hrir_done     = locals().get('hrir_temp') is not None
        _ar_flag: list = [] if _hrir_done else ["-ar", str(actual_sr)]

        cmd = [
            "ffmpeg", "-y",
            "-i", _ffmpeg_input,
            "-filter_complex", filtergraph,
            "-map", out_label,
            *_ar_flag,
            *codec,
            output_file
        ]

        await manager.send_progress(job_id, 35, "Applying binaural ITD + panningâ€¦")
        total_dur = get_audio_duration(_ffmpeg_input)

        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        _active_procs[job_id] = proc   # register for cancel endpoint
        stderr_lines = []
        while True:
            line = await proc.stderr.readline()
            if not line:
                break
            ls = line.decode('utf-8', errors='ignore')
            stderr_lines.append(ls)
            if 'time=' in ls:
                raw = await ffmpeg_progress(ls, total_dur)
                pct = min(35 + int(raw * 57), 92)
                await manager.send_progress(job_id, pct, "Encoding spatial audioâ€¦")

        await proc.wait()

        if proc.returncode != 0:
            tail = "".join(stderr_lines[-50:])
            print(f"âŒ FFmpeg error:\n{tail}")
            raise Exception(f"FFmpeg failed (code {proc.returncode}).")

        await manager.send_progress(job_id, 96, "Finalising masterâ€¦")

        if not os.path.exists(output_file):
            raise Exception("Output file was not created")

        # â”€â”€ Spatial QA pass â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Analyse the rendered output. If issues are found (muffled audio,
        # broken rotation, mono collapse etc.) apply a second FFmpeg correction
        # pass and replace the output file in-place.
        if ADVANCED_ANALYSIS:
            try:
                await manager.send_progress(job_id, 97, "ðŸ” Running spatial QA analysisâ€¦")
                qa_report = spatial_qa.analyse(output_file)

                if not qa_report.passed and qa_report.corrections:
                    # Don't re-render if the engine itself needs to be fixed
                    if qa_report.corrections.get("_needs_regen"):
                        print(f"  âš   QA score={qa_report.score:.2f} â€” fundamental engine issue, "
                              f"cannot fix with EQ pass. Issues: {qa_report.issues}")
                    else:
                        await manager.send_progress(
                            job_id, 98,
                            f"âš™ï¸  Applying QA corrections (score={qa_report.score:.2f})â€¦"
                        )
                        corr_chain = _build_qa_correction_filtergraph(qa_report.corrections)
                        if corr_chain:
                            # Determine the correct temp-file extension so FFmpeg
                            # can detect the container format (bug: ".qa_tmp" has none)
                            _ext = {
                                "mp3": ".mp3", "flac": ".flac",
                                "wav": ".wav"
                            }.get(params.output_format, ".wav")
                            qa_corrected = output_file + f".qa_tmp{_ext}"
                            qa_cmd = [
                                "ffmpeg", "-y", "-i", output_file,
                                "-af", corr_chain,
                                "-ar", str(params.sample_rate or 48000),
                            ]
                            # Match original codec
                            if params.output_format == "mp3":
                                qa_cmd += ["-c:a", "libmp3lame", "-b:a", f"{params.bitrate}k"]
                            elif params.output_format == "flac":
                                qa_cmd += ["-c:a", "flac", "-compression_level", "8"]
                            else:
                                qa_cmd += ["-c:a", "pcm_s24le"]
                            qa_cmd.append(qa_corrected)

                            qa_proc = await asyncio.create_subprocess_exec(
                                *qa_cmd,
                                stdout=asyncio.subprocess.PIPE,
                                stderr=asyncio.subprocess.PIPE,
                            )
                            await qa_proc.wait()
                            if qa_proc.returncode == 0 and os.path.exists(qa_corrected):
                                os.replace(qa_corrected, output_file)
                                print(f"  âœ… QA correction applied â€” re-analysingâ€¦")
                                # Final verification pass (no correction loop)
                                qa_final = spatial_qa.analyse(output_file)
                                print(f"  ðŸ“Š Post-correction QA: score={qa_final.score:.2f}  "
                                      f"tilt={qa_final.metrics.get('freq_tilt_db','?')}dB  "
                                      f"modulation={qa_final.metrics.get('pan_mod_depth','?')}")
                            else:
                                print(f"  âš   QA correction FFmpeg pass failed â€” keeping original output")
                                if os.path.exists(qa_corrected):
                                    os.unlink(qa_corrected)
                elif qa_report.passed:
                    print(f"  âœ… QA passed â€” score={qa_report.score:.2f}")

            except Exception as qa_err:
                print(f"  âš   QA pass error (non-fatal): {qa_err}")
                import traceback; traceback.print_exc()

        # Clean up HRIR temp file
        _hrir_temp_ref = locals().get('hrir_temp')
        if _hrir_temp_ref and os.path.exists(_hrir_temp_ref):
            try:
                os.unlink(_hrir_temp_ref)
            except Exception:
                pass

        video_url = None
        if params.generate_video:
            vid_out = str(OUTPUT_DIR / (Path(output_file).stem + "_viz.mp4"))
            ok = await generate_video(output_file, vid_out, params, job_id)
            if ok:
                video_url = f"http://localhost:8000/download/{Path(vid_out).name}"

        await manager.send_progress(job_id, 100, "âœ… 8D processing complete!")
        out_url = f"http://localhost:8000/download/{Path(output_file).name}"
        await manager.send_complete(job_id, out_url, video_url=video_url)
        print(f"âœ… Done â†’ {out_url}")
        _active_procs.pop(job_id, None)
        _active_tasks.pop(job_id, None)
        return True

    except asyncio.CancelledError:
        # Job was cancelled via the DELETE /process/{job_id} endpoint
        print(f"â›” Job {job_id} cancelled by user")
        _active_procs.pop(job_id, None)
        _active_tasks.pop(job_id, None)
        await manager.send_error(job_id, "Processing cancelled by user")
        return False

    except Exception as e:
        print(f"âŒ Processing error: {e}")
        _active_procs.pop(job_id, None)
        _active_tasks.pop(job_id, None)
        await manager.send_error(job_id, str(e))
        return False


# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/health")
async def health():
    ok      = check_ffmpeg()
    has_rev = check_reverberate_filter() if ok else False

    # Explicit variables so the values are readable in logs/tests
    stem_ok  = STEM_SEPARATION   # True when demucs or spleeter is available
    stem_eng = STEM_ENGINE        # "demucs" | "spleeter" | "none"

    # Report whether real KEMAR HRIR data is loaded
    engine = get_hrir_engine()
    sofa_loaded = getattr(engine, "using_sofa", False)

    return {
        "status":               "healthy" if ok else "degraded",
        "ffmpeg":               ok,
        "advanced_analysis":    ADVANCED_ANALYSIS,
        "youtube_support":      YOUTUBE_SUPPORT,
        "stem_separation":      stem_ok,   # â† True when demucs/spleeter installed
        "stem_engine":          stem_eng,  # â† "demucs" when Demucs 4.x present
        "reverb_engine":        "reverberate" if has_rev else "aecho",
        "has_reverberate":      has_rev,
        "version":              "10.0.0",
        "analysis_bands":       10,
        "eq_bands":             12,
        "spatial_bands":        8,
        "genres":               15,
        "itd_simulation":       True,
        "pinna_notch_eq":       True,
        "diffuse_field_eq":     True,
        "allpass_diffusion":    True,
        "ambisonics_foa":       True,
        "atmos_71_4":           True,
        "video_visualizer":     True,
        "hrir_convolution":     ADVANCED_ANALYSIS,
        "hrir_source":          "KEMAR_SOFA" if sofa_loaded else "synthetic",
        "beat_locked_rotation": ADVANCED_ANALYSIS,
        "dual_band_elevation":  True,
        "ms_eq":                True,
        "pre_delay_ms":         "15-35",
        "spatial_qa":           ADVANCED_ANALYSIS,
        "qa_auto_correction":   ADVANCED_ANALYSIS,
        "symmetric_head_shadow": True,
        "dc_blocker":           True,
    }

@app.get("/presets")
async def list_presets():
    """Return the full preset library for the frontend dropdown."""
    return {"presets": PresetLibrary.get_preset_list()}


@app.post("/analyze")
async def analyze_audio(file: UploadFile = File(...)):
    try:
        fid  = str(uuid.uuid4())
        path = TEMP_DIR / f"{fid}_{file.filename}"
        path.write_bytes(await file.read())
        result = audio_analyzer.analyze_comprehensive(str(path))
        path.unlink(missing_ok=True)

        # Attach preset suggestions
        if ADVANCED_ANALYSIS:
            rec = PresetLibrary.select_preset(result)
            result['available_presets']    = PresetLibrary.get_preset_list()
            result['recommended_preset']   = rec
            result['recommended_preset_id'] = result.get('genre', 'pop')

        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/process")
async def process_audio(
    audio_file: Optional[UploadFile] = File(None),
    params: str = Form(...)
):
    try:
        pp = ProcessingParams.model_validate(json.loads(params))
        job_id = str(uuid.uuid4())

        if audio_file:
            in_path = UPLOAD_DIR / f"{job_id}_{audio_file.filename}"
            in_path.write_bytes(await audio_file.read())
        else:
            raise HTTPException(status_code=400, detail="No audio file provided")

        # Apply genre preset if preset_id matches a known genre
        if pp.preset_id and pp.preset_id in PresetLibrary.GENRE_PRESETS:
            preset_data = PresetLibrary.GENRE_PRESETS[pp.preset_id]
            pp = ProcessingParams.apply_preset(pp, preset_data)
            print(f"  â†³ Preset applied: {pp.preset_id} â†’ {preset_data['name']}")

        # Determine file extension based on format
        ext_map = {
            "mp3": "mp3", "wav": "wav", "flac": "flac",
            "ambisonics_foa": "wav", "atmos_71_4": "wav",
        }
        ext = ext_map.get(pp.output_format, pp.output_format)
        out_name = f"{job_id}_8d.{ext}"
        out_path  = OUTPUT_DIR / out_name

        task = asyncio.create_task(
            process_8d_audio(str(in_path), str(out_path), pp, job_id, None)
        )
        _active_tasks[job_id] = task
        return {"job_id": job_id, "status": "processing"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/process/{job_id}")
async def cancel_job(job_id: str):
    """
    Cancel an in-flight processing job.

    Terminates the FFmpeg subprocess (if still running) and cancels the
    asyncio task.  The WebSocket client will receive an error message so
    the UI can update accordingly.
    """
    cancelled_anything = False

    # 1. Kill the FFmpeg subprocess
    proc = _active_procs.get(job_id)
    if proc and proc.returncode is None:
        try:
            proc.terminate()
            await asyncio.sleep(0.5)
            if proc.returncode is None:
                proc.kill()   # force-kill if SIGTERM wasn't enough
        except Exception:
            pass
        cancelled_anything = True

    # 2. Cancel the asyncio task (raises CancelledError inside process_8d_audio)
    task = _active_tasks.get(job_id)
    if task and not task.done():
        task.cancel()
        cancelled_anything = True

    if not cancelled_anything:
        raise HTTPException(status_code=404, detail="Job not found or already complete")

    return {"job_id": job_id, "status": "cancelled"}
async def stems_separate(
    audio_file: UploadFile = File(...),
    model: str = Form("htdemucs"),  # htdemucs | htdemucs_6s | spleeter
):
    """
    Separate audio into stems and return a session ID for reuse.

    model options:
      htdemucs    â€” 4 stems: vocals / drums / bass / other  (default)
      htdemucs_6s â€” 6 stems: vocals / drums / bass / guitar / piano / other
      spleeter    â€” 4 stems via Spleeter (fallback if Demucs unavailable)
    """
    if not STEM_SEPARATION:
        raise HTTPException(
            status_code=501,
            detail="Stem separation not available. Install demucs: pip install demucs"
        )
    job_id  = str(uuid.uuid4())
    in_path = UPLOAD_DIR / f"{job_id}_{audio_file.filename}"
    in_path.write_bytes(await audio_file.read())

    result = await separate_stems(str(in_path), job_id, model=model)
    if not result:
        raise HTTPException(status_code=500, detail="Stem separation failed")

    stems_map, session_id = result
    return {
        "session_id": session_id,
        "model_used": model,
        "stems":      list(stems_map.keys()),
        "stem_count": len(stems_map),
        "download_urls": {
            name: f"http://localhost:8000/stems/{session_id}/{name}"
            for name in stems_map
        }
    }


@app.get("/stems/{session_id}/{stem_name}")
async def download_stem(session_id: str, stem_name: str):
    """Download a separated stem WAV file."""
    if session_id not in stem_sessions:
        raise HTTPException(status_code=404, detail="Stem session not found")
    stems = stem_sessions[session_id]
    if stem_name not in stems:
        raise HTTPException(status_code=404, detail=f"Stem '{stem_name}' not found")
    fp = Path(stems[stem_name])
    if not fp.exists():
        raise HTTPException(status_code=404, detail="Stem file not found on disk")
    return FileResponse(fp, media_type="audio/wav", filename=f"{stem_name}.wav")

@app.websocket("/ws/{job_id}")
async def ws_endpoint(websocket: WebSocket, job_id: str):
    await manager.connect(job_id, websocket)
    try:
        while True:
            await websocket.receive_text()
    except Exception:
        pass
    finally:
        manager.disconnect(job_id)

@app.get("/download/{filename}")
async def download(filename: str):
    fp = OUTPUT_DIR / filename
    if not fp.exists():
        raise HTTPException(status_code=404, detail="File not found")
    return FileResponse(fp, media_type="audio/mpeg", filename=filename)

@app.post("/batch/process")
async def batch_process(
    files: List[UploadFile] = File(...),
    params: str = Form(...)
):
    try:
        batch_id   = str(uuid.uuid4())
        params_obj = ProcessingParams(**json.loads(params))
        batch_queue[batch_id] = []
        _session_timestamps[batch_id] = time.time()  # TTL clock

        for file in files:
            job_id     = str(uuid.uuid4())
            input_path = UPLOAD_DIR / f"{job_id}_{file.filename}"
            input_path.write_bytes(await file.read())
            batch_queue[batch_id].append(BatchJob(
                job_id=job_id, filename=file.filename, status="queued"
            ))

        asyncio.create_task(_process_batch(batch_id, params_obj))
        return {"batch_id": batch_id, "total_files": len(files)}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/batch/{batch_id}/status")
async def get_batch_status(batch_id: str):
    if batch_id not in batch_queue:
        raise HTTPException(status_code=404, detail="Batch not found")
    jobs = batch_queue[batch_id]
    return {
        "batch_id":   batch_id,
        "total":      len(jobs),
        "completed":  sum(1 for j in jobs if j.status == "completed"),
        "failed":     sum(1 for j in jobs if j.status == "failed"),
        "processing": sum(1 for j in jobs if j.status == "processing"),
        "jobs":       [j.dict() for j in jobs],
    }

async def _process_batch(batch_id: str, params: ProcessingParams):
    for job in batch_queue[batch_id]:
        try:
            job.status = "processing"
            files = list(UPLOAD_DIR.glob(f"{job.job_id}_*"))
            if not files:
                job.status = "failed"; job.error = "Input not found"; continue
            in_path  = files[0]
            base     = Path(in_path.name.replace(f"{job.job_id}_", "")).stem
            out_name = f"{base}_8D.{params.output_format}"
            out_path = OUTPUT_DIR / out_name
            success  = await process_8d_audio(str(in_path), str(out_path), params, job.job_id)
            job.status = "completed" if success else "failed"
            if success:
                job.progress   = 100
                job.output_url = f"/download/{out_name}"
            if in_path.exists():
                in_path.unlink()
        except Exception as e:
            job.status = "failed"; job.error = str(e)

@app.post("/youtube/download")
async def yt_download(request: YouTubeDownloadRequest):
    if not YOUTUBE_SUPPORT:
        raise HTTPException(status_code=501, detail="yt-dlp not installed")
    try:
        job_id = str(uuid.uuid4())
        out    = UPLOAD_DIR / f"{job_id}_youtube.mp3"
        opts   = {
            'format': 'bestaudio/best',
            'postprocessors': [{'key': 'FFmpegExtractAudio',
                                'preferredcodec': 'mp3', 'preferredquality': '320'}],
            'outtmpl': str(out.with_suffix('')),
            'quiet': True,
        }
        with yt_dlp.YoutubeDL(opts) as ydl:
            info = ydl.extract_info(request.url, download=True)
        return {"success": True, "title": info.get('title', 'Unknown'),
                "audio_url": f"http://localhost:8000/download/{out.name}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# STARTUP
# ============================================================================

if __name__ == "__main__":
    import uvicorn

    # Exception handler is now registered inside the lifespan context manager,
    # so uvicorn.run() can be called directly without any manual loop setup.

    print("\n" + "="*72)
    print("  8D Audio Converter â€” Deep Analysis Backend  v10.0")
    print("="*72)
    print(f"  Analysis bands    : 10  (sub â†’ air)")
    print(f"  EQ bands          : 12  (30 Hz â†’ 16 kHz)")
    print(f"  Spatial bands     : 8   (HRTF + independent rotation)")
    print(f"  Genre profiles    : 15  (incl. Bollywood/Bhangra/Ghazal/Folk)")
    print(f"  Spatial formats   : Stereo Â· Ambisonics FOA Â· Dolby Atmos 7.1.4")
    print(f"  Analysis v6.0     : MFCC Â· Chroma/Key Â· Crest Factor Â· HNR")
    print(f"                      Stereo Correlation Â· Transient Density")
    print(f"                      Tonnetz Â· ZCR Â· Spectral Rolloff")
    print(f"  8D v10.0 (NEW)    : Analysis-calibrated ILD curve (38-song dataset)")
    print(f"                      Sub-bass ILD Â±2 dB â†’ Treble ILD Â±20 dB")
    print(f"                      ITD only below 1500 Hz (max 0.64 ms)")
    print(f"                      Reverb 90% wet (DRR âˆ’32.6 dB dataset mean)")
    print(f"                      Pre-delay 25 ms (analysis target)")
    print(f"                      Rotation median 0.10 Hz (10 s orbit)")
    print(f"                      QA: 3-metric 8D discrimination check")
    print(f"                      TP ceiling âˆ’1.0 dBFS (streaming safe)")
    print(f"  8D v9.x           : HRIR Convolution Â· Beat-Locked Rotation")
    print(f"                      Pinna notch EQ (8.5/10.5/13 kHz)")
    print(f"                      Diffuse-field EQ (IEC 711)")
    print(f"                      Equal-loudness compensation (ISO 226)")
    print(f"  Advanced analysis : {'âœ…' if ADVANCED_ANALYSIS else 'âŒ  pip install librosa soundfile scipy'}")
    print(f"  SOFA/HRTF dataset : {'âœ…  h5py available' if H5PY_AVAILABLE else 'âŒ  pip install h5py'}")
    print(f"  YouTube support   : {'âœ…' if YOUTUBE_SUPPORT else 'âŒ  pip install yt-dlp'}")
    print(f"  Stem separation   : {'âœ…  ' + STEM_ENGINE if STEM_SEPARATION else 'âŒ  pip install --upgrade demucs'}")

    auto_detect_ffmpeg()

    if check_ffmpeg():
        print(f"  FFmpeg            : âœ…  available")
        if check_reverberate_filter():
            print(f"  Reverb engine     : âœ…  reverberate (FFmpeg 5.0+)")
        else:
            print(f"  Reverb engine     : ðŸ”„  4-tap aecho (compatible fallback)")
    else:
        print(f"  FFmpeg            : âŒ  NOT FOUND")
        print(f"    Windows â†’ https://ffmpeg.org/download.html")
        print(f"    Mac     â†’ brew install ffmpeg")
        print(f"    Linux   â†’ sudo apt install ffmpeg")
        sys.exit(1)

    print("="*72)
    print("\n  http://localhost:8000     docs â†’ /docs\n")

    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
